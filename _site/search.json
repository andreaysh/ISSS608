[
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html",
    "href": "Take-home_Ex/Take-home_Ex01.html",
    "title": "Take-home Exercise 01",
    "section": "",
    "text": "This analysis explores heart attack incidents in Japan, focusing on the differences in risk factors between youth and adult age groups. With the growing global prevalence of heart disease, understanding how age influences heart attack likelihood and identifying the strongest predictors are crucial for shaping targeted prevention strategies and healthcare interventions.\nThe dataset from Kaggle - Heart Attack in Japan Youth Vs Adult provides an opportunity to analyze these aspects, helping healthcare providers, policymakers, and researchers develop age-specific awareness campaigns, preventive measures, and resource allocation strategies tailored to reducing heart attack risks in Japan.\n\n\n\nUsing the Heart Attack in Japan: Youth vs. Adult dataset, this Take-home_Ex01 applies appropriate Exploratory Data Analysis (EDA) methods, using the tidyverse package and ggplot functions to:\n\nexplore how age influences the likelihood of heart attack incidents\nidentify the strongest predictors contributing to heart attack incidents\n\n\n\n\n\n\n\nThe following R packages will be loaded for this exercise using pacman::p_load():\n\nreadr: Part of a tidyverse package for fast and efficient reading of rectangular data (CSV, TSV, and other delimited files) into R\ndplyr: Part of a tidyverse package for efficient data manipulation, including filtering, selecting, mutating, summarizing, and grouping data in R\nggplot2: Part of a tidyverse package allowing for flexible and layered creation of complex plots\ntidyverse: A collection of R packages for data manipulation, visualization, and analysis\nknitr: Enables dynamic report generation with R Markdown\npatchwork: Combines multiple ggplot2 plots into a single layout\nggthemes: Provides additional themes and scales for ggplot2\nggridges: Creates ridge plots for density visualization\nFunnelPlotrR:Used to create funnel plots for visualizing and comparing proportions, rates, or ratios across different groups while accounting for statistical variation.\nperformance: Provides tools for assessing and comparing statistical models, including metrics for model quality, diagnostics, and visualization.\nstats: Provides essential statistical functions for modeling, hypothesis testing, regression, and probability distributions.\nparameters: Provides tools for processing, reporting, and visualizing model parameters in a tidy and user-friendly format.\nggstatsplot: Creates visualizations with statistical details embedded, combining ggplot2 with statistical tests for easy interpretation.\npatchwork: Allows easy combination and arrangement of multiple ggplot2 plots into a single cohesive layout.\n\n\n\nCode\npacman::p_load(tidyverse, knitr,\n               patchwork, ggthemes, scales,\n               ggridges, ggdist, ggtext, ggalt,\n               cowplot, ggnewscale,FunnelPlotR, performance, parameters, ggstatsplot)\n\n\n\n\n\n\n\n\nThe code chunk below imports the Heart Attack in Japan: Youth vs. Adult dataset, downloaded from Kaggle, using the read_csv() function from the readr package.\n\n\nCode\nheart_attack &lt;- read_csv(\"data/japan_heart_attack_dataset.csv\", show_col_types = FALSE)\n\nheart_attack\n\n\n# A tibble: 30,000 × 32\n     Age Gender Region Smoking_History Diabetes_History Hypertension_History\n   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;           &lt;chr&gt;            &lt;chr&gt;               \n 1    56 Male   Urban  Yes             No               No                  \n 2    69 Male   Urban  No              No               No                  \n 3    46 Male   Rural  Yes             No               No                  \n 4    32 Female Urban  No              No               No                  \n 5    60 Female Rural  No              No               No                  \n 6    25 Female Rural  No              No               No                  \n 7    78 Male   Urban  No              Yes              Yes                 \n 8    38 Female Urban  Yes             No               No                  \n 9    56 Male   Rural  No              No               Yes                 \n10    75 Male   Urban  No              No               No                  \n# ℹ 29,990 more rows\n# ℹ 26 more variables: Cholesterol_Level &lt;dbl&gt;, Physical_Activity &lt;chr&gt;,\n#   Diet_Quality &lt;chr&gt;, Alcohol_Consumption &lt;chr&gt;, Stress_Levels &lt;dbl&gt;,\n#   BMI &lt;dbl&gt;, Heart_Rate &lt;dbl&gt;, Systolic_BP &lt;dbl&gt;, Diastolic_BP &lt;dbl&gt;,\n#   Family_History &lt;chr&gt;, Heart_Attack_Occurrence &lt;chr&gt;, Extra_Column_1 &lt;dbl&gt;,\n#   Extra_Column_2 &lt;dbl&gt;, Extra_Column_3 &lt;dbl&gt;, Extra_Column_4 &lt;dbl&gt;,\n#   Extra_Column_5 &lt;dbl&gt;, Extra_Column_6 &lt;dbl&gt;, Extra_Column_7 &lt;dbl&gt;, …\n\n\nThe dataset is structured as a tibble dataframe, containing 30,000 rows and 32 columns. Each observation represents an individual case, and the variables capture key medical and demographic information relevant to heart attack incidents across different age groups in Japan.\n\n\n\nWe will check the dataset using below\n\nglimpse(): provides a transposed overview of a dataset, showing variables and their types in a concise format.\nhead(): displays the first few rows of a dataset (default is 6 rows) to give a quick preview of the data.\nsummary(): generates a statistical summary of each variable, including measures like mean, median, and range for numeric data.\nduplicated():returns a logical vector indicating which elements or rows in a vector or data frame are duplicates.\ncolSums(is.na()): counts the number of missing values (NA) in each column of the data frame.\nspec(): use spec() to quickly inspect the column\n\n\nglimpse()head()summary()duplicated()colSum(is.na())spec())\n\n\n\n\nCode\nglimpse(heart_attack)\n\n\nRows: 30,000\nColumns: 32\n$ Age                     &lt;dbl&gt; 56, 69, 46, 32, 60, 25, 78, 38, 56, 75, 36, 40…\n$ Gender                  &lt;chr&gt; \"Male\", \"Male\", \"Male\", \"Female\", \"Female\", \"F…\n$ Region                  &lt;chr&gt; \"Urban\", \"Urban\", \"Rural\", \"Urban\", \"Rural\", \"…\n$ Smoking_History         &lt;chr&gt; \"Yes\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"No\", \"Y…\n$ Diabetes_History        &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No…\n$ Hypertension_History    &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No…\n$ Cholesterol_Level       &lt;dbl&gt; 186.4002, 185.1367, 210.6966, 211.1655, 223.81…\n$ Physical_Activity       &lt;chr&gt; \"Moderate\", \"Low\", \"Low\", \"Moderate\", \"High\", …\n$ Diet_Quality            &lt;chr&gt; \"Poor\", \"Good\", \"Average\", \"Good\", \"Good\", \"Go…\n$ Alcohol_Consumption     &lt;chr&gt; \"Low\", \"Low\", \"Moderate\", \"High\", \"High\", \"Hig…\n$ Stress_Levels           &lt;dbl&gt; 3.644786, 3.384056, 3.810911, 6.014878, 6.8068…\n$ BMI                     &lt;dbl&gt; 33.96135, 28.24287, 27.60121, 23.71729, 19.771…\n$ Heart_Rate              &lt;dbl&gt; 72.30153, 57.45764, 64.65870, 55.13147, 76.667…\n$ Systolic_BP             &lt;dbl&gt; 123.90209, 129.89331, 145.65490, 131.78522, 10…\n$ Diastolic_BP            &lt;dbl&gt; 85.68281, 73.52426, 71.99481, 68.21133, 92.902…\n$ Family_History          &lt;chr&gt; \"No\", \"Yes\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No…\n$ Heart_Attack_Occurrence &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n$ Extra_Column_1          &lt;dbl&gt; 0.40498852, 0.03627815, 0.85297888, 0.39085280…\n$ Extra_Column_2          &lt;dbl&gt; 0.43330004, 0.51256694, 0.21959083, 0.29684675…\n$ Extra_Column_3          &lt;dbl&gt; 0.62871236, 0.66839275, 0.61343656, 0.15572404…\n$ Extra_Column_4          &lt;dbl&gt; 0.70160955, 0.11552874, 0.50800995, 0.87025144…\n$ Extra_Column_5          &lt;dbl&gt; 0.49814235, 0.42381938, 0.90066981, 0.39035591…\n$ Extra_Column_6          &lt;dbl&gt; 0.007901312, 0.083932768, 0.227205241, 0.40318…\n$ Extra_Column_7          &lt;dbl&gt; 0.79458257, 0.68895108, 0.49634358, 0.74140891…\n$ Extra_Column_8          &lt;dbl&gt; 0.29077922, 0.83016364, 0.75210679, 0.22396813…\n$ Extra_Column_9          &lt;dbl&gt; 0.49719307, 0.63449028, 0.18150125, 0.32931387…\n$ Extra_Column_10         &lt;dbl&gt; 0.52199452, 0.30204337, 0.62918031, 0.14319054…\n$ Extra_Column_11         &lt;dbl&gt; 0.79965663, 0.04368285, 0.01827617, 0.90778075…\n$ Extra_Column_12         &lt;dbl&gt; 0.72239788, 0.45166789, 0.06322702, 0.54232201…\n$ Extra_Column_13         &lt;dbl&gt; 0.1487387, 0.8786714, 0.1465122, 0.9224606, 0.…\n$ Extra_Column_14         &lt;dbl&gt; 0.8340099, 0.5356022, 0.9972962, 0.6262165, 0.…\n$ Extra_Column_15         &lt;dbl&gt; 0.061632229, 0.617825340, 0.974455410, 0.22860…\n\n\n\n\n\n\nCode\nhead(heart_attack)\n\n\n# A tibble: 6 × 32\n    Age Gender Region Smoking_History Diabetes_History Hypertension_History\n  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;           &lt;chr&gt;            &lt;chr&gt;               \n1    56 Male   Urban  Yes             No               No                  \n2    69 Male   Urban  No              No               No                  \n3    46 Male   Rural  Yes             No               No                  \n4    32 Female Urban  No              No               No                  \n5    60 Female Rural  No              No               No                  \n6    25 Female Rural  No              No               No                  \n# ℹ 26 more variables: Cholesterol_Level &lt;dbl&gt;, Physical_Activity &lt;chr&gt;,\n#   Diet_Quality &lt;chr&gt;, Alcohol_Consumption &lt;chr&gt;, Stress_Levels &lt;dbl&gt;,\n#   BMI &lt;dbl&gt;, Heart_Rate &lt;dbl&gt;, Systolic_BP &lt;dbl&gt;, Diastolic_BP &lt;dbl&gt;,\n#   Family_History &lt;chr&gt;, Heart_Attack_Occurrence &lt;chr&gt;, Extra_Column_1 &lt;dbl&gt;,\n#   Extra_Column_2 &lt;dbl&gt;, Extra_Column_3 &lt;dbl&gt;, Extra_Column_4 &lt;dbl&gt;,\n#   Extra_Column_5 &lt;dbl&gt;, Extra_Column_6 &lt;dbl&gt;, Extra_Column_7 &lt;dbl&gt;,\n#   Extra_Column_8 &lt;dbl&gt;, Extra_Column_9 &lt;dbl&gt;, Extra_Column_10 &lt;dbl&gt;, …\n\n\n\n\n\n\nCode\nsummary(heart_attack)\n\n\n      Age           Gender             Region          Smoking_History   \n Min.   :18.00   Length:30000       Length:30000       Length:30000      \n 1st Qu.:33.00   Class :character   Class :character   Class :character  \n Median :48.00   Mode  :character   Mode  :character   Mode  :character  \n Mean   :48.49                                                           \n 3rd Qu.:64.00                                                           \n Max.   :79.00                                                           \n Diabetes_History   Hypertension_History Cholesterol_Level Physical_Activity \n Length:30000       Length:30000         Min.   : 80.02    Length:30000      \n Class :character   Class :character     1st Qu.:179.55    Class :character  \n Mode  :character   Mode  :character     Median :199.77    Mode  :character  \n                                         Mean   :199.90                      \n                                         3rd Qu.:220.16                      \n                                         Max.   :336.86                      \n Diet_Quality       Alcohol_Consumption Stress_Levels         BMI       \n Length:30000       Length:30000        Min.   : 0.000   Min.   : 5.58  \n Class :character   Class :character    1st Qu.: 3.644   1st Qu.:21.63  \n Mode  :character   Mode  :character    Median : 4.993   Median :24.96  \n                                        Mean   : 5.002   Mean   :25.00  \n                                        3rd Qu.: 6.353   3rd Qu.:28.36  \n                                        Max.   :10.000   Max.   :46.10  \n   Heart_Rate      Systolic_BP      Diastolic_BP    Family_History    \n Min.   : 30.03   Min.   : 56.23   Min.   : 39.95   Length:30000      \n 1st Qu.: 63.25   1st Qu.:109.79   1st Qu.: 73.26   Class :character  \n Median : 69.95   Median :119.90   Median : 80.12   Mode  :character  \n Mean   : 69.98   Mean   :119.91   Mean   : 80.03                     \n 3rd Qu.: 76.66   3rd Qu.:130.02   3rd Qu.: 86.76                     \n Max.   :108.78   Max.   :178.77   Max.   :117.67                     \n Heart_Attack_Occurrence Extra_Column_1      Extra_Column_2     \n Length:30000            Min.   :0.0000071   Min.   :0.0000052  \n Class :character        1st Qu.:0.2533084   1st Qu.:0.2473606  \n Mode  :character        Median :0.5008204   Median :0.4961980  \n                         Mean   :0.5019388   Mean   :0.4978940  \n                         3rd Qu.:0.7505286   3rd Qu.:0.7473954  \n                         Max.   :0.9999654   Max.   :0.9999894  \n Extra_Column_3      Extra_Column_4      Extra_Column_5     \n Min.   :0.0000227   Min.   :0.0000934   Min.   :0.0001051  \n 1st Qu.:0.2483093   1st Qu.:0.2522110   1st Qu.:0.2518029  \n Median :0.4976104   Median :0.4976175   Median :0.5019871  \n Mean   :0.4981949   Mean   :0.5005952   Mean   :0.5014100  \n 3rd Qu.:0.7476807   3rd Qu.:0.7505662   3rd Qu.:0.7536569  \n Max.   :0.9999694   Max.   :0.9999869   Max.   :0.9999949  \n Extra_Column_6      Extra_Column_7      Extra_Column_8     \n Min.   :0.0000531   Min.   :0.0000678   Min.   :0.0000449  \n 1st Qu.:0.2559989   1st Qu.:0.2482839   1st Qu.:0.2509790  \n Median :0.5017726   Median :0.4988157   Median :0.4985698  \n Mean   :0.5027631   Mean   :0.4980753   Mean   :0.5003557  \n 3rd Qu.:0.7511886   3rd Qu.:0.7456378   3rd Qu.:0.7507293  \n Max.   :0.9998892   Max.   :0.9999900   Max.   :0.9999300  \n Extra_Column_9      Extra_Column_10     Extra_Column_11    \n Min.   :0.0000305   Min.   :0.0000133   Min.   :0.0000008  \n 1st Qu.:0.2502452   1st Qu.:0.2484256   1st Qu.:0.2538092  \n Median :0.4984491   Median :0.5031040   Median :0.5067589  \n Mean   :0.5002292   Mean   :0.5010694   Mean   :0.5044949  \n 3rd Qu.:0.7512186   3rd Qu.:0.7522686   3rd Qu.:0.7556257  \n Max.   :0.9999852   Max.   :0.9999928   Max.   :0.9999578  \n Extra_Column_12     Extra_Column_13     Extra_Column_14    \n Min.   :0.0000713   Min.   :0.0000204   Min.   :0.0000025  \n 1st Qu.:0.2505341   1st Qu.:0.2473108   1st Qu.:0.2482152  \n Median :0.5038609   Median :0.5041162   Median :0.4943841  \n Mean   :0.5008624   Mean   :0.5004557   Mean   :0.4976507  \n 3rd Qu.:0.7511780   3rd Qu.:0.7497094   3rd Qu.:0.7456212  \n Max.   :0.9999484   Max.   :0.9999451   Max.   :0.9999779  \n Extra_Column_15    \n Min.   :0.0000241  \n 1st Qu.:0.2482573  \n Median :0.5009406  \n Mean   :0.4999634  \n 3rd Qu.:0.7487379  \n Max.   :0.9999913  \n\n\n\n\n\n\nCode\nheart_attack[duplicated(heart_attack),]\n\n\n# A tibble: 0 × 32\n# ℹ 32 variables: Age &lt;dbl&gt;, Gender &lt;chr&gt;, Region &lt;chr&gt;, Smoking_History &lt;chr&gt;,\n#   Diabetes_History &lt;chr&gt;, Hypertension_History &lt;chr&gt;,\n#   Cholesterol_Level &lt;dbl&gt;, Physical_Activity &lt;chr&gt;, Diet_Quality &lt;chr&gt;,\n#   Alcohol_Consumption &lt;chr&gt;, Stress_Levels &lt;dbl&gt;, BMI &lt;dbl&gt;,\n#   Heart_Rate &lt;dbl&gt;, Systolic_BP &lt;dbl&gt;, Diastolic_BP &lt;dbl&gt;,\n#   Family_History &lt;chr&gt;, Heart_Attack_Occurrence &lt;chr&gt;, Extra_Column_1 &lt;dbl&gt;,\n#   Extra_Column_2 &lt;dbl&gt;, Extra_Column_3 &lt;dbl&gt;, Extra_Column_4 &lt;dbl&gt;, …\n\n\n\n\n\n\n\n\nLimitations of pie charts in data visualization\n\n\n\n\nDistored perception: Pie chart makes comparisons difficult due to the reliance on angles and areas rather than a common baseline.\nDifficult to compare similar data slices: Can be misleading when there are many segments/ similar-sized portions making it hard to interpret differences accurately.\nSpace constraints: Pie chart can take up more space than necessary and can clutter dashboards or reports.\nPoor for trend analysis: Pie charts only show a single point in time and do not help in comparing trends over multiple years.\n\n\n\n\nRefer to this page to find out why the use of pie charts are discouraged.\n\n\n\n\n\n\n\n\nCode\ncolSums(is.na(heart_attack))\n\n\n                    Age                  Gender                  Region \n                      0                       0                       0 \n        Smoking_History        Diabetes_History    Hypertension_History \n                      0                       0                       0 \n      Cholesterol_Level       Physical_Activity            Diet_Quality \n                      0                       0                       0 \n    Alcohol_Consumption           Stress_Levels                     BMI \n                      0                       0                       0 \n             Heart_Rate             Systolic_BP            Diastolic_BP \n                      0                       0                       0 \n         Family_History Heart_Attack_Occurrence          Extra_Column_1 \n                      0                       0                       0 \n         Extra_Column_2          Extra_Column_3          Extra_Column_4 \n                      0                       0                       0 \n         Extra_Column_5          Extra_Column_6          Extra_Column_7 \n                      0                       0                       0 \n         Extra_Column_8          Extra_Column_9         Extra_Column_10 \n                      0                       0                       0 \n        Extra_Column_11         Extra_Column_12         Extra_Column_13 \n                      0                       0                       0 \n        Extra_Column_14         Extra_Column_15 \n                      0                       0 \n\n\n\n\n\n\n\n\nNote\n\n\n\n\nEnsure that there are no NA values, if not will have to investigate further.\nPossibility to use drop_na() function to drop rows where any specified column contains a missing value.\n\n\n\n\n\n\n\nCode\nspec(heart_attack)\n\n\ncols(\n  Age = col_double(),\n  Gender = col_character(),\n  Region = col_character(),\n  Smoking_History = col_character(),\n  Diabetes_History = col_character(),\n  Hypertension_History = col_character(),\n  Cholesterol_Level = col_double(),\n  Physical_Activity = col_character(),\n  Diet_Quality = col_character(),\n  Alcohol_Consumption = col_character(),\n  Stress_Levels = col_double(),\n  BMI = col_double(),\n  Heart_Rate = col_double(),\n  Systolic_BP = col_double(),\n  Diastolic_BP = col_double(),\n  Family_History = col_character(),\n  Heart_Attack_Occurrence = col_character(),\n  Extra_Column_1 = col_double(),\n  Extra_Column_2 = col_double(),\n  Extra_Column_3 = col_double(),\n  Extra_Column_4 = col_double(),\n  Extra_Column_5 = col_double(),\n  Extra_Column_6 = col_double(),\n  Extra_Column_7 = col_double(),\n  Extra_Column_8 = col_double(),\n  Extra_Column_9 = col_double(),\n  Extra_Column_10 = col_double(),\n  Extra_Column_11 = col_double(),\n  Extra_Column_12 = col_double(),\n  Extra_Column_13 = col_double(),\n  Extra_Column_14 = col_double(),\n  Extra_Column_15 = col_double()\n)\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nEnsure that all variables are correctly classified by data type; recast variable types if needed.\nVariables are correctly classified - where categorical variables are classified as character, while continuous variables are classified as double.\n\n\n\n\n\n\nThe heart_attack tibble contains 32 attributes, as shown above.\nThe following preprocessing checks were conducted as part of data preparation:\n\n\n\n\n\n\nPreprocessing Checks\n\n\n\n\nVerified that the correct data types were loaded in the heart_attack dataset using glimpse() and spec()\nEnsured there were no duplicate variable names using duplicated() in the dataset\nChecked for missing values using colSums(is.na())\n\n\n\n\n\n\nThe table below presents the metadata for the dataset, classifying each variable as either categorical or continuous based on its nature and data type.\n\nCategorical attributes:\n\n\n\n\n\n\n\n\nVariable Names\nType\nDescription\n\n\n\n\nGender\nCHAR\nBiological sex of the individual (e.g., Male, Female)\n\n\nRegion\nCHAR\nGeographic location where the individual resides (e.g., Urban, Rural)\n\n\nSmoking_History\nCHAR\nPast or current smoking habits (e.g., Yes, No)\n\n\nDiabetes_History\nCHAR\nHistory of diabetes diagnosis (Yes/No)\n\n\nHypertension_History\nCHAR\nHistory of high blood pressure diagnosis (Yes/No)\n\n\nPhysical_Activity\nCHAR\nLevel of physical activity (e.g., Low, Moderate, High)\n\n\nDiet_Quality\nCHAR\nDietary habits and nutritional intake assessment (e.g., Poor, Average, Good)\n\n\nAlcohol_Consumption\nCHAR\nFrequency or amount of alcohol intake (e.g., Low, Moderate, High, None)\n\n\nFamily_History\nCHAR\nPresence of heart disease in close relatives (Yes/No)\n\n\nHeart_Attack_Occurrence\nCHAR\nWhether the individual has experienced a heart attack (Yes/No)\n\n\n\nContinuous attributes:\n\n\n\n\n\n\n\n\nVariable Names\nType\nDescription\n\n\n\n\nAge\nNUM\nAge of the individual in years\n\n\nCholesterol_Level\nNUM\nMeasured cholesterol level in the blood\n\n\nStress_Levels\nNUM\nMeasured or self-reported stress level\n\n\nBMI\nNUM\nBody Mass Index, calculated from height and weight\n\n\nHeart_Rate\nNUM\nResting heart rate in beats per minute (bpm)\n\n\nSystolic_BP\nNUM\nSystolic blood pressure measurement (mmHg)\n\n\nDiastolic_BP\nNUM\nDiastolic blood pressure measurement (mmHg)\n\n\nExtra_Column_1\nNUM\n\n\n\nExtra_Column_2\nNUM\n\n\n\nExtra_Column_3\nNUM\n\n\n\nExtra_Column_4\nNUM\n\n\n\nExtra_Column_5\nNUM\n\n\n\nExtra_Column_6\nNUM\n\n\n\nExtra_Column_7\nNUM\n\n\n\nExtra_Column_8\nNUM\n\n\n\nExtra_Column_9\nNUM\n\n\n\nExtra_Column_10\nNUM\n\n\n\nExtra_Column_11\nNUM\n\n\n\nExtra_Column_12\nNUM\n\n\n\nExtra_Column_13\nNUM\n\n\n\nExtra_Column_14\nNUM\n\n\n\nExtra_Column_15\nNUM\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nNotice that Extra_Column_1 to Extra_Column_15 are additional numeric figures without clear definition.\nThey are most likely derived calculations from the variables.\n\n\n\n\n\n\nOf the 32 variables (columns), only 17 variables(columns) are selected for analysis\n\nAll columns are selected except for Extra_Column_1 to Extra_Column_15\n\nThe select() function in the dplyr package is used to obtain these rows, and stored as the R object, heart_attack_1.\n\n\nCode\nheart_attack_1 &lt;- heart_attack %&gt;% \n  select(\"Age\", \"Gender\", \"Region\", \"Smoking_History\", \"Diabetes_History\", \n         \"Hypertension_History\", \"Cholesterol_Level\", \"Physical_Activity\", \n         \"Diet_Quality\", \"Alcohol_Consumption\", \"Stress_Levels\", \"BMI\", \n         \"Heart_Rate\", \"Systolic_BP\", \"Diastolic_BP\", \"Family_History\", \n         \"Heart_Attack_Occurrence\")\n\nheart_attack_1\n\n\n# A tibble: 30,000 × 17\n     Age Gender Region Smoking_History Diabetes_History Hypertension_History\n   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;           &lt;chr&gt;            &lt;chr&gt;               \n 1    56 Male   Urban  Yes             No               No                  \n 2    69 Male   Urban  No              No               No                  \n 3    46 Male   Rural  Yes             No               No                  \n 4    32 Female Urban  No              No               No                  \n 5    60 Female Rural  No              No               No                  \n 6    25 Female Rural  No              No               No                  \n 7    78 Male   Urban  No              Yes              Yes                 \n 8    38 Female Urban  Yes             No               No                  \n 9    56 Male   Rural  No              No               Yes                 \n10    75 Male   Urban  No              No               No                  \n# ℹ 29,990 more rows\n# ℹ 11 more variables: Cholesterol_Level &lt;dbl&gt;, Physical_Activity &lt;chr&gt;,\n#   Diet_Quality &lt;chr&gt;, Alcohol_Consumption &lt;chr&gt;, Stress_Levels &lt;dbl&gt;,\n#   BMI &lt;dbl&gt;, Heart_Rate &lt;dbl&gt;, Systolic_BP &lt;dbl&gt;, Diastolic_BP &lt;dbl&gt;,\n#   Family_History &lt;chr&gt;, Heart_Attack_Occurrence &lt;chr&gt;\n\n\nThe output shows a tibble dataframe with 30,000 rows and 17 columns.\n\n\n\n\n\nIn the following section, we will recode specific continuous variables into categorical groups for better interpretability.\nWhile variables like Cholesterol_Level, Stress_Levels, BMI, Heart_Rate, Systolic_BP, and Diastolic_BP provide valuable insights in their continuous form, categorizing them into meaningful groups can enhance our ability to analyze trends and risk factors more effectively.\n\n\n\n\n\n\n\n\nVariable Names\nMeasurement\nCategorical ranges Approximated classification\n\n\n\n\nCholesterol_Level\nmg/dL\n\nLow: ≤ 150\nModerate: 151–200\nHigh: &gt; 200\n\n\n\nStress_Levels\nSelf-reported scale\n\nMiniminal_Stress: 0\nLow_Stress: 1–3\nModerate_Stress: 4–7\nHigh_Stress: 8–10\n\n\n\nBMI\nBody Mass Index\n\nUnderweight: &lt; 18.5\nNormal_Weight: 18.5–24.9\nOverweight: 25–29.9\nObese: ≥ 30\n\n\n\nHeart_Rate\nbeats per minute\n\nLow: &lt; 60\nNormal: 60–100\nHigh: &gt; 100\n\n\n\nSystolic_BP\nmmHg\n\nNormal: &lt; 120\nElevated: 120–129\nHypertension_Stage_1: 130–139\nHypertension_Stage_2: ≥ 140\n\n\n\nDiastolic_BP\nmmHg\n\nNormal: &lt; 80\nElevated: 80–89\nHypertension_Stage_1: 90–99\nHypertension_Stage_2: ≥ 100\n\n\n\n\n\n\nCode\nlibrary(dplyr)\n\nheart_attack_1 &lt;- heart_attack %&gt;%\n  select(\"Age\", \"Gender\", \"Region\", \"Smoking_History\", \"Diabetes_History\", \n         \"Hypertension_History\", \"Cholesterol_Level\", \"Physical_Activity\", \n         \"Diet_Quality\", \"Alcohol_Consumption\", \"Stress_Levels\", \"BMI\", \n         \"Heart_Rate\", \"Systolic_BP\", \"Diastolic_BP\", \"Family_History\", \n         \"Heart_Attack_Occurrence\") %&gt;%\n  mutate(\n    Cholesterol_Level_Category = case_when(\n      Cholesterol_Level &lt;= 150 ~ \"Low\",\n      Cholesterol_Level &lt;= 200 ~ \"Moderate\",\n      TRUE ~ \"High\"\n    ),\n    \n    # **ROUND Stress_Levels before categorization**\n    Rounded_Stress_Levels = round(Stress_Levels), \n    \n    Stress_Levels_Category = case_when(\n      is.na(Rounded_Stress_Levels) ~ \"Unknown\",  # Handle missing values\n      Rounded_Stress_Levels == 0 ~ \"Miniminal_Stress\",\n      Rounded_Stress_Levels %in% 1:3 ~ \"Low_Stress\",\n      Rounded_Stress_Levels %in% 4:7 ~ \"Moderate_Stress\",\n      Rounded_Stress_Levels %in% 8:10 ~ \"High_Stress\",\n      TRUE ~ \"Unknown\"\n    ),\n    \n    BMI_Category = case_when(\n      BMI &lt; 18.5 ~ \"Underweight\",\n      BMI &gt;= 18.5 & BMI &lt; 25 ~ \"Normal_Weight\",\n      BMI &gt;= 25 & BMI &lt; 30 ~ \"Overweight\",\n      BMI &gt;= 30 ~ \"Obese\",\n      TRUE ~ \"Unknown\"\n    ),\n    \n    Heart_Rate_Category = case_when(\n      Heart_Rate &lt; 60 ~ \"Low\",\n      Heart_Rate &gt;= 60 & Heart_Rate &lt;= 100 ~ \"Normal\",\n      Heart_Rate &gt; 100 ~ \"High\",\n      TRUE ~ \"Unknown\"\n    ),\n    \n    Systolic_BP_Category = case_when(\n      Systolic_BP &lt; 120 ~ \"Normal\",\n      Systolic_BP &gt;= 120 & Systolic_BP &lt; 130 ~ \"Elevated\",\n      Systolic_BP &gt;= 130 & Systolic_BP &lt; 140 ~ \"Hypertension_Stage_1\",\n      Systolic_BP &gt;= 140 ~ \"Hypertension_Stage_2\",\n      TRUE ~ \"Unknown\"\n    ),\n    \n    Diastolic_BP_Category = case_when(\n      Diastolic_BP &lt; 80 ~ \"Normal\",\n      Diastolic_BP &gt;= 80 & Diastolic_BP &lt; 90 ~ \"Elevated\",\n      Diastolic_BP &gt;= 90 & Diastolic_BP &lt; 100 ~ \"Hypertension_Stage_1\",\n      Diastolic_BP &gt;= 100 ~ \"Hypertension_Stage_2\",\n      TRUE ~ \"Unknown\"\n    )\n  )\n\n# View the modified dataframe with new categorical variables\nheart_attack_1\n\n\n# A tibble: 30,000 × 24\n     Age Gender Region Smoking_History Diabetes_History Hypertension_History\n   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;           &lt;chr&gt;            &lt;chr&gt;               \n 1    56 Male   Urban  Yes             No               No                  \n 2    69 Male   Urban  No              No               No                  \n 3    46 Male   Rural  Yes             No               No                  \n 4    32 Female Urban  No              No               No                  \n 5    60 Female Rural  No              No               No                  \n 6    25 Female Rural  No              No               No                  \n 7    78 Male   Urban  No              Yes              Yes                 \n 8    38 Female Urban  Yes             No               No                  \n 9    56 Male   Rural  No              No               Yes                 \n10    75 Male   Urban  No              No               No                  \n# ℹ 29,990 more rows\n# ℹ 18 more variables: Cholesterol_Level &lt;dbl&gt;, Physical_Activity &lt;chr&gt;,\n#   Diet_Quality &lt;chr&gt;, Alcohol_Consumption &lt;chr&gt;, Stress_Levels &lt;dbl&gt;,\n#   BMI &lt;dbl&gt;, Heart_Rate &lt;dbl&gt;, Systolic_BP &lt;dbl&gt;, Diastolic_BP &lt;dbl&gt;,\n#   Family_History &lt;chr&gt;, Heart_Attack_Occurrence &lt;chr&gt;,\n#   Cholesterol_Level_Category &lt;chr&gt;, Rounded_Stress_Levels &lt;dbl&gt;,\n#   Stress_Levels_Category &lt;chr&gt;, BMI_Category &lt;chr&gt;, …\n\n\n\n\n\nWhile analyzing age as a continuous variable provides detailed insights, categorizing the Age variable into distinct age groups allows us to explore how age influences the likelihood of heart attack incidents.\nThe table below are the proposed age categories for this analysis:\n\n\n\nCategories\nAge\n\n\n\n\nYouth\n≤25 years\n\n\nYoung adults\n26–40 years\n\n\nMiddle-aged adults\n41–55 years\n\n\nOlder adults\n56–70 years\n\n\nElderly\n≥71 years\n\n\n\n\n\nCode\nheart_attack_2 &lt;- heart_attack_1 %&gt;%\n  mutate(\n    Age_Category = case_when(\n      Age &lt;= 25 ~ \"Youth\",\n      Age &gt;= 26 & Age &lt;= 40 ~ \"Young Adult\",\n      Age &gt;= 41 & Age &lt;= 55 ~ \"Middle-Aged Adult\",\n      Age &gt;= 56 & Age &lt;= 70 ~ \"Older Adult\",\n      Age &gt;= 71 ~ \"Elderly\",\n      TRUE ~ \"Unknown\"\n    )\n  )\n\n# View the updated dataset\nheart_attack_2\n\n\n# A tibble: 30,000 × 25\n     Age Gender Region Smoking_History Diabetes_History Hypertension_History\n   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;           &lt;chr&gt;            &lt;chr&gt;               \n 1    56 Male   Urban  Yes             No               No                  \n 2    69 Male   Urban  No              No               No                  \n 3    46 Male   Rural  Yes             No               No                  \n 4    32 Female Urban  No              No               No                  \n 5    60 Female Rural  No              No               No                  \n 6    25 Female Rural  No              No               No                  \n 7    78 Male   Urban  No              Yes              Yes                 \n 8    38 Female Urban  Yes             No               No                  \n 9    56 Male   Rural  No              No               Yes                 \n10    75 Male   Urban  No              No               No                  \n# ℹ 29,990 more rows\n# ℹ 19 more variables: Cholesterol_Level &lt;dbl&gt;, Physical_Activity &lt;chr&gt;,\n#   Diet_Quality &lt;chr&gt;, Alcohol_Consumption &lt;chr&gt;, Stress_Levels &lt;dbl&gt;,\n#   BMI &lt;dbl&gt;, Heart_Rate &lt;dbl&gt;, Systolic_BP &lt;dbl&gt;, Diastolic_BP &lt;dbl&gt;,\n#   Family_History &lt;chr&gt;, Heart_Attack_Occurrence &lt;chr&gt;,\n#   Cholesterol_Level_Category &lt;chr&gt;, Rounded_Stress_Levels &lt;dbl&gt;,\n#   Stress_Levels_Category &lt;chr&gt;, BMI_Category &lt;chr&gt;, …\n\n\n\n\n\nWe will re-check the dataset after filtering and recoding of continuous variables using below:\n\nglimpse()head()summary()duplicated()colSum(is.na())spec())\n\n\n\n\nCode\nglimpse(heart_attack_2)\n\n\nRows: 30,000\nColumns: 25\n$ Age                        &lt;dbl&gt; 56, 69, 46, 32, 60, 25, 78, 38, 56, 75, 36,…\n$ Gender                     &lt;chr&gt; \"Male\", \"Male\", \"Male\", \"Female\", \"Female\",…\n$ Region                     &lt;chr&gt; \"Urban\", \"Urban\", \"Rural\", \"Urban\", \"Rural\"…\n$ Smoking_History            &lt;chr&gt; \"Yes\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"No\",…\n$ Diabetes_History           &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", …\n$ Hypertension_History       &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", …\n$ Cholesterol_Level          &lt;dbl&gt; 186.4002, 185.1367, 210.6966, 211.1655, 223…\n$ Physical_Activity          &lt;chr&gt; \"Moderate\", \"Low\", \"Low\", \"Moderate\", \"High…\n$ Diet_Quality               &lt;chr&gt; \"Poor\", \"Good\", \"Average\", \"Good\", \"Good\", …\n$ Alcohol_Consumption        &lt;chr&gt; \"Low\", \"Low\", \"Moderate\", \"High\", \"High\", \"…\n$ Stress_Levels              &lt;dbl&gt; 3.644786, 3.384056, 3.810911, 6.014878, 6.8…\n$ BMI                        &lt;dbl&gt; 33.96135, 28.24287, 27.60121, 23.71729, 19.…\n$ Heart_Rate                 &lt;dbl&gt; 72.30153, 57.45764, 64.65870, 55.13147, 76.…\n$ Systolic_BP                &lt;dbl&gt; 123.90209, 129.89331, 145.65490, 131.78522,…\n$ Diastolic_BP               &lt;dbl&gt; 85.68281, 73.52426, 71.99481, 68.21133, 92.…\n$ Family_History             &lt;chr&gt; \"No\", \"Yes\", \"No\", \"No\", \"No\", \"No\", \"No\", …\n$ Heart_Attack_Occurrence    &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"…\n$ Cholesterol_Level_Category &lt;chr&gt; \"Moderate\", \"Moderate\", \"High\", \"High\", \"Hi…\n$ Rounded_Stress_Levels      &lt;dbl&gt; 4, 3, 4, 6, 7, 8, 5, 5, 7, 4, 3, 2, 2, 3, 8…\n$ Stress_Levels_Category     &lt;chr&gt; \"Moderate_Stress\", \"Low_Stress\", \"Moderate_…\n$ BMI_Category               &lt;chr&gt; \"Obese\", \"Overweight\", \"Overweight\", \"Norma…\n$ Heart_Rate_Category        &lt;chr&gt; \"Normal\", \"Low\", \"Normal\", \"Low\", \"Normal\",…\n$ Systolic_BP_Category       &lt;chr&gt; \"Elevated\", \"Elevated\", \"Hypertension_Stage…\n$ Diastolic_BP_Category      &lt;chr&gt; \"Elevated\", \"Normal\", \"Normal\", \"Normal\", \"…\n$ Age_Category               &lt;chr&gt; \"Older Adult\", \"Older Adult\", \"Middle-Aged …\n\n\n\n\n\n\nCode\nhead(heart_attack_2)\n\n\n# A tibble: 6 × 25\n    Age Gender Region Smoking_History Diabetes_History Hypertension_History\n  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;           &lt;chr&gt;            &lt;chr&gt;               \n1    56 Male   Urban  Yes             No               No                  \n2    69 Male   Urban  No              No               No                  \n3    46 Male   Rural  Yes             No               No                  \n4    32 Female Urban  No              No               No                  \n5    60 Female Rural  No              No               No                  \n6    25 Female Rural  No              No               No                  \n# ℹ 19 more variables: Cholesterol_Level &lt;dbl&gt;, Physical_Activity &lt;chr&gt;,\n#   Diet_Quality &lt;chr&gt;, Alcohol_Consumption &lt;chr&gt;, Stress_Levels &lt;dbl&gt;,\n#   BMI &lt;dbl&gt;, Heart_Rate &lt;dbl&gt;, Systolic_BP &lt;dbl&gt;, Diastolic_BP &lt;dbl&gt;,\n#   Family_History &lt;chr&gt;, Heart_Attack_Occurrence &lt;chr&gt;,\n#   Cholesterol_Level_Category &lt;chr&gt;, Rounded_Stress_Levels &lt;dbl&gt;,\n#   Stress_Levels_Category &lt;chr&gt;, BMI_Category &lt;chr&gt;,\n#   Heart_Rate_Category &lt;chr&gt;, Systolic_BP_Category &lt;chr&gt;, …\n\n\n\n\n\n\nCode\nsummary(heart_attack_2)\n\n\n      Age           Gender             Region          Smoking_History   \n Min.   :18.00   Length:30000       Length:30000       Length:30000      \n 1st Qu.:33.00   Class :character   Class :character   Class :character  \n Median :48.00   Mode  :character   Mode  :character   Mode  :character  \n Mean   :48.49                                                           \n 3rd Qu.:64.00                                                           \n Max.   :79.00                                                           \n Diabetes_History   Hypertension_History Cholesterol_Level Physical_Activity \n Length:30000       Length:30000         Min.   : 80.02    Length:30000      \n Class :character   Class :character     1st Qu.:179.55    Class :character  \n Mode  :character   Mode  :character     Median :199.77    Mode  :character  \n                                         Mean   :199.90                      \n                                         3rd Qu.:220.16                      \n                                         Max.   :336.86                      \n Diet_Quality       Alcohol_Consumption Stress_Levels         BMI       \n Length:30000       Length:30000        Min.   : 0.000   Min.   : 5.58  \n Class :character   Class :character    1st Qu.: 3.644   1st Qu.:21.63  \n Mode  :character   Mode  :character    Median : 4.993   Median :24.96  \n                                        Mean   : 5.002   Mean   :25.00  \n                                        3rd Qu.: 6.353   3rd Qu.:28.36  \n                                        Max.   :10.000   Max.   :46.10  \n   Heart_Rate      Systolic_BP      Diastolic_BP    Family_History    \n Min.   : 30.03   Min.   : 56.23   Min.   : 39.95   Length:30000      \n 1st Qu.: 63.25   1st Qu.:109.79   1st Qu.: 73.26   Class :character  \n Median : 69.95   Median :119.90   Median : 80.12   Mode  :character  \n Mean   : 69.98   Mean   :119.91   Mean   : 80.03                     \n 3rd Qu.: 76.66   3rd Qu.:130.02   3rd Qu.: 86.76                     \n Max.   :108.78   Max.   :178.77   Max.   :117.67                     \n Heart_Attack_Occurrence Cholesterol_Level_Category Rounded_Stress_Levels\n Length:30000            Length:30000               Min.   : 0.000       \n Class :character        Class :character           1st Qu.: 4.000       \n Mode  :character        Mode  :character           Median : 5.000       \n                                                    Mean   : 4.998       \n                                                    3rd Qu.: 6.000       \n                                                    Max.   :10.000       \n Stress_Levels_Category BMI_Category       Heart_Rate_Category\n Length:30000           Length:30000       Length:30000       \n Class :character       Class :character   Class :character   \n Mode  :character       Mode  :character   Mode  :character   \n                                                              \n                                                              \n                                                              \n Systolic_BP_Category Diastolic_BP_Category Age_Category      \n Length:30000         Length:30000          Length:30000      \n Class :character     Class :character      Class :character  \n Mode  :character     Mode  :character      Mode  :character  \n                                                              \n                                                              \n                                                              \n\n\n\n\n\n\nCode\nheart_attack_2[duplicated(heart_attack_2),]\n\n\n# A tibble: 0 × 25\n# ℹ 25 variables: Age &lt;dbl&gt;, Gender &lt;chr&gt;, Region &lt;chr&gt;, Smoking_History &lt;chr&gt;,\n#   Diabetes_History &lt;chr&gt;, Hypertension_History &lt;chr&gt;,\n#   Cholesterol_Level &lt;dbl&gt;, Physical_Activity &lt;chr&gt;, Diet_Quality &lt;chr&gt;,\n#   Alcohol_Consumption &lt;chr&gt;, Stress_Levels &lt;dbl&gt;, BMI &lt;dbl&gt;,\n#   Heart_Rate &lt;dbl&gt;, Systolic_BP &lt;dbl&gt;, Diastolic_BP &lt;dbl&gt;,\n#   Family_History &lt;chr&gt;, Heart_Attack_Occurrence &lt;chr&gt;,\n#   Cholesterol_Level_Category &lt;chr&gt;, Rounded_Stress_Levels &lt;dbl&gt;, …\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nEnsure that there are no duplicated columns, if not will have to investigate further.\n\n\n\n\n\n\n\nCode\ncolSums(is.na(heart_attack_2))\n\n\n                       Age                     Gender \n                         0                          0 \n                    Region            Smoking_History \n                         0                          0 \n          Diabetes_History       Hypertension_History \n                         0                          0 \n         Cholesterol_Level          Physical_Activity \n                         0                          0 \n              Diet_Quality        Alcohol_Consumption \n                         0                          0 \n             Stress_Levels                        BMI \n                         0                          0 \n                Heart_Rate                Systolic_BP \n                         0                          0 \n              Diastolic_BP             Family_History \n                         0                          0 \n   Heart_Attack_Occurrence Cholesterol_Level_Category \n                         0                          0 \n     Rounded_Stress_Levels     Stress_Levels_Category \n                         0                          0 \n              BMI_Category        Heart_Rate_Category \n                         0                          0 \n      Systolic_BP_Category      Diastolic_BP_Category \n                         0                          0 \n              Age_Category \n                         0 \n\n\n\n\n\n\n\n\nNote\n\n\n\n\nEnsure that there are no NA values, if not will have to investigate further.\nPossibility to use drop_na() function to drop rows where any specified column contains a missing value.\n\n\n\n\n\n\n\nCode\nspec(heart_attack_2)\n\n\nNULL\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nEnsure that all variables are correctly classified by data type; recast variable types if needed.\nVariables are correctly classified - where categorical variables are classified as character, while continuous variables are classified as double.\n\n\n\n\n\n\n\n\n\n\n\n\nPreprocessing Checks\n\n\n\n\nVerified that the correct data types were loaded in the heart_attack dataset using glimpse() and spec()\nEnsured there were no duplicate variable names using duplicated() in the dataset\nChecked for missing values using colSums(is.na())\n\n\n\nThe final output - heart_attack_2 shows a tibble dataframe with 30,000 rows and 17 columns.\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nObservations\n\nUneven representation of age groups\n\nNoticeably lower number of Youth (≤25 years) and Elderly (≥71 years) compared to the middle age groups\nYoung adults (26–40 years), middle-aged adults (41–55 years), and older adults (56–70 years) make up the majority of the dataset\n\nPotential bias in data collection\n\nUnderrepresentation of elderly individuals might inflate their heart attack rate in percentage-based analyses\nLow count of youth suggests they may be under-sampled\n\n\nWhat can we do:\n\nIn the future, ensure that the data collected are proportionally across all age groups\nCheck for potential sampling bias\nNormalize the heart attack rates based on the total population for each age group\n\n\n\n\nAbsolute age count()Visualization: Absolute age count()Count by Age_Category()Visualization: Age_Category()\n\n\n\n\nCode\n# Count the number of individuals by exact age\nage_distribution &lt;- heart_attack_2 %&gt;%\n  group_by(Age) %&gt;%\n  summarise(Count = n()) %&gt;%\n  arrange(Age)\n\n# Print age count\nprint(age_distribution)\n\n\n# A tibble: 62 × 2\n     Age Count\n   &lt;dbl&gt; &lt;int&gt;\n 1    18   478\n 2    19   480\n 3    20   483\n 4    21   483\n 5    22   468\n 6    23   462\n 7    24   466\n 8    25   493\n 9    26   480\n10    27   493\n# ℹ 52 more rows\n\n\n\n\n\n\nCode\nggplot(age_distribution, aes(x = Age, y = Count)) +\n  geom_bar(stat = \"identity\", fill = \"#78B3EA\", color = \"black\", alpha = 0.7) +\n  labs(title = \"Age Distribution of Individuals in Dataset\",\n       x = \"Age\",\n       y = \"Number of Individuals\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Define age category levels in the required order\nage_levels &lt;- c(\n  \"Youth (≤25 years)\",\n  \"Young adults (26–40 years)\",\n  \"Older adults (56–70 years)\",\n  \"Middle-aged adults (41–55 years)\",\n  \"Elderly (≥71 years)\"\n)\n\n# Count individuals by Age Category with ordered factor levels\nage_category_distribution &lt;- heart_attack_2 %&gt;%\n  mutate(Age_Category = case_when(\n    Age &lt;= 25 ~ \"Youth (≤25 years)\",\n    Age &gt;= 26 & Age &lt;= 40 ~ \"Young adults (26–40 years)\",\n    Age &gt;= 41 & Age &lt;= 55 ~ \"Middle-aged adults (41–55 years)\",\n    Age &gt;= 56 & Age &lt;= 70 ~ \"Older adults (56–70 years)\",\n    Age &gt;= 71 ~ \"Elderly (≥71 years)\",\n    TRUE ~ \"Unknown\"\n  )) %&gt;%\n  group_by(Age_Category) %&gt;%\n  summarise(Count = n()) %&gt;%\n  mutate(Age_Category = factor(Age_Category, levels = age_levels)) %&gt;%  # Apply custom sorting\n  arrange(Age_Category)\n\n# Print age category count\nprint(age_category_distribution)\n\n\n# A tibble: 5 × 2\n  Age_Category                     Count\n  &lt;fct&gt;                            &lt;int&gt;\n1 Youth (≤25 years)                 3813\n2 Young adults (26–40 years)        7432\n3 Older adults (56–70 years)        7201\n4 Middle-aged adults (41–55 years)  7153\n5 Elderly (≥71 years)               4401\n\n\n\n\n\n\nCode\n# Visualization: Age Category Distribution\nggplot(age_category_distribution, aes(x = Age_Category, y = Count, fill = Age_Category)) +\n  geom_bar(stat = \"identity\", color = \"black\", alpha = 0.7) +\n  \n  # Add text labels above bars\n  geom_text(aes(label = Count), vjust = -0.5, color = \"black\", size = 2.5) +\n  \n  labs(title = \"Age Category Distribution in Dataset\",\n       x = \"Age Category\",\n       y = \"Number of Individuals\") +\n  \n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set2\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations:\n\nHeart attacks occur across all ages:\n\nHistogram shows a wide distribution of heart attack patients across various age groups\n\nAge group variation in heart attack rates:\n\nHeart attack rate appears to be relatively stable across most age bins, ranging between 8.6% to 11.3%.\nHighest normalized heart attack rate is in the [78,83] age bin (~11.3%).\n\nGradual increase in heart attack rates:\n\nGradual increase in heart attack rates between younger groups [18,23] to middle-aged adults [38,43]\n\n\n\nGraph()Code()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe cut()function was used to divide a continuous variable (like Age) into discrete intervals such as bins.\ncut() function is part of Base R, which does not require any additional packages to use.\n\n\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Step 1: Remove any NA values in Age before binning\nheart_attack_2 &lt;- heart_attack_2 %&gt;%\n  filter(!is.na(Age)) %&gt;%  # Remove missing Age values\n  mutate(Age_Bin = cut(Age, \n                       breaks = seq(floor(min(Age, na.rm = TRUE)), \n                                    ceiling(max(Age, na.rm = TRUE)) + 5,  \n                                    by = 5), \n                       right = FALSE))  \n\n# Step 2: Count total individuals per age bin\ntotal_population &lt;- heart_attack_2 %&gt;%\n  group_by(Age_Bin) %&gt;%\n  summarise(Total_Count = n(), .groups = \"drop\")\n\n# Step 3: Count heart attack occurrences per age bin\nheart_attack_counts &lt;- heart_attack_2 %&gt;%\n  filter(Heart_Attack_Occurrence == \"Yes\") %&gt;%\n  group_by(Age_Bin) %&gt;%\n  summarise(Heart_Attack_Count = n(), .groups = \"drop\")\n\n# Step 4: Merge the two tables and replace NA values with 0\nnormalized_data &lt;- left_join(total_population, heart_attack_counts, by = \"Age_Bin\") %&gt;%\n  mutate(Heart_Attack_Count = replace_na(Heart_Attack_Count, 0),  # Replace NA counts with 0\n         Normalized_Heart_Attack_Rate = (Heart_Attack_Count / Total_Count) * 100) %&gt;%\n  filter(!is.na(Age_Bin))  # Ensure Age_Bin does not contain NA values\n\n# Step 5: Create the normalized histogram\nggplot(normalized_data, aes(x = Age_Bin, y = Normalized_Heart_Attack_Rate, fill = Age_Bin)) +\n  geom_bar(stat = \"identity\", color = \"black\", alpha = 0.7) +\n  \n  # Add text labels showing the percentage per bin\n  geom_text(aes(label = sprintf(\"%.1f%%\", Normalized_Heart_Attack_Rate)), vjust = -0.5, color = \"black\") +\n  \n  labs(title = \"Normalized Age Distribution of Heart Attack Patients\",\n       x = \"Age Groups (Binned by 5 Years)\",\n       y = \"Heart Attack Rate (Normalized, % of total in group)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  \n\n\n\n\n\n\n\n\nAnother point of view, categorizing ages into groups and calculating the heart attack rate for each group.\nObservations:\n\nAlmost similar rates across age groups :\n\nHeart attack rates range between 8.9% and 10.3%, showing no extreme variation.\nYoung Adults (26–40) & Elderly (≥71) have the highest rate - at 10.3%\nYouth (≤25) has the lowest rate - at 8.9%, only slightly lower than other groups.\n\nSuggests that age alone may not be the strongest predictor of heart attack risk, and other demographic, lifestyle or health factors might play a key role.\n\n\nGraph()Code()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Define Age Category Labels with Age Ranges\nage_labels &lt;- c(\n  \"Youth (≤25 years)\", \n  \"Young adults (26–40 years)\", \n  \"Middle-aged adults (41–55 years)\", \n  \"Older adults (56–70 years)\", \n  \"Elderly (≥71 years)\"\n)\n\n# Count total individuals per age category\ntotal_population &lt;- heart_attack_2 %&gt;%\n  group_by(Age_Category) %&gt;%\n  summarise(Total_Count = n())\n\n# Count heart attack occurrences per age category\nheart_attack_counts &lt;- heart_attack_2 %&gt;%\n  filter(Heart_Attack_Occurrence == \"Yes\") %&gt;%\n  group_by(Age_Category) %&gt;%\n  summarise(Heart_Attack_Count = n())\n\n# Merge both datasets and calculate the normalized heart attack rate\nnormalized_data &lt;- left_join(heart_attack_counts, total_population, by = \"Age_Category\") %&gt;%\n  mutate(Normalized_Heart_Attack_Rate = (Heart_Attack_Count / Total_Count) * 100) %&gt;%\n  \n  # Convert Age_Category into a factor with labels\n  mutate(Age_Category = factor(Age_Category, \n                               levels = c(\"Youth\", \"Young Adult\", \"Middle-Aged Adult\", \"Older Adult\", \"Elderly\"), \n                               labels = age_labels))\n\n# Create bar plot with normalized rates\nggplot(normalized_data, aes(x = Age_Category, y = Normalized_Heart_Attack_Rate, fill = Age_Category)) +\n  geom_bar(stat = \"identity\", color = \"black\", alpha = 0.7) +\n  \n  # Add data labels to show percentages\n  geom_text(aes(label = sprintf(\"%.1f%%\", Normalized_Heart_Attack_Rate)), vjust = -0.5, color = \"black\",size = 2.5) +\n  \n  labs(title = \"Normalized Heart Attack Rate by Age Group\",\n       x = \"Age Group\",\n       y = \"Heart Attack Rate (% of total in group)\",\n       fill = \"Age Categories (with Ranges)\") +  # Updated legend title\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set2\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        axis.text.y = element_text(size = 8))  \n\n\n\n\n\n\n\n\nObservations:\n\nHeart attacks occur across all ages\n\nThe density distribution shows that heart attacks (Yes) occur at all age ranges, from young to elderly individuals. Suggests that age alone is not a definitive factor.\n\nSimilar age distribution for both groups\n\nThe Heart Attack Occurrence = “No” group is more evenly distributed across age ranges\nOverall shape of the density curves for individuals who had heart attacks and those who did not appear similar.\n\nNeed for deeper analysis to identify other contributing factors that play a more crucial role.\n\n\nGraph()Code()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Ensure Heart Attack Occurrence is a factor\nheart_attack_2 &lt;- heart_attack_2 %&gt;%\n  mutate(Heart_Attack_Occurrence = factor(Heart_Attack_Occurrence, levels = c(\"No\", \"Yes\")))\n\n# Create the Ridgeline Density Plot \nggplot(heart_attack_2, aes(x = Age, y = Heart_Attack_Occurrence, fill = after_stat(x))) +\n  geom_density_ridges_gradient(scale = 2, rel_min_height = 0.01, show.legend = TRUE) +\n  scale_fill_viridis_c(option = \"magma\") +  \n\n  labs(title = \"Ridgeline Density Plot: Age Distribution by Heart Attack Occurrence\",\n       x = \"Age\",\n       y = \"Heart Attack Occurrence\",\n       fill = \"Age\") +  \n\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\n\n\nThe analysis is based on five age categories, with heart attack occurrence represented as a proportion.\n\n\n\nCategories\nAge\n\n\n\n\nYouth\n≤25 years\n\n\nYoung adults\n26–40 years\n\n\nMiddle-aged adults\n41–55 years\n\n\nOlder adults\n56–70 years\n\n\nElderly\n≥71 years\n\n\n\nObservations:\n\nNo overdispersion was detected, meaning the data does not exhibit excessive variance beyond what is expected.\nThe funnel plot confirms that all data points fall within expected control limits, indicating no statistically significant outliers.\nAlthough, the heart attack occurrence rate varies among age categories but remains within the expected range.\n\nFurther analysis might be needed to examine specific risk factors that contribute to heart attack rates in different age groups.\n\nGraph()Code()\n\n\n\n\n\n\n\n\n\n\n\nA funnel plot object with 5 points of which 0 are outliers. \nPlot is not adjusted for overdispersion. \n\n\n\n\n\n\nCode\n# Step 1: Convert \"Yes\"/\"No\" to 1/0\nheart_attack_2 &lt;- heart_attack_2 %&gt;%\n  mutate(Heart_Attack_Occurrence = ifelse(Heart_Attack_Occurrence == \"Yes\", 1, 0))\n\n# Step 2: Summarize data for all 5 age categories\nage_summary &lt;- heart_attack_2 %&gt;%\n  group_by(Age_Category) %&gt;%\n  summarise(\n    Heart_Attack_Count = sum(Heart_Attack_Occurrence, na.rm = TRUE),  # Numerator\n    Total_Population = n()  # Denominator\n  )\n\n# Step 3: Funnel Plot for all age categories\nfunnel_plot(\n  .data = age_summary,\n  numerator = \"Heart_Attack_Count\",\n  denominator = \"Total_Population\",\n  group = \"Age_Category\",\n  data_type = \"PR\",  # Proportion Rate\n  x_range = c(0, max(age_summary$Total_Population) + 500),  \n  y_range = c(0, max(age_summary$Heart_Attack_Count / age_summary$Total_Population) + 0.02) \n)\n\n\n\n\n\n\n\n\n\nFrom the above analysis, it suggests that age alone may not be the strongest predictor of heart attack risk, other factors might play a key role.\nIn the following analysis, we will examine the strongest predictors of heart attack incidents based on two key factor types: Demographic and Lifestyle/ Health.\n\nDemographic variables are inherent characteristics that define a person but are not directly influence by behavior or lifestyle\nLifestyle/ health variables are a result from both genetics and lifestyle factors\n\n\n\n\n\n\n\n\nFactor types\nVariables to consider\n\n\n\n\nDemographic\nGender, Region, Family_History\n\n\nLifestyle/ Health\nSmoking_History, Alcohol_Consumption, Physical_Activity, Diet_Quality, Stress_Levels_Category, Diabetes_History, Hypertension_History, Cholesterol_Level, BMI, Heart_Rate, Systolic_BP, Diastolic_BP\n\n\n\n\n\nObservations:\n\nNo statistically significant relationship was found between Gender, Region, or Family History and heart attack occurrence (p-values &gt; 0.05).\nSince all p-values are above 0.05, demographic factors alone do not play a significant role in determining heart attack risk\n\n\nChi-test()Code()\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nGender (p = 0.0758) is the closest to significance, suggesting a potential but weak association\nRegion (p = 0.9890) and Family History (p = 0.8531) show almost no correlation with heart attack occurrence, indicating that location and family history may not be strong predictors.\n\n\n\n\n\n                      Factor   Chi_Square    p_value\nX-squared...1         Gender 3.1522718581 0.07582133\nX-squared...2         Region 0.0001911346 0.98896948\nX-squared...3 Family_History 0.0342903979 0.85309047\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\n\n# Select demographic factors\ndemographic_factors &lt;- heart_attack_2 %&gt;%\n  select(Gender, Region, Family_History, Heart_Attack_Occurrence) %&gt;% \n  mutate(across(everything(), as.factor))  # Ensure all categorical variables are factors\n\n# Function to perform chi-square test for each demographic factor\nperform_chi_test &lt;- function(var) {\n  contingency_table &lt;- table(demographic_factors[[var]], demographic_factors$Heart_Attack_Occurrence)\n  test_result &lt;- chisq.test(contingency_table)\n  return(data.frame(\n    Factor = var,\n    Chi_Square = test_result$statistic,\n    p_value = test_result$p.value\n  ))\n}\n\n# Apply the chi-square test to all demographic variables\nchi_test_results &lt;- map_df(c(\"Gender\", \"Region\", \"Family_History\"), perform_chi_test)\n\n# Display chi-square test results\nprint(chi_test_results)\n\n\n\n\n\n\n\nDrawing inference from the half-eye plot with boxplot which visualizes the distribution of age across demographic factors for heart attack cases.\nObservations:\n\nDistributions across demographics look similar\n\nAge distributions for different demographic categories overlap significantly. There is no obvious skew or deviations which suggests a particular demographic factor influence heart attack risk\n\nBoxplots show similar medians & spreads\n\nMedians for each group are relatively aligned, the spread (IQR) across categories does not indicate a distinct outlier effect.\n\nThe statistical evidence (Chi-Square Test) aligns with the visual analysis (Half-Eye Plot).\n\n\nGraphCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(ggdist)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Prepare Data: Convert demographic factors to long format & Filter for \"Yes\"\nheart_attack_long &lt;- heart_attack_2 %&gt;%\n  select(Age, Gender, Region, Family_History, Heart_Attack_Occurrence) %&gt;%\n  filter(Heart_Attack_Occurrence == \"1\") %&gt;%  # 🔹 Keep only heart attack cases\n  pivot_longer(cols = c(Gender, Region, Family_History), \n               names_to = \"Demographic_Factor\", \n               values_to = \"Category\") %&gt;%\n  mutate(Age = as.numeric(Age),\n         Heart_Attack_Occurrence = as.factor(Heart_Attack_Occurrence))\n\n# Half-Eye Plot with Boxplot Overlay (Filtered)\nggplot(heart_attack_long, aes(x = Age, \n                              y = interaction(Demographic_Factor, Category), \n                              fill = Category)) +  # 🔹 Change fill to Category for demographic focus\n  stat_halfeye(adjust = 0.5, justification = -0.2,  \n               .width = c(0.5, 0.8, 0.95),  \n               slab_alpha = 0.8, \n               point_colour = NA) +  # Remove points from the half-eye plot\n  geom_boxplot(width = 0.20, \n               outlier.shape = NA, \n               alpha = 0.6) +  # Add boxplot without outliers\n  scale_fill_brewer(palette = \"Set2\", name = \"Demographic Category\") +  # 🔹 Different colors for categories\n  theme_minimal() +\n  labs(title = \"Half-Eye Plot with Boxplot: Age Distribution for Heart Attack Cases\",\n       x = \"Age\", y = \"Demographic Categories\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe below are the steps taken to prepare the dataset for logistic regression. The key lifestyle/ health factors were selected, where categorical variables were encoded, and ordinal scales variables were converted to numeric values.\n\nTableCode\n\n\n\n\n\n\n\n\n\n\nVariable Names\nType (Ordinal/ Binary)\nValues\n\n\n\n\nGender\nBinary\nMale, Female\n\n\nRegion\nBinary\nRural, Urban\n\n\nSmoking_History\nBinary\nYes = 1, No = 0\n\n\nDiabetes_History\nBinary\nYes = 1, No = 0\n\n\nHypertension_History\nBinary\nYes = 1, No = 0\n\n\nPhysical_Activity\nOrdinal\nLow = 0, Moderate = 1, High = 2\n\n\nDiet_Quality\nOrdinal\nPoor = 0, Average = 1, Good = 2\n\n\nAlcohol_Consumption\nOrdinal\nNone = 0, Low = 1, Moderate = 2, High = 3\n\n\nFamily_History\nBinary\nYes = 1, No = 0\n\n\nCholesterol_Level_Category\nOrdinal\nLow = 0, Moderate = 1, High = 2\n\n\nStress_Levels_Category\nOrdinal\nMinimal Stress = 0, Low Stress = 1, Moderate Stress = 2, High Stress = 3\n\n\nBMI_Category\nOrdinal\nUnderweight = 0, Normal Weight = 1, Overweight = 2, Obese = 3\n\n\nHeart_Rate_Category\nOrdinal\nLow = 0, Normal = 1, High = 2\n\n\nSystolic_BP_Category\nOrdinal\nNormal = 0, Elevated = 1, Hypertension Stage 1 = 2, Hypertension Stage 2 = 3\n\n\nDiastolic_BP_Category\nOrdinal\nNormal = 0, Elevated = 1, Hypertension Stage 1 = 2, Hypertension Stage 2 = 3\n\n\nHeart_Attack_Occurrence\nBinary\nYes = 1, No = 0\n\n\n\n\n\n\n\nCode\n# Select relevant health-related columns\nrelevant_factors &lt;- heart_attack_2 %&gt;% \n  select(\"Gender\", \"Region\", \"Smoking_History\", \"Diabetes_History\", \"Hypertension_History\", \"Physical_Activity\",\n         \"Diet_Quality\", \"Alcohol_Consumption\", \"Family_History\", \"Cholesterol_Level_Category\", \"Heart_Attack_Occurrence\", \n         \"Stress_Levels_Category\", \"BMI_Category\", \"Heart_Rate_Category\", \"Systolic_BP_Category\", \"Diastolic_BP_Category\")\n\n# Convert categorical variables\n# Gender encoding\nrelevant_factors$Gender &lt;- factor(relevant_factors$Gender, levels = c(\"Male\", \"Female\"))\n\n# Region encoding\nrelevant_factors$Region &lt;- factor(relevant_factors$Region, levels = c(\"Rural\", \"Urban\"))\n\n# Smoking_History encoding\nrelevant_factors$Smoking_History &lt;- factor(relevant_factors$Smoking_History, levels = c(\"Yes\", \"No\"))\n\n# Diabetes_History encoding\nrelevant_factors$Diabetes_History &lt;- factor(relevant_factors$Diabetes_History, levels = c(\"Yes\", \"No\"))\n\n# Hypertension_History encoding\nrelevant_factors$Hypertension_History &lt;- factor(relevant_factors$Hypertension_History, levels = c(\"Yes\", \"No\"))\n\n# Physical_Activity is now numeric (Low = 0, Moderate = 1, High = 2)\nrelevant_factors$Physical_Activity &lt;- recode(relevant_factors$Physical_Activity, \"Low\" = 0, \"Moderate\" = 1, \"High\" = 2) %&gt;% as.numeric()\n\n# Diet_Quality is now numeric (Poor = 0, Average = 1, Good = 2)\nrelevant_factors$Diet_Quality &lt;- recode(relevant_factors$Diet_Quality, \"Poor\" = 0, \"Average\" = 1, \"Good\" = 2) %&gt;% as.numeric()\n\n# Alcohol_Consumption is now numeric (None = 0, Low = 1, Moderate = 2, High = 3)\nrelevant_factors$Alcohol_Consumption &lt;- recode(relevant_factors$Alcohol_Consumption, \"None\" = 0, \"Low\" = 1, \"Moderate\" = 2, \"High\" = 3) %&gt;% as.numeric()\n\n# Family_History encoding\nrelevant_factors$Family_History &lt;- factor(relevant_factors$Family_History, levels = c(\"Yes\", \"No\"))\n\n# Cholesterol_Level_Category is now numeric (Low = 0, Moderate = 1, High = 2)\nrelevant_factors$Cholesterol_Level_Category &lt;- recode(relevant_factors$Cholesterol_Level_Category, \"Low\" = 0, \"Moderate\" = 1, \"High\" = 2) %&gt;% as.numeric()\n\n# Stress_Levels_Category is now numeric (Minimal Stress = 0, Low Stress = 1, Moderate Stress = 2, High Stress = 3)\nrelevant_factors$Stress_Levels_Category &lt;- recode(relevant_factors$Stress_Levels_Category,\n  \"Miniminal_Stress\" = 0,\n  \"Low_Stress\" = 1,\n  \"Moderate_Stress\" = 2,\n  \"High_Stress\" = 3\n) %&gt;% as.numeric()\n\n# BMI_Category is now numeric (Underweight = 0, Normal_Weight = 1, Overweight = 2, Obese = 3)\nrelevant_factors$BMI_Category &lt;- recode(relevant_factors$BMI_Category, \"Underweight\" = 0, \"Normal_Weight\" = 1, \"Overweight\" = 2, \"Obese\" = 3) %&gt;% as.numeric()\n\n# Heart_Rate_Category is now numeric (Low = 0, Normal = 1, High = 2)\nrelevant_factors$Heart_Rate_Category &lt;- recode(relevant_factors$Heart_Rate_Category, \"Low\" = 0, \"Normal\" = 1, \"High\" = 2) %&gt;% as.numeric()\n\n# Systolic_BP_Category is now numeric (Normal = 0, Elevated = 1, Hypertension_Stage_1 = 2, Hypertension_Stage_2 = 3)\nrelevant_factors$Systolic_BP_Category &lt;- recode(relevant_factors$Systolic_BP_Category, \"Normal\" = 0, \"Elevated\" = 1, \"Hypertension_Stage_1\" = 2, \"Hypertension_Stage_2\" = 3) %&gt;% as.numeric()\n\n# Diastolic_BP_Category is now numeric (Normal = 0, Elevated = 1, Hypertension_Stage_1 = 2, Hypertension_Stage_2 = 3)\nrelevant_factors$Diastolic_BP_Category &lt;- recode(relevant_factors$Diastolic_BP_Category, \"Normal\" = 0, \"Elevated\" = 1, \"Hypertension_Stage_1\" = 2, \"Hypertension_Stage_2\" = 3) %&gt;% as.numeric()\n\n# Convert Heart_Attack_Occurrence to binary and ensure it remains a factor\nrelevant_factors$Heart_Attack_Occurrence &lt;- factor(ifelse(relevant_factors$Heart_Attack_Occurrence == \"Yes\", 1, 0), levels = c(0, 1))\n\n\n\n\n\n\n\n\nThe code below is used to calibrate a multiple linear regression model by using glm() of Base Stats of R.\n\nResultCode\n\n\n\n\n\nCall:  glm(formula = Heart_Attack_Occurrence ~ ., family = binomial(), \n    data = relevant_factors)\n\nCoefficients:\n               (Intercept)                GenderFemale  \n                -2.657e+01                  -1.461e-13  \n               RegionUrban           Smoking_HistoryNo  \n                 1.057e-13                  -2.443e-13  \n        Diabetes_HistoryNo      Hypertension_HistoryNo  \n                 9.216e-14                   9.946e-14  \n         Physical_Activity                Diet_Quality  \n                 1.453e-15                  -1.592e-13  \n       Alcohol_Consumption            Family_HistoryNo  \n                -6.374e-14                   1.125e-13  \nCholesterol_Level_Category      Stress_Levels_Category  \n                -1.047e-13                   2.965e-14  \n              BMI_Category         Heart_Rate_Category  \n                 1.425e-13                   8.528e-14  \n      Systolic_BP_Category       Diastolic_BP_Category  \n                 1.136e-14                   3.630e-14  \n\nDegrees of Freedom: 29999 Total (i.e. Null);  29984 Residual\nNull Deviance:      0 \nResidual Deviance: 1.74e-07     AIC: 32\n\n\n\n\n\n\nCode\n# Run Logistic Regression Model\nlogistic_model &lt;- glm(Heart_Attack_Occurrence ~ ., data = relevant_factors, family = binomial())\n\n# Print Model Summary\nlogistic_model\n\n\n\n\n\n\n\n\nIn the code below, we use check_collinearity() of performance package to check for multicolinearity.\nObservations:\n\nLow Multicollinearity – VIF values are all ~1.00, indicating minimal correlation among independent variables.\nNo Immediate Need for Variable Removal – Since there is no strong multicollinearity, all predictors can remain in the model.\n\n\nResultCode\n\n\n\n\n# Check for Multicollinearity\n\nLow Correlation\n\n                       Term  VIF       VIF 95% CI Increased SE Tolerance\n                     Gender 1.00 [1.00, 14468.50]         1.00      1.00\n                     Region 1.00 [1.00, 8.76e+11]         1.00      1.00\n            Smoking_History 1.00 [1.00,      Inf]         1.00      1.00\n           Diabetes_History 1.00 [1.00, 95895.49]         1.00      1.00\n       Hypertension_History 1.00 [1.00, 1.72e+06]         1.00      1.00\n          Physical_Activity 1.00 [1.00, 2.28e+09]         1.00      1.00\n               Diet_Quality 1.00 [1.00, 1.15e+09]         1.00      1.00\n        Alcohol_Consumption 1.00 [1.00,      Inf]         1.00      1.00\n             Family_History 1.00 [1.00, 8.30e+06]         1.00      1.00\n Cholesterol_Level_Category 1.00 [1.00,      Inf]         1.00      1.00\n     Stress_Levels_Category 1.00 [1.00, 6.32e+12]         1.00      1.00\n               BMI_Category 1.00 [1.00, 3.32e+08]         1.00      1.00\n        Heart_Rate_Category 1.00 [1.00, 1.16e+11]         1.00      1.00\n       Systolic_BP_Category 1.00 [1.00, 9.06e+06]         1.00      1.00\n      Diastolic_BP_Category 1.00 [1.00, 1.13e+15]         1.00      1.00\n Tolerance 95% CI\n     [0.00, 1.00]\n     [0.00, 1.00]\n     [0.00, 1.00]\n     [0.00, 1.00]\n     [0.00, 1.00]\n     [0.00, 1.00]\n     [0.00, 1.00]\n     [0.00, 1.00]\n     [0.00, 1.00]\n     [0.00, 1.00]\n     [0.00, 1.00]\n     [0.00, 1.00]\n     [0.00, 1.00]\n     [0.00, 1.00]\n     [0.00, 1.00]\n\n\n\n\n\n\nCode\ncheck_collinearity(logistic_model)\n\n\n\n\n\n\n\n\nObservations:\n\nLow Correlations Between Variables\n\nCorrelation values range between -0.01 and 0.01, indicating very weak relationships.\nNo strong correlations suggest that multicollinearity is not a major issue\nNo predictor strongly influences another, meaning variables are mostly independent in the model, to include all variables in regression model\n\n\n\nGraphCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Generate correlation matrix plot \nggstatsplot::ggcorrmat(\n  data = relevant_factors, \n  cor.vars = 1:ncol(relevant_factors), \n  ggcorrplot.args = list(\n    outline.color = \"blue\",  # Add black outline for clarity\n    hc.order = TRUE,          # Hierarchical clustering to group correlated variables\n    tl.cex = 10               # Adjust text size for readability\n  ),\n  title    = \"Correlogram for Heart Attack Risk Factors\",\n  subtitle = \"Significance level: p &lt; 0.05\"\n)\n\n\n\n\n\n\n\n\n\nStress Levels Category is the only statistically significant variable (p = 0.0273).\nGender (Female) is the next most significant (p = 0.0666), although it is not below the traditional 0.05 threshold, it is marginally significant (p &lt; 0.1).\nAll other variables (Hypertension, Smoking, Diabetes, etc.) have p-values &gt; 0.1, making them far less significant.\n\n\nGraphCode\n\n\n\n\n\n\n\nCode\nsummary(logistic_model)\n\n\n\n\n\n\n\n\n\nGraphCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggcoefstats(logistic_model, \n            output = \"plot\")\n\n\n\n\n\n\n\n\n\nObservations:\n\nStepwise selection has removed many non-significant variables, leaving Gender, Cholesterol Levels, Stress Levels, and BMI Category as predictors.\nThis model has a lower AIC (19337), suggesting better fit compared to previous models.\n\n\nResultCode - Iteration of stepwise selection\n\n\n\n\n\n\n\nCode\n# Fit logistic regression model with all predictors + Stress Levels interactions\nfull_model &lt;- glm(Heart_Attack_Occurrence ~ . + Stress_Levels_Category * Hypertension_History, \n                  family = binomial(), data = relevant_factors)\n\n# Check significance of all predictors\nsummary(full_model)\n\nlibrary(MASS)\noptimized_model &lt;- stepAIC(full_model, direction = \"both\")\nsummary(optimized_model)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nStatistically significant variables table from each model:\n\n\n\n\n\n\n\nModel\nStatistically significant\n\n\n\n\nLogistic regression\nStress Levels Category, and Gender\n\n\nStepwise selection\nGender, Cholesterol Levels, Stress Levels Category, BMI Category\n\n\n\n\n\n\n\n\n\n\n\n\nBased on the statistically significant variables identified through logistic regression and stepwise selection — Gender, Cholesterol Levels, Stress Levels Category, and BMI Category — the following analysis focuses on individuals aged 26 to 40 years.\nThis analysis focuses on young adults (26–40 years), as they have the highest normalized heart attack rate (10.3%). We further filter for urban residents with hypertension history, as urban lifestyles and hypertension are key risk factors of heart attack occurence. By comparing males and females, we aim to assess whether stress and cholesterol levels contribute differently to heart attack risk across genders.\nObservations:\n\nWeak correlation between stress and cholesterol levels for both genders; regression lines are nearly flat.\nGender Differences: Males: Slight positive trend (higher stress → slightly higher cholesterol), greater cholesterol variability. Females: Slight negative trend (higher stress → slightly lower cholesterol).\nHeart Attack Patterns: Cases are evenly spread for males but slightly more frequent at mid-range cholesterol levels for females.\nBoxplots: Males show wider cholesterol variation; stress levels are similar across genders.\nOverall: No strong evidence of a direct link\n\n\nGraph - FemaleCode - FemaleGraph - MaleCode - Male\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(ggExtra)\n\n# Filter dataset: Females only, Age 26-40, Urban region, Underlying hypertension history\nheart_attack_filtered &lt;- heart_attack_3 %&gt;%\n  filter(Gender == \"Female\" & Age &gt;= 26 & Age &lt;= 40 & Region == \"Urban\" & Hypertension_History == \"Yes\") %&gt;%\n  mutate(Heart_Attack_Occurrence = factor(Heart_Attack_Occurrence, levels = c(\"No\", \"Yes\"))) %&gt;%  \n  filter(!is.na(Heart_Attack_Occurrence))  # Remove NA values to fix legend issue\n\n# Create scatter plot (Original Stress Levels vs. Cholesterol Level)\np &lt;- ggplot(heart_attack_filtered, aes(x = Stress_Levels, y = Cholesterol_Level, color = Heart_Attack_Occurrence)) +\n  geom_point(alpha = 0.8, size = 2) +  # Improve visibility\n  geom_smooth(method = \"lm\", se = TRUE, aes(color = Heart_Attack_Occurrence)) +  \n  scale_color_manual(values = c(\"No\" = \"lightpink\", \"Yes\" = \"deeppink\")) +  \n  labs(\n    title = \"Scatter Plot of Cholesterol Level vs Stress Levels\",\n    subtitle = \"Filtered: Females, Age 26-40, Urban, with Hypertension History\",\n    x = \"Stress Levels\",\n    y = \"Cholesterol Level\",\n    color = \"Heart Attack\"\n  ) +\n  theme_minimal()\n\n# Add marginal boxplots\nggMarginal(p, type = \"boxplot\", groupColour = TRUE, groupFill = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(ggExtra)\n\n# Filter dataset: Males only, Age 26-40, Urban region, Underlying hypertension history\nheart_attack_filtered &lt;- heart_attack_3 %&gt;%\n  filter(Gender == \"Male\" & Age &gt;= 26 & Age &lt;= 40 & Region == \"Urban\" & Hypertension_History == \"Yes\") %&gt;%\n  mutate(Heart_Attack_Occurrence = factor(Heart_Attack_Occurrence, levels = c(\"No\", \"Yes\"))) %&gt;%  \n  filter(!is.na(Heart_Attack_Occurrence))  \n\n# Create scatter plot (Original Stress Levels vs. Cholesterol Level)\np &lt;- ggplot(heart_attack_filtered, aes(x = Stress_Levels, y = Cholesterol_Level, color = Heart_Attack_Occurrence)) +\n  geom_point(alpha = 0.8, size = 2) +  \n  geom_smooth(method = \"lm\", se = TRUE, aes(color = Heart_Attack_Occurrence)) +  \n  scale_color_manual(values = c(\"No\" = \"skyblue\", \"Yes\" = \"navy\")) +  \n  labs(\n    title = \"Scatter Plot of Cholesterol Level vs Stress Levels\",\n    subtitle = \"Filtered: Males, Age 26-40, Urban, with Hypertension History\",\n    x = \"Stress Levels\",\n    y = \"Cholesterol Level\",\n    color = \"Heart Attack\"\n  ) +\n  theme_minimal()\n\nggMarginal(p, type = \"boxplot\", groupColour = TRUE, groupFill = TRUE)\n\n\n\n\n\n\n\n\n\nCholesterol levels above 200 are prevalent in heart attack cases.\nNo strong correlation between stress levels and cholesterol, but stress around 5+ is common.\nMajority of heart attack cases cluster around high cholesterol levels (above the 200 threshold).\nStress alone may not be a primary driver, but high cholesterol appears to be a stronger risk factor.\n\n\nGraphCode\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(ggExtra)\nlibrary(ggiraph)  \n\n# Filter dataset: Age 26-40, Urban region, Hypertension History, and Heart Attack = \"Yes\"\nheart_attack_filtered &lt;- heart_attack_3 %&gt;%\n  filter(Age &gt;= 26 & Age &lt;= 40 & Region == \"Urban\" & \n         Hypertension_History == \"Yes\" & Heart_Attack_Occurrence == \"Yes\") %&gt;%  \n  filter(!is.na(Cholesterol_Level))  \n\np &lt;- ggplot(heart_attack_filtered, aes(\n    x = Stress_Levels, \n    y = Cholesterol_Level, \n    color = \"Heart Attack (Yes)\",  # Dummy legend entry\n    tooltip = paste(\"Stress:\", Stress_Levels, \"&lt;br&gt;Cholesterol:\", Cholesterol_Level, \"&lt;br&gt;Heart Attack: Yes\")  \n  )) +\n  geom_point_interactive(alpha = 0.6) +  \n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +  # Regression trendline\n  geom_hline(yintercept = 200, linetype = \"dashed\", color = \"blue\") +  # Horizontal line at Cholesterol = 200\n  geom_vline(xintercept = 5, linetype = \"dotted\", color = \"green\") +   # Vertical line at Stress = 5\n  scale_color_manual(values = c(\"Heart Attack (Yes)\" = \"red\")) +  # Add legend entry\n  labs(\n    title = \"Interactive Scatter Plot of Stress Levels vs. Cholesterol Level\",\n    subtitle = \"Filtered: Age 26-40, Urban, with Hypertension History & Heart Attack Occurrence = Yes\",\n    x = \"Stress Levels\",\n    y = \"Cholesterol Level\",\n    color = \"Heart Attack Status\"  \n  ) +\n  theme_minimal()\n\n# Convert to interactive plot\ninteractive_plot &lt;- girafe(ggobj = p)\n\n# Display the interactive plot\ninteractive_plot\n\n\n\n\n\n\n\n\n\n\n\nAge alone is not the strongest predictor—lifestyle & health factors play a bigger role.\nIn this analysis, high cholesterol levels (&gt;200 mg/dL) are a primary risk factor for heart attacks.\nGender differences exist, but cholesterol remains the dominant factor.\nTargeted interventions should focus on reducing cholesterol and managing hypertension rather than stress alone"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#overview",
    "href": "Take-home_Ex/Take-home_Ex01.html#overview",
    "title": "Take-home Exercise 01",
    "section": "",
    "text": "This analysis explores heart attack incidents in Japan, focusing on the differences in risk factors between youth and adult age groups. With the growing global prevalence of heart disease, understanding how age influences heart attack likelihood and identifying the strongest predictors are crucial for shaping targeted prevention strategies and healthcare interventions.\nThe dataset from Kaggle - Heart Attack in Japan Youth Vs Adult provides an opportunity to analyze these aspects, helping healthcare providers, policymakers, and researchers develop age-specific awareness campaigns, preventive measures, and resource allocation strategies tailored to reducing heart attack risks in Japan.\n\n\n\nUsing the Heart Attack in Japan: Youth vs. Adult dataset, this Take-home_Ex01 applies appropriate Exploratory Data Analysis (EDA) methods, using the tidyverse package and ggplot functions to:\n\nexplore how age influences the likelihood of heart attack incidents\nidentify the strongest predictors contributing to heart attack incidents"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS608 - Visual Analytics and Applications",
    "section": "",
    "text": "The future of analytics"
  },
  {
    "objectID": "index.html#welcome-to-iss608---visual-analytics-and-applications-ay20242025-jan-class",
    "href": "index.html#welcome-to-iss608---visual-analytics-and-applications-ay20242025-jan-class",
    "title": "ISSS608 - Visual Analytics and Applications",
    "section": "Welcome to ISS608 - Visual Analytics and Applications, AY2024/2025 Jan class!",
    "text": "Welcome to ISS608 - Visual Analytics and Applications, AY2024/2025 Jan class!\n\nOverview\nThis page documents my learning journey of creating data visualization beyond default, using R and Tableau.This is part of the ISSS608 course by SMU MITB program, by Prof Kam Tin Seong\nIf you have any feedback or need any clarification, you may contact me at andrea.yeo.2023@mitb.smu.edu.sg\nI hope you enjoy it!\n\n\nLearning journey\nMy journey into programming felt like starring in a drama-comedy series\n- nervously stared at the code like it was ancient hieroglyphs\n- hours of debugging only to realize I missed a semicolon/indentation\n- the code worked (or so I thought).\n- proudly watching my quarto run smoothly… until the next bug sneaked in"
  },
  {
    "objectID": "index.html#find-out-what-i-have-learnt",
    "href": "index.html#find-out-what-i-have-learnt",
    "title": "ISSS608 - Visual Analytics and Applications",
    "section": "Find out what I have learnt",
    "text": "Find out what I have learnt\n\n\n\n\n\n\n\n\nIn-class Exercise\nHands-on Exercise\nTake-home Exercise"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04d.html",
    "href": "Hands-on_Ex/Hands-on_Ex04d.html",
    "title": "Hands-on Exercise 04d",
    "section": "",
    "text": "With the assistance of ChatGPT"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04d.html#funnel-plots-for-fair-comparisons",
    "href": "Hands-on_Ex/Hands-on_Ex04d.html#funnel-plots-for-fair-comparisons",
    "title": "Hands-on Exercise 04d",
    "section": "4. Funnel Plots for Fair Comparisons",
    "text": "4. Funnel Plots for Fair Comparisons\n\n4.1 Overview\nIn this exercise, we will be\n\nCreating funnel plots using the funnelPlotR package.\nDesigning static funnel plots with ggplot2.\nBuilding interactive funnel plots by combining plotly and ggplot2.\n\n\n\n4.2 Installing and launching R packages\nThe below are the R packages that will be used:\n\nreadr: For importing CSV files into R.\nFunnelPlotR: For generating funnel plots.\nggplot2: For manually creating funnel plots.\nknitr: For generating static HTML tables.\nplotly:For creating interactive funnel plots.\n\n\n\nCode\npacman::p_load(tidyverse, FunnelPlotR, plotly, knitr)\n\n\n\n\n4.3 Importing data\nWe will be using the COVID-19_DKI_Jakarta dataset, sourced from the Open Data Covid-19 Provinsi DKI Jakarta portal.\nIt focuses on comparing the cumulative COVID-19 cases and deaths by sub-district (kelurahan) as of 31st July 2021 in DKI Jakarta.\nThe code below imports the data into R and save it into a tibble data frame object called covid19\n\n\nCode\ncovid19 &lt;- read_csv(\"data/COVID-19_DKI_Jakarta.csv\") %&gt;%\n  mutate_if(is.character, as.factor)\n\nhead(covid19)\n\n\n# A tibble: 6 × 7\n  `Sub-district ID` City        District `Sub-district` Positive Recovered Death\n              &lt;dbl&gt; &lt;fct&gt;       &lt;fct&gt;    &lt;fct&gt;             &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1        3172051003 JAKARTA UT… PADEMAN… ANCOL              1776      1691    26\n2        3173041007 JAKARTA BA… TAMBORA  ANGKE              1783      1720    29\n3        3175041005 JAKARTA TI… KRAMAT … BALE KAMBANG       2049      1964    31\n4        3175031003 JAKARTA TI… JATINEG… BALI MESTER         827       797    13\n5        3175101006 JAKARTA TI… CIPAYUNG BAMBU APUS         2866      2792    27\n6        3174031002 JAKARTA SE… MAMPANG… BANGKA             1828      1757    26\n\n\nWe will check the dataset using below\n\nglimpse(): provides a transposed overview of a dataset, showing variables and their types in a concise format.\nhead(): displays the first few rows of a dataset (default is 6 rows) to give a quick preview of the data.\nsummary(): generates a statistical summary of each variable, including measures like mean, median, and range for numeric data.\nduplicated():returns a logical vector indicating which elements or rows in a vector or data frame are duplicates.\ncolSums(is.na()): counts the number of missing values (NA) in each column of the data frame.\nspec(): use spec() to quickly inspect the column\n\n\nglimpse()head()summary()duplicated()colSum(is.na(dataset))\n\n\n\n\nCode\nglimpse(covid19)\n\n\nRows: 267\nColumns: 7\n$ `Sub-district ID` &lt;dbl&gt; 3172051003, 3173041007, 3175041005, 3175031003, 3175…\n$ City              &lt;fct&gt; JAKARTA UTARA, JAKARTA BARAT, JAKARTA TIMUR, JAKARTA…\n$ District          &lt;fct&gt; PADEMANGAN, TAMBORA, KRAMAT JATI, JATINEGARA, CIPAYU…\n$ `Sub-district`    &lt;fct&gt; ANCOL, ANGKE, BALE KAMBANG, BALI MESTER, BAMBU APUS,…\n$ Positive          &lt;dbl&gt; 1776, 1783, 2049, 827, 2866, 1828, 2541, 3608, 2012,…\n$ Recovered         &lt;dbl&gt; 1691, 1720, 1964, 797, 2792, 1757, 2433, 3445, 1937,…\n$ Death             &lt;dbl&gt; 26, 29, 31, 13, 27, 26, 37, 68, 38, 52, 72, 28, 25, …\n\n\n\n\n\n\nCode\nhead(covid19)\n\n\n# A tibble: 6 × 7\n  `Sub-district ID` City        District `Sub-district` Positive Recovered Death\n              &lt;dbl&gt; &lt;fct&gt;       &lt;fct&gt;    &lt;fct&gt;             &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1        3172051003 JAKARTA UT… PADEMAN… ANCOL              1776      1691    26\n2        3173041007 JAKARTA BA… TAMBORA  ANGKE              1783      1720    29\n3        3175041005 JAKARTA TI… KRAMAT … BALE KAMBANG       2049      1964    31\n4        3175031003 JAKARTA TI… JATINEG… BALI MESTER         827       797    13\n5        3175101006 JAKARTA TI… CIPAYUNG BAMBU APUS         2866      2792    27\n6        3174031002 JAKARTA SE… MAMPANG… BANGKA             1828      1757    26\n\n\n\n\n\n\nCode\nsummary(covid19)\n\n\n Sub-district ID                     City              District  \n Min.   :3.101e+09   JAKARTA BARAT     :56   TAMBORA       : 11  \n 1st Qu.:3.172e+09   JAKARTA PUSAT     :44   KEBAYORAN BARU: 10  \n Median :3.173e+09   JAKARTA SELATAN   :65   CIPAYUNG      :  8  \n Mean   :3.172e+09   JAKARTA TIMUR     :65   JATINEGARA    :  8  \n 3rd Qu.:3.174e+09   JAKARTA UTARA     :31   KEMAYORAN     :  8  \n Max.   :3.175e+09   KAB.ADM.KEP.SERIBU: 6   SETIA BUDI    :  8  \n                                             (Other)       :214  \n       Sub-district    Positive      Recovered        Death       \n ANCOL       :  1   Min.   :  72   Min.   :  69   Min.   :  0.00  \n ANGKE       :  1   1st Qu.:1644   1st Qu.:1578   1st Qu.: 24.50  \n BALE KAMBANG:  1   Median :2420   Median :2329   Median : 39.00  \n BALI MESTER :  1   Mean   :2572   Mean   :2477   Mean   : 40.99  \n BAMBU APUS  :  1   3rd Qu.:3372   3rd Qu.:3242   3rd Qu.: 55.00  \n BANGKA      :  1   Max.   :6231   Max.   :5970   Max.   :158.00  \n (Other)     :261                                                 \n\n\n\n\n\n\nCode\ncovid19[duplicated(covid19),]\n\n\n# A tibble: 0 × 7\n# ℹ 7 variables: Sub-district ID &lt;dbl&gt;, City &lt;fct&gt;, District &lt;fct&gt;,\n#   Sub-district &lt;fct&gt;, Positive &lt;dbl&gt;, Recovered &lt;dbl&gt;, Death &lt;dbl&gt;\n\n\n\n\n\n\nCode\ncolSums(is.na(covid19))\n\n\nSub-district ID            City        District    Sub-district        Positive \n              0               0               0               0               0 \n      Recovered           Death \n              0               0 \n\n\n\n\nCode\nspec(covid19)\n\n\nNULL\n\n\n\n\n\nThe covid19 tibble contains seven attributes, as shown above:\n\nCategorical attributes: City, District, Sub-district\nContinuous attributes: Sub-district ID, Positive, Recovered, Death\n\n\n\n4.4 FunnelPlotR methods\nThe FunnelPlotR package uses ggplot2 to create funnel plots, requiring a numerator (events of interest), denominator (population), and a group. Key customization arguments include:\n\nlimit: Defines plot limits (95% or 99%).\nlabel_outliers: Labels outliers when set to TRUE or FALSE.\nPoisson_limits: Adds Poisson limits to the plot.\nOD_adjust: Adds overdispersed limits to the plot.\nxrange and yrange: Specify axis ranges, functioning like a zoom feature.\nAdditional aesthetics like graph titles, axis labels, and more.\n\n\n4.4.1 FunnelPlotR methods: The basic plot\nThe code below plots a funnel plot.\n\n\n\n\n\n\nThings to learn from the code\n\n\n\n\nThe group parameter differs from its use in scatterplots; here, it defines the level of data points to plot, such as Sub-district, District, or City. Selecting City results in only six data points.\nThe data_type argument defaults to “SR”, where “SR” stands for Standardized Ratio - used to compare the performance or outcomes of different groups.\n‘limit’ sets the plot limits, with accepted values of 95 or 99, representing 95% or 99.8% quantiles of the distribution.\n\n\n\n\nGraph()Code()\n\n\n\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 0 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\n\n\n\nCode\nfunnel_plot(\n  .data = covid19,\n  numerator = Positive,\n  denominator = Death,\n  group = `Sub-district`\n)\n\n\n\n\n\n\nA funnel plot object with 267 points of which 0 are outliers.\nPlot is adjusted for overdispersion.\n\n\n\n4.4.2 FunnelPlotR methods: Makeover 1\n\n\n\n\n\n\nThings to learn from the code\n\n\n\n\n\ndata_type argument is used to change from default “SR” to “PR” (i.e. proportions).\n\n\nxrange and yrange are used to set the range of x-axis and y-axis\n\n\n\n\n\nGraph()Code()\n\n\n\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\n\n\n\nCode\nfunnel_plot(\n  .data = covid19,\n  numerator = Death,\n  denominator = Positive,\n  group = `Sub-district`,\n  data_type = \"PR\",     #&lt;&lt;\n  xrange = c(0, 6500),  #&lt;&lt;\n  yrange = c(0, 0.05)   #&lt;&lt;\n)\n\n\n\n\n\n\n\n4.4.3 FunnelPlotR methods: Makeover 2\nThe code below plots a funnel plot.\n\n\n\n\n\n\nThings to learn from the code\n\n\n\n\nlabel = NA: argument disables the default outlier labeling feature.\ntitle: argument adds a title to the plot.\nx_label and y_label: arguments are used to add or modify the titles of the x-axis and y-axis.\n\n\n\n\nGraph()Code()\n\n\n\n\n\n\n\n\n\n\n\nA funnel plot object with 267 points of which 7 are outliers. \nPlot is adjusted for overdispersion. \n\n\n\n\n\n\nCode\nfunnel_plot(\n  .data = covid19,\n  numerator = Death,\n  denominator = Positive,\n  group = `Sub-district`,\n  data_type = \"PR\",   \n  xrange = c(0, 6500),  \n  yrange = c(0, 0.05),\n  label = NA,\n  title = \"Cumulative COVID-19 Fatality Rate by Cumulative Total Number of COVID-19 Positive Cases\", #&lt;&lt;           \n  x_label = \"Cumulative COVID-19 Positive Cases\", #&lt;&lt;\n  y_label = \"Cumulative Fatality Rate\"  #&lt;&lt;\n)\n\n\n\n\n\n\n\n\n4.5 Funnel plot for fair visual comparison\nIn this section, we will learn how to create funnel plots using ggplots.\n\n4.5.1 Computing the basic derived fields\nTo plot the funnel plot from scratch, we have to derive cumulative death rate and standard error of cumulative death rate.\n\n\nCode\ndf &lt;- covid19 %&gt;%\n  mutate(rate = Death / Positive) %&gt;%\n  mutate(rate.se = sqrt((rate*(1-rate)) / (Positive))) %&gt;%\n  filter(rate &gt; 0)\n\n\nThe fit.mean is computed using the code below:\n\n\nCode\nfit.mean &lt;- weighted.mean(df$rate, 1/df$rate.se^2)\n\n\n\n\n4.5.2 Calculate the lower and upper limit for 95% and 99% CI\nThe code below is used to compute the lower and upper limits for 95% confidence interval.\n\n\nCode\nnumber.seq &lt;- seq(1, max(df$Positive), 1)\nnumber.ll95 &lt;- fit.mean - 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul95 &lt;- fit.mean + 1.96 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ll999 &lt;- fit.mean - 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \nnumber.ul999 &lt;- fit.mean + 3.29 * sqrt((fit.mean*(1-fit.mean)) / (number.seq)) \ndfCI &lt;- data.frame(number.ll95, number.ul95, number.ll999, \n                   number.ul999, number.seq, fit.mean)\n\n\n\n\n4.5.3 Plotting a static funnel plot\nThe code below uses ggplot2 functions to plot a static funnel plot.\n\nGraph()Code()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\np &lt;- ggplot(df, aes(x = Positive, y = rate)) +\n  geom_point(aes(label=`Sub-district`), \n             alpha=0.4) +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul95), \n            size = 0.4, \n            colour = \"grey40\", \n            linetype = \"dashed\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ll999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_line(data = dfCI, \n            aes(x = number.seq, \n                y = number.ul999), \n            size = 0.4, \n            colour = \"grey40\") +\n  geom_hline(data = dfCI, \n             aes(yintercept = fit.mean), \n             size = 0.4, \n             colour = \"grey40\") +\n  coord_cartesian(ylim=c(0,0.05)) +\n  annotate(\"text\", x = 1, y = -0.13, label = \"95%\", size = 3, colour = \"grey40\") + \n  annotate(\"text\", x = 4.5, y = -0.18, label = \"99%\", size = 3, colour = \"grey40\") + \n  ggtitle(\"Cumulative Fatality Rate by Cumulative Number of COVID-19 Cases\") +\n  xlab(\"Cumulative Number of COVID-19 Cases\") + \n  ylab(\"Cumulative Fatality Rate\") +\n  theme_light() +\n  theme(plot.title = element_text(size=12),\n        legend.position = c(0.91,0.85), \n        legend.title = element_text(size=7),\n        legend.text = element_text(size=7),\n        legend.background = element_rect(colour = \"grey60\", linetype = \"dotted\"),\n        legend.key.height = unit(0.3, \"cm\"))\np\n\n\n\n\n\n\n\n4.5.4 Interactive funnel plot: plotly + ggplot2\nWe can make the funnel plot interactive with ggplotly() of plotly r package.\n\nGraph()Code()\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nfp_ggplotly &lt;- ggplotly(p,\n  tooltip = c(\"label\", \n              \"x\", \n              \"y\"))\nfp_ggplotly\n\n\n\n\n\n\n\n\n4.6 References\n\nKam, T.S(2024). Visualising Uncertainty\nfunnelPlotR package.\nFunnel Plots for Indirectly-standardised ratios\nChanging funnel plot options\nggplot2 package\n\n\n\n4.8 Takeaway\n\n\n\n\n\n\nKey takeaways\n\n\n\n\nLearnt about the general concepts of funnel plots which as specialized data visualizations used for unbiased comparisons between entities like outlets, stores, or sub-districts. Can help to identify outliers by comparing performance metrics against expected variability.\nSome of the R packages used - FunnelPlotR,ggplot2,plotly,ggplot2,knitr etc.\nLearnt about funnel plot customization with R, using limit, label_outliers, poisson_limits,OD_adjust, xrange, yrange, title, x_label, and y_label etc. - Learnt that it is possible to convert static ggplot2 plots into interactive plots using ggplotly().\n\n\n\n\n\n4.9 Further exploration"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04b.html",
    "href": "Hands-on_Ex/Hands-on_Ex04b.html",
    "title": "Hands-on Exercise 04b",
    "section": "",
    "text": "With the assistance of ChatGPT"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04b.html#visualising-statistical-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex04b.html#visualising-statistical-analysis",
    "title": "Hands-on Exercise 04b",
    "section": "4. Visualising Statistical Analysis",
    "text": "4. Visualising Statistical Analysis\n\n4.1 Learning outcome\nIn this hands-on exercise, we will be exploring:\n\nggstatsplot for creating statistical visualizations,\nperformance for visualizing model diagnostics, and\nparameters for visualizing model parameters.\n\n\n\n4.2 Visual Statistical Analysis with ggstatsplot\nggstatsplot extends ggplot2, integrating statistical test results directly into visualizations.\n\nOffers alternative statistical inference methods by default.\nEnsures best practices for statistical reporting, following APA standards for consistency.\nExample: Displays results from a robust t-test within the plot.\n\n\n\n4.3 Getting Started\n\n4.3.1 Installing and loading the packages\nWe will be using ggstatsplot and tidyverse packags.\n\n\nCode\npacman::p_load(ggstatsplot, tidyverse)\n\n\n\n\n4.3.2 Importing data\nIn this exercise, Exam_data.csv will be used. The `read_csv() function from the readr package is used to import the dataset into R and store it as a tibble data frame.\n\n\nCode\nexam &lt;- read_csv(\"data/Exam_data.csv\", show_col_types = FALSE)\n\n\n\n\nCode\nstat(exam)\n\n\n# A tibble: 322 × 7\n   ID         CLASS GENDER RACE    ENGLISH MATHS SCIENCE\n   &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 Student321 3I    Male   Malay        21     9      15\n 2 Student305 3I    Female Malay        24    22      16\n 3 Student289 3H    Male   Chinese      26    16      16\n 4 Student227 3F    Male   Chinese      27    77      31\n 5 Student318 3I    Male   Malay        27    11      25\n 6 Student306 3I    Female Malay        31    16      16\n 7 Student313 3I    Male   Chinese      31    21      25\n 8 Student316 3I    Male   Malay        31    18      27\n 9 Student312 3I    Male   Malay        33    19      15\n10 Student297 3H    Male   Indian       34    49      37\n# ℹ 312 more rows\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThis section data is the same dataset used in Hands-on_Ex01, Hands-on_Ex02, Hands-on_Ex03a, and Hands-on_Ex04a\n\n\n\n\n\n4.3.3 One-sample test: gghistostats() method\ngghistostats() will be used to build a visual of one-sample test on English scores.\n\n\nCode\nset.seed(1234)\n\ngghistostats(\n  data = exam,\n  x = ENGLISH,\n  type = \"bayes\",\n  test.value = 60,\n  xlab = \"English scores\"\n)\n\n\n\n\n\n\n\n\n\nDefault information: - statistical details - Bayes Factor - sample sizes - distribution summary\n\n\n4.3.4 Unpacking the Bayes Factor\n\nThe Bayes Factor quantifies the strength of evidence in favor of one hypothesis over another, comparing the alternative hypothesis (H₁) to the null hypothesis (H₀).\nIt helps evaluate data in support of the null hypothesis while incorporating external information.\nA common method for approximating the Bayes Factor is the Schwarz criterion.\n\n\n\n4.3.5 How to interpret Bayes Factor\nThe Bayes Factor (BF) is a positive number used to assess the strength of evidence for one hypothesis over another.\nRefer here for one of the most common interpretations of Bayes Factor, first proposed by Harold Jeffereys (1961) and slightly modified by Lee and Wagenmakers in 2013.\n\n\n\n4.3.6 Two-sample mean test: ggbetweenstats()\nThe code below, ggbetweenstats() is used to build a visual for two-sample mean test of Maths scores by gender.\n\n\nCode\nggbetweenstats(\n  data = exam,\n  x = GENDER, \n  y = MATHS,\n  type = \"np\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\n\nDefault information: - statistical details - Bayes Factor - sample sizes - distribution summary\n\n\n4.3.7 Oneway ANOVA Test: ggbetweenstats()\nThe code below used ggbetweenstats() to build a visual for One-way ANOVA test on English score by race.\n\n\nCode\nggbetweenstats(\n  data = exam,\n  x = RACE, \n  y = ENGLISH,\n  type = \"p\",\n  mean.ci = TRUE, \n  pairwise.comparisons = TRUE, \n  pairwise.display = \"s\",\n  p.adjust.method = \"fdr\",\n  messages = FALSE\n)\n\n\n\n\n\n\n\n\n\nMeaning of the symbol function of ggbetweenstats():\n\n\n\nggbetweenstats() symbol arguments meaning\n\n\n\n\n\n\nSymbol\nMeaning\n\n\n\n\nns\npairwise.display - only non-significant\n\n\ns\npairwise.display- only significant\n\n\nall\npairwise.display - everything\n\n\np\ntype - Parametric tests (default)\n\n\nnp\ntype - Non-parametric tests (e.g., Kruskal-Wallis, Wilcoxon)\n\n\nr\ntype - Robust statistical tests (e.g., trimmed means ANOVA)\n\n\nbayes\ntype - Bayesian analysis for comparisons\n\n\n\n\n\n\n\n4.3.8 Significant test of correlation: ggscatterstats()\nThe code below, ggscatterstats() is used to build a visual for significant test of correlation between MATHS and ENGLISH scores\n\n\nCode\nggscatterstats(\n  data = exam,\n  x = MATHS,\n  y = ENGLISH,\n  marginal = FALSE,\n  )\n\n\n\n\n\n\n\n\n\n\n\n4.3.9 Significant test of association (dependence): ggbarstats()\nThe Maths scores is binned into 4-class variables using cut()\n\n\nCode\nexam1 &lt;- exam %&gt;% \n  mutate(MATHS_bins = \n           cut(MATHS, \n               breaks = c(0,60,75,85,100))\n)\n\n\nNext, ggbarstats() is used to build a visual for significant test of association\n\n\nCode\nggbarstats(exam1, \n           x = MATHS_bins, \n           y = GENDER)\n\n\n\n\n\n\n\n\n\n\n\n\n4.5 Getting Started\nThis section covers visualizing model diagnostics and parameters using the parameters package.\nThe Toyota Corolla case study will be used to build a model that identifies key factors influencing used car prices, considering various explanatory variables.\n\n\n4.6 Installing and loading the required libraries\n\n\nCode\npacman::p_load(readxl, performance, parameters, see)\n\n\n\n4.6.1 Importing Excel file using readxl methods\nThe code below will use read_xls() of readxl to import the data worksheet of ToyoyaCorolla.xls workbook into R.\n\n\nCode\ncar_resale &lt;- read_xls(\"data/ToyotaCorolla.xls\", \n                       \"data\")\ncar_resale\n\n\n# A tibble: 1,436 × 38\n      Id Model    Price Age_08_04 Mfg_Month Mfg_Year     KM Quarterly_Tax Weight\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n 1    81 TOYOTA … 18950        25         8     2002  20019           100   1180\n 2     1 TOYOTA … 13500        23        10     2002  46986           210   1165\n 3     2 TOYOTA … 13750        23        10     2002  72937           210   1165\n 4     3  TOYOTA… 13950        24         9     2002  41711           210   1165\n 5     4 TOYOTA … 14950        26         7     2002  48000           210   1165\n 6     5 TOYOTA … 13750        30         3     2002  38500           210   1170\n 7     6 TOYOTA … 12950        32         1     2002  61000           210   1170\n 8     7  TOYOTA… 16900        27         6     2002  94612           210   1245\n 9     8 TOYOTA … 18600        30         3     2002  75889           210   1245\n10    44 TOYOTA … 16950        27         6     2002 110404           234   1255\n# ℹ 1,426 more rows\n# ℹ 29 more variables: Guarantee_Period &lt;dbl&gt;, HP_Bin &lt;chr&gt;, CC_bin &lt;chr&gt;,\n#   Doors &lt;dbl&gt;, Gears &lt;dbl&gt;, Cylinders &lt;dbl&gt;, Fuel_Type &lt;chr&gt;, Color &lt;chr&gt;,\n#   Met_Color &lt;dbl&gt;, Automatic &lt;dbl&gt;, Mfr_Guarantee &lt;dbl&gt;,\n#   BOVAG_Guarantee &lt;dbl&gt;, ABS &lt;dbl&gt;, Airbag_1 &lt;dbl&gt;, Airbag_2 &lt;dbl&gt;,\n#   Airco &lt;dbl&gt;, Automatic_airco &lt;dbl&gt;, Boardcomputer &lt;dbl&gt;, CD_Player &lt;dbl&gt;,\n#   Central_Lock &lt;dbl&gt;, Powered_Windows &lt;dbl&gt;, Power_Steering &lt;dbl&gt;, …\n\n\nThe output object car_resale is a tibble data frame.\nWe will check the dataset using below\n\nglimpse(): provides a transposed overview of a dataset, showing variables and their types in a concise format.\nhead(): displays the first few rows of a dataset (default is 6 rows) to give a quick preview of the data.\nsummary(): generates a statistical summary of each variable, including measures like mean, median, and range for numeric data.\nduplicated():returns a logical vector indicating which elements or rows in a vector or data frame are duplicates.\ncolSums(is.na()): counts the number of missing values (NA) in each column of the data frame.\nspec(): use spec() to quickly inspect the column\n\n\nglimpse()head()summary()duplicated()colSum(is.na(dataset))\n\n\n\n\nCode\nglimpse(car_resale)\n\n\nRows: 1,436\nColumns: 38\n$ Id               &lt;dbl&gt; 81, 1, 2, 3, 4, 5, 6, 7, 8, 44, 45, 46, 47, 49, 51, 6…\n$ Model            &lt;chr&gt; \"TOYOTA Corolla 1.6 5drs 1 4/5-Doors\", \"TOYOTA Coroll…\n$ Price            &lt;dbl&gt; 18950, 13500, 13750, 13950, 14950, 13750, 12950, 1690…\n$ Age_08_04        &lt;dbl&gt; 25, 23, 23, 24, 26, 30, 32, 27, 30, 27, 22, 23, 27, 2…\n$ Mfg_Month        &lt;dbl&gt; 8, 10, 10, 9, 7, 3, 1, 6, 3, 6, 11, 10, 6, 11, 11, 11…\n$ Mfg_Year         &lt;dbl&gt; 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002,…\n$ KM               &lt;dbl&gt; 20019, 46986, 72937, 41711, 48000, 38500, 61000, 9461…\n$ Quarterly_Tax    &lt;dbl&gt; 100, 210, 210, 210, 210, 210, 210, 210, 210, 234, 234…\n$ Weight           &lt;dbl&gt; 1180, 1165, 1165, 1165, 1165, 1170, 1170, 1245, 1245,…\n$ Guarantee_Period &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,…\n$ HP_Bin           &lt;chr&gt; \"100-120\", \"&lt; 100\", \"&lt; 100\", \"&lt; 100\", \"&lt; 100\", \"&lt; 100…\n$ CC_bin           &lt;chr&gt; \"1600\", \"&gt;1600\", \"&gt;1600\", \"&gt;1600\", \"&gt;1600\", \"&gt;1600\", …\n$ Doors            &lt;dbl&gt; 5, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 3, 3,…\n$ Gears            &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,…\n$ Cylinders        &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,…\n$ Fuel_Type        &lt;chr&gt; \"Petrol\", \"Diesel\", \"Diesel\", \"Diesel\", \"Diesel\", \"Di…\n$ Color            &lt;chr&gt; \"Blue\", \"Blue\", \"Silver\", \"Blue\", \"Black\", \"Black\", \"…\n$ Met_Color        &lt;dbl&gt; 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,…\n$ Automatic        &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Mfr_Guarantee    &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,…\n$ BOVAG_Guarantee  &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ ABS              &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airbag_1         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airbag_2         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airco            &lt;dbl&gt; 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Automatic_airco  &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,…\n$ Boardcomputer    &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ CD_Player        &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,…\n$ Central_Lock     &lt;dbl&gt; 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Powered_Windows  &lt;dbl&gt; 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Power_Steering   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Radio            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Mistlamps        &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Sport_Model      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,…\n$ Backseat_Divider &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Metallic_Rim     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Radio_cassette   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Tow_Bar          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n\n\n\n\n\n\nCode\nhead(car_resale)\n\n\n# A tibble: 6 × 38\n     Id Model      Price Age_08_04 Mfg_Month Mfg_Year    KM Quarterly_Tax Weight\n  &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n1    81 TOYOTA Co… 18950        25         8     2002 20019           100   1180\n2     1 TOYOTA Co… 13500        23        10     2002 46986           210   1165\n3     2 TOYOTA Co… 13750        23        10     2002 72937           210   1165\n4     3  TOYOTA C… 13950        24         9     2002 41711           210   1165\n5     4 TOYOTA Co… 14950        26         7     2002 48000           210   1165\n6     5 TOYOTA Co… 13750        30         3     2002 38500           210   1170\n# ℹ 29 more variables: Guarantee_Period &lt;dbl&gt;, HP_Bin &lt;chr&gt;, CC_bin &lt;chr&gt;,\n#   Doors &lt;dbl&gt;, Gears &lt;dbl&gt;, Cylinders &lt;dbl&gt;, Fuel_Type &lt;chr&gt;, Color &lt;chr&gt;,\n#   Met_Color &lt;dbl&gt;, Automatic &lt;dbl&gt;, Mfr_Guarantee &lt;dbl&gt;,\n#   BOVAG_Guarantee &lt;dbl&gt;, ABS &lt;dbl&gt;, Airbag_1 &lt;dbl&gt;, Airbag_2 &lt;dbl&gt;,\n#   Airco &lt;dbl&gt;, Automatic_airco &lt;dbl&gt;, Boardcomputer &lt;dbl&gt;, CD_Player &lt;dbl&gt;,\n#   Central_Lock &lt;dbl&gt;, Powered_Windows &lt;dbl&gt;, Power_Steering &lt;dbl&gt;,\n#   Radio &lt;dbl&gt;, Mistlamps &lt;dbl&gt;, Sport_Model &lt;dbl&gt;, Backseat_Divider &lt;dbl&gt;, …\n\n\n\n\n\n\nCode\nsummary(car_resale)\n\n\n       Id            Model               Price         Age_08_04    \n Min.   :   1.0   Length:1436        Min.   : 4350   Min.   : 1.00  \n 1st Qu.: 361.8   Class :character   1st Qu.: 8450   1st Qu.:44.00  \n Median : 721.5   Mode  :character   Median : 9900   Median :61.00  \n Mean   : 721.6                      Mean   :10731   Mean   :55.95  \n 3rd Qu.:1081.2                      3rd Qu.:11950   3rd Qu.:70.00  \n Max.   :1442.0                      Max.   :32500   Max.   :80.00  \n   Mfg_Month         Mfg_Year          KM         Quarterly_Tax   \n Min.   : 1.000   Min.   :1998   Min.   :     1   Min.   : 19.00  \n 1st Qu.: 3.000   1st Qu.:1998   1st Qu.: 43000   1st Qu.: 69.00  \n Median : 5.000   Median :1999   Median : 63390   Median : 85.00  \n Mean   : 5.549   Mean   :2000   Mean   : 68533   Mean   : 87.12  \n 3rd Qu.: 8.000   3rd Qu.:2001   3rd Qu.: 87021   3rd Qu.: 85.00  \n Max.   :12.000   Max.   :2004   Max.   :243000   Max.   :283.00  \n     Weight     Guarantee_Period    HP_Bin             CC_bin         \n Min.   :1000   Min.   : 3.000   Length:1436        Length:1436       \n 1st Qu.:1040   1st Qu.: 3.000   Class :character   Class :character  \n Median :1070   Median : 3.000   Mode  :character   Mode  :character  \n Mean   :1072   Mean   : 3.815                                        \n 3rd Qu.:1085   3rd Qu.: 3.000                                        \n Max.   :1615   Max.   :36.000                                        \n     Doors           Gears         Cylinders  Fuel_Type        \n Min.   :2.000   Min.   :3.000   Min.   :4   Length:1436       \n 1st Qu.:3.000   1st Qu.:5.000   1st Qu.:4   Class :character  \n Median :4.000   Median :5.000   Median :4   Mode  :character  \n Mean   :4.033   Mean   :5.026   Mean   :4                     \n 3rd Qu.:5.000   3rd Qu.:5.000   3rd Qu.:4                     \n Max.   :5.000   Max.   :6.000   Max.   :4                     \n    Color             Met_Color        Automatic       Mfr_Guarantee   \n Length:1436        Min.   :0.0000   Min.   :0.00000   Min.   :0.0000  \n Class :character   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000  \n Mode  :character   Median :1.0000   Median :0.00000   Median :0.0000  \n                    Mean   :0.6748   Mean   :0.05571   Mean   :0.4095  \n                    3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:1.0000  \n                    Max.   :1.0000   Max.   :1.00000   Max.   :1.0000  \n BOVAG_Guarantee       ABS            Airbag_1         Airbag_2     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.8955   Mean   :0.8134   Mean   :0.9708   Mean   :0.7228  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n     Airco        Automatic_airco   Boardcomputer      CD_Player     \n Min.   :0.0000   Min.   :0.00000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median :0.00000   Median :0.0000   Median :0.0000  \n Mean   :0.5084   Mean   :0.05641   Mean   :0.2946   Mean   :0.2187  \n 3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.00000   Max.   :1.0000   Max.   :1.0000  \n  Central_Lock    Powered_Windows Power_Steering       Radio       \n Min.   :0.0000   Min.   :0.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.000   Median :1.0000   Median :0.0000  \n Mean   :0.5801   Mean   :0.562   Mean   :0.9777   Mean   :0.1462  \n 3rd Qu.:1.0000   3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.000   Max.   :1.0000   Max.   :1.0000  \n   Mistlamps      Sport_Model     Backseat_Divider  Metallic_Rim   \n Min.   :0.000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :0.000   Median :0.0000   Median :1.0000   Median :0.0000  \n Mean   :0.257   Mean   :0.3001   Mean   :0.7702   Mean   :0.2047  \n 3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n Radio_cassette      Tow_Bar      \n Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000  \n Mean   :0.1455   Mean   :0.2779  \n 3rd Qu.:0.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000  \n\n\n\n\n\n\nCode\ncar_resale[duplicated(car_resale),]\n\n\n# A tibble: 0 × 38\n# ℹ 38 variables: Id &lt;dbl&gt;, Model &lt;chr&gt;, Price &lt;dbl&gt;, Age_08_04 &lt;dbl&gt;,\n#   Mfg_Month &lt;dbl&gt;, Mfg_Year &lt;dbl&gt;, KM &lt;dbl&gt;, Quarterly_Tax &lt;dbl&gt;,\n#   Weight &lt;dbl&gt;, Guarantee_Period &lt;dbl&gt;, HP_Bin &lt;chr&gt;, CC_bin &lt;chr&gt;,\n#   Doors &lt;dbl&gt;, Gears &lt;dbl&gt;, Cylinders &lt;dbl&gt;, Fuel_Type &lt;chr&gt;, Color &lt;chr&gt;,\n#   Met_Color &lt;dbl&gt;, Automatic &lt;dbl&gt;, Mfr_Guarantee &lt;dbl&gt;,\n#   BOVAG_Guarantee &lt;dbl&gt;, ABS &lt;dbl&gt;, Airbag_1 &lt;dbl&gt;, Airbag_2 &lt;dbl&gt;,\n#   Airco &lt;dbl&gt;, Automatic_airco &lt;dbl&gt;, Boardcomputer &lt;dbl&gt;, CD_Player &lt;dbl&gt;, …\n\n\n\n\n\n\nCode\ncolSums(is.na(car_resale))\n\n\n              Id            Model            Price        Age_08_04 \n               0                0                0                0 \n       Mfg_Month         Mfg_Year               KM    Quarterly_Tax \n               0                0                0                0 \n          Weight Guarantee_Period           HP_Bin           CC_bin \n               0                0                0                0 \n           Doors            Gears        Cylinders        Fuel_Type \n               0                0                0                0 \n           Color        Met_Color        Automatic    Mfr_Guarantee \n               0                0                0                0 \n BOVAG_Guarantee              ABS         Airbag_1         Airbag_2 \n               0                0                0                0 \n           Airco  Automatic_airco    Boardcomputer        CD_Player \n               0                0                0                0 \n    Central_Lock  Powered_Windows   Power_Steering            Radio \n               0                0                0                0 \n       Mistlamps      Sport_Model Backseat_Divider     Metallic_Rim \n               0                0                0                0 \n  Radio_cassette          Tow_Bar \n               0                0 \n\n\n\n\nCode\nspec(car_resale)\n\n\nNULL\n\n\n\n\n\n\n\n4.6.2 Multiple Regression Model using lm()\nThe code used to calibrate a multiple linear regression model by using lm()of Base Stats of R.\n\n\nCode\nmodel &lt;- lm(Price ~ Age_08_04 + Mfg_Year + KM + \n              Weight + Guarantee_Period, data = car_resale)\nmodel\n\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01  \n\n\n\n\n4.6.3 Multiple diagnostic: checking for multicolinearity:\nThe code below checks for multicolinearity using the check_colinearity() of performance package.\n\n\nCode\ncheck_collinearity(model)\n\n\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n               KM 1.46 [ 1.37,  1.57]         1.21      0.68     [0.64, 0.73]\n           Weight 1.41 [ 1.32,  1.51]         1.19      0.71     [0.66, 0.76]\n Guarantee_Period 1.04 [ 1.01,  1.17]         1.02      0.97     [0.86, 0.99]\n\nHigh Correlation\n\n      Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n Age_08_04 31.07 [28.08, 34.38]         5.57      0.03     [0.03, 0.04]\n  Mfg_Year 31.16 [28.16, 34.48]         5.58      0.03     [0.03, 0.04]\n\n\n\n\nCode\ncheck_c &lt;- check_collinearity(model)\nplot(check_c)\n\n\n\n\n\n\n\n\n\n\n\n4.6.4 Model diagnostic: checking normality assumption\nThe code below checks for normality using the check_normality() of performance package.\n\n\nCode\nmodel1 &lt;- lm(Price ~ Age_08_04 + KM + \n              Weight + Guarantee_Period, data = car_resale)\n\n\n\n\nCode\ncheck_n &lt;- check_normality(model1)\n\n\n\n\nCode\nplot(check_n)\n\n\n\n\n\n\n\n\n\n\n\n4.6.5 Model diagnostic: checking model for homogeneity of variances\nThe code below checks for homogeneity of variances using the check_heteroscedasticity() of performance package.\n\n\nCode\ncheck_h &lt;- check_heteroscedasticity(model1)\n\n\n\n\nCode\nplot(check_h)\n\n\n\n\n\n\n\n\n\n\n\n4.6.6 Model diagnostic: complete check\nWe can perform a complete check by using check_model()\n\n\nCode\ncheck_model(model1)\n\n\n\n\n\n\n\n\n\n\n\n4.6.7 Visualising regression parameters: see methods\nThe code below utilizes the plot() function from the see package and the parameters() function from the parameters package to visualize the parameters of a regression model.\n\n\nCode\nplot(parameters(model1))\n\n\n\n\n\n\n\n\n\n\n\n4.6.8 Visualising regression parameters: ggcoefstats() methods\n\n\nCode\nggcoefstats(model1, \n            output = \"plot\")\n\n\n\n\n\n\n\n\n\n\n\n\n4.7 References\n\nKam, T.S(2024). Visual Statistical Analysis.\n\n\n\n4.8 Takeaway\n\n\n\n\n\n\nKey takeaways\n\n\n\n\nLearnt that by combining plots with statistical summaries can improves data storytelling.\nLearnt that it is important to check for assumptions like multicollinearity, normality, and homoscedasticity before interpreting model results.\nKey R packages used - ggstatsplot, performance,parameters,and seeto perform visual statistical analyses and model diagnostics.\n\n\n\n\n\n4.9 Further exploration\n\nTo explore the distribution of MATHS Grades across different class\n\nColor gradient progresses smoothly from red to green, aligning with grade performance levels -\n\nGrade F –&gt; Red\nGrade D –&gt; Orange\nGrade C –&gt; Yellow\nGrade B –&gt; Light Green\nGrade A –&gt; Green\n\nObservations:\n\nClass 3A has the highest proportion of top scorers (Grade A).\nClass 3I has the most failing students (91% Grade F).\nClass 3F has the most balanced grade distribution.\n\n\nGraphCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nexam1 &lt;- exam %&gt;% \n  mutate(MATHS_bins = \n           cut(MATHS, \n               breaks = c(0, 49, 59, 69, 79, 100),  # 5 bins\n               labels = c(\"Grade F\", \"Grade D\", \"Grade C\", \"Grade B\", \"Grade A\"),  # Labels\n               right = TRUE)  # Include upper bound in interval\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo explore demographics factors such as GENDER, RACE and CLASS affecting MATHS scores.\n\nDependent variables: MATHS score\nIndependent variable: GENDER, RACE, CLASS\n\nGraph - 3 predictorsCode - 3 predictors\n\n\nThe 3 predictors are - GENDER, RACE, CLASS\nObservations:\n\nModel fit\n\nBoth multiple R-squared (0.8033), and Adjusted R-squared (0.7957) shows a strong model fit\n\nImpact of GENDER on MATHS scores\n\nGENDERMale Coefficient: -0.00676\nNo significant difference in MATHS scores between male and female students.\n\nImpact of RACE on MATHS scores\n\nOnly Malay students -4.77578 (p = 8.37e-05) show a significant difference, performing worse on average\n\nImpact of CLASS on MATHS Scores\n\nSeveral CLASS categories have strong negative coefficients with high statistical significance (p &lt; 0.001) such as CLASS3I, CLASS3H, CLASS3G, CLASS3E, and CLASS3F\nClass Differences Are Critical, and it shows that Class assignment has the strongest and most significant impact on MATHS scores.\n\n\n\n\n\nCall:\nlm(formula = MATHS ~ GENDER + RACE + CLASS, data = exam)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-31.344  -4.985   0.023   5.399  32.226 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  90.28726    1.51601  59.556  &lt; 2e-16 ***\nGENDERMale   -0.00676    1.02486  -0.007   0.9947    \nRACEIndian   -1.30523    2.76090  -0.473   0.6367    \nRACEMalay    -4.77578    1.19796  -3.987 8.37e-05 ***\nRACEOthers   -3.50495    3.14092  -1.116   0.2653    \nCLASS3B      -3.51440    2.06425  -1.703   0.0897 .  \nCLASS3C     -11.30379    2.04351  -5.532 6.77e-08 ***\nCLASS3D     -11.30251    2.07113  -5.457 9.94e-08 ***\nCLASS3E     -16.76621    2.08137  -8.055 1.74e-14 ***\nCLASS3F     -16.42073    2.08316  -7.883 5.54e-14 ***\nCLASS3G     -32.68651    2.18254 -14.976  &lt; 2e-16 ***\nCLASS3H     -42.93694    2.24643 -19.113  &lt; 2e-16 ***\nCLASS3I     -59.73049    2.49074 -23.981  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.03 on 309 degrees of freedom\nMultiple R-squared:  0.8033,    Adjusted R-squared:  0.7957 \nF-statistic: 105.2 on 12 and 309 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\nCode\nmodel_demographics &lt;- lm(MATHS ~ GENDER + RACE + CLASS, data = exam)\n\n\n\n\nCode\nsummary(model_demographics)\n\n\n\n\n\n\nTo perform a complete diagnostic check on the regression model (above)\n\n\n\nCode\ncheck_model(model_demographics)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03b.html",
    "href": "Hands-on_Ex/Hands-on_Ex03b.html",
    "title": "Hands-on Exercise 03b",
    "section": "",
    "text": "With the assistance of ChatGPT"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03b.html#b.-programming-animated-statistical-graphics-with-r",
    "href": "Hands-on_Ex/Hands-on_Ex03b.html#b.-programming-animated-statistical-graphics-with-r",
    "title": "Hands-on Exercise 03b",
    "section": "3b. Programming animated statistical graphics with R",
    "text": "3b. Programming animated statistical graphics with R\n\n3.1 Overview\nIn this Hands-on exercise 03b, we will learn how to create engaging animated data visualizations using the gganimate and plotly R packages. We will also learn how to reshape data with the tidyr package and process, wrangle, and transform data with the dplyr package.\nOverall, animated graphics not only captivate the audience but also leave a lasting impression, making them an effective tool for visually-driven data storytelling.\n\n3.1.1 Basic concepts of animation\nAnimations in data visualization are created by generating a series of individual plots, each representing a subset of the data. These plots are then stitched together into sequential frames to create the illusion of motion, similar to a flipbook or traditional cartoons. The animated effect is driven by the transitions between data subsets over time.\n\n\n\n3.1.2 Terminology\nBefore creating an animated statistical graph, it’s important to understand key concepts:\n\nFrame: each frame represents a specific point in time or category, updating the graph’s data points as it changes.\nAnimation attributes: control the animation’s behavior, such as frame duration, easing functions for transitions, and whether the animation starts from the current frame or resets to the beginning.\n\n\n\n\n\n\n\nTo read\n\n\n\nConsider whether the effort is justified before creating animated graphs. For exploratory data analysis, animations may not be worth the time. However, in presentations, well-placed animations can significantly enhance audience engagement compared to static visuals.\n\n\n\n\n\n3.2 Getting started\n\n3.2.1 Loading the R packages\nFirst, we install and load the folliwing R packages:\n\nplotly: An R library for creating interactive statistical graphs.\ngganimate: A ggplot2 extension for making animated graphs\ngifski: A tool for converting video frames into high-quality animated GIFs using advanced palette and dithering techniques.\ngapminder: A dataset excerpt from Gapminder.org, often used for its country_colors schemes.\ntidyverse: A collection of modern R packages designed for data science tasks, including analysis, communication, and creating static graphs.\n\n\n\nCode\npacman::p_load(readxl, gifski, gapminder,\n               plotly, gganimate, tidyverse)\n\n\n\n\n3.2.2 Importing the data\nIn this hands-on exercise, the Data worksheet from GlobalPopulation Excel workbook will be used.\nImporting Data worksheet from GlobalPopulation Excel workbook by using appropriate R package from tidyverse family.\n\n\n\n\n\n\nNote\n\n\n\n\nread_xls(): Imports Excel worksheets, readxl package\nmutate_each_():Converts all character data types to factors, dplyr package\nmutate(): Converts the Year field values to integers, dplyr package\n\n\n\n\n\nCode\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate_each_(funs(factor(.)), col) %&gt;%\n  mutate(Year = as.integer(Year))\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\nWarning: mutate_each_() was deprecated in dplyr 0.7.0.\nWarning: funs() was deprecated in dplyr 0.8.0.\n\n\n\nWe will re-write the code by using mutate_at() as shown below.\n’mutate(across())` can be used to derive the same outputs.\n\nmutate_at()mutate(across())\n\n\n\n\nCode\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate_at(col, as.factor) %&gt;%\n  mutate(Year = as.integer(Year))\n\n\n\n\n\n\nCode\ncol &lt;- c(\"Country\", \"Continent\")\nglobalPop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\") %&gt;%\n  mutate(across(col, as.factor)) %&gt;%\n  mutate(Year = as.integer(Year))\n\n\n\n\n\n\n\n3.2.3 Inspecting the data\n\n\nCode\nglobalPop &lt;- read_xls(\"data/GlobalPopulation.xls\", sheet = \"Data\")\n\n\nWe will check the dataset using below\n\nglimpse(): provides a transposed overview of a dataset, showing variables and their types in a concise format.\nhead(): displays the first few rows of a dataset (default is 6 rows) to give a quick preview of the data.\nsummary(): generates a statistical summary of each variable, including measures like mean, median, and range for numeric data.\nduplicated():Returns a logical vector indicating which elements or rows in a vector or data frame are duplicates.\ncolSums(is.na()): Counts the number of missing values (NA) in each column of the data frame.\n\n\nglimpse()head()summary()duplicated()colSum(is.na(dataset))\n\n\n\n\nCode\nglimpse(globalPop)\n\n\nRows: 6,204\nColumns: 6\n$ Country    &lt;chr&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanistan\",…\n$ Year       &lt;dbl&gt; 1996, 1998, 2000, 2002, 2004, 2006, 2008, 2010, 2012, 2014,…\n$ Young      &lt;dbl&gt; 83.6, 84.1, 84.6, 85.1, 84.5, 84.3, 84.1, 83.7, 82.9, 82.1,…\n$ Old        &lt;dbl&gt; 4.5, 4.5, 4.5, 4.5, 4.5, 4.6, 4.6, 4.6, 4.6, 4.7, 4.7, 4.7,…\n$ Population &lt;dbl&gt; 21559.9, 22912.8, 23898.2, 25268.4, 28513.7, 31057.0, 32738…\n$ Continent  &lt;chr&gt; \"Asia\", \"Asia\", \"Asia\", \"Asia\", \"Asia\", \"Asia\", \"Asia\", \"As…\n\n\n\n\n\n\nCode\nhead(globalPop)\n\n\n# A tibble: 6 × 6\n  Country      Year Young   Old Population Continent\n  &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;    \n1 Afghanistan  1996  83.6   4.5     21560. Asia     \n2 Afghanistan  1998  84.1   4.5     22913. Asia     \n3 Afghanistan  2000  84.6   4.5     23898. Asia     \n4 Afghanistan  2002  85.1   4.5     25268. Asia     \n5 Afghanistan  2004  84.5   4.5     28514. Asia     \n6 Afghanistan  2006  84.3   4.6     31057  Asia     \n\n\n\n\n\n\nCode\nsummary(globalPop)\n\n\n   Country               Year          Young             Old       \n Length:6204        Min.   :1996   Min.   : 15.50   Min.   : 1.00  \n Class :character   1st Qu.:2010   1st Qu.: 25.70   1st Qu.: 6.90  \n Mode  :character   Median :2024   Median : 34.30   Median :12.80  \n                    Mean   :2023   Mean   : 41.66   Mean   :17.93  \n                    3rd Qu.:2038   3rd Qu.: 53.60   3rd Qu.:25.90  \n                    Max.   :2050   Max.   :109.20   Max.   :77.10  \n   Population         Continent        \n Min.   :      3.3   Length:6204       \n 1st Qu.:    605.9   Class :character  \n Median :   5771.6   Mode  :character  \n Mean   :  34860.9                     \n 3rd Qu.:  22711.0                     \n Max.   :1807878.6                     \n\n\n\n\n\n\nCode\nglobalPop[duplicated(globalPop),]\n\n\n# A tibble: 0 × 6\n# ℹ 6 variables: Country &lt;chr&gt;, Year &lt;dbl&gt;, Young &lt;dbl&gt;, Old &lt;dbl&gt;,\n#   Population &lt;dbl&gt;, Continent &lt;chr&gt;\n\n\n\n\n\n\nCode\ncolSums(is.na(globalPop))\n\n\n   Country       Year      Young        Old Population  Continent \n         0          0          0          0          0          0 \n\n\n\n\n\n\n\n\n3.3 Animated data visualisation: gganimate methods\ngganimate extends ggplot2 by adding animation-specific grammar, allowing plots to dynamically change over time with customizable transitions.\n\ntransition_*(): Defines how data is distributed and related over time.\nview_*(): Controls how positional scales change during the animation.\nshadow_*(): Determines how data from other time points is displayed at a given moment.\nenter_*() / exit_*(): Specifies how new data enters and old data exits during the animation.\nease_aes(): Adjusts how aesthetics transition smoothly over time.\n\n\n3.3.1 Building a static population bubble plot\nThe code below uses the basic ggplot2 function to create a static bubble plot.\n\n\nCode\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') \n\n\n\n\n\n\n\n\n\n\n\n3.3.2 Building the animated bubble plot\nThe code below uses the two functions to create an animated bubble plot. - transition_time() of gganimate is usedto create transition through distinct states in time (i.e.: Year) - ease_aes() is used to control easing of aesthetics. The default is linear. Other methods are: quadratic, cubic, quartic, quintic, sine, circular, exponential, elastic, back, and bounce.\n\n\nCode\nggplot(globalPop, aes(x = Old, y = Young, \n                      size = Population, \n                      colour = Country)) +\n  geom_point(alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(title = 'Year: {frame_time}', \n       x = '% Aged', \n       y = '% Young') +\n  transition_time(Year) +       \n  ease_aes('linear')          \n\n\n\n\n\n\n\n\n\n\n\n\n3.4 Animated data visualisation: plotly\nIn the Plotly R package, both ggplotly() and plot_ly() enable keyframe animations using the frame argument or aesthetic. Additionally, they support the ids argument or aesthetic to ensure smooth transitions for objects with the same ID, promoting object constancy during animations.\n\n3.4.1 Building an animated bubble plot: ggplotly() method\n\nPlot()Code()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe animated bubble plot will includes a play/pause button and a slider component for controlling the animation\n\n\n\n\n\n\n\nCode\ngg &lt;- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7, \n             show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young')\n\nggplotly(gg)\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nA static bubble plot is created using ggplot2 functions and saved as an R object named gg.\nThe ggplotly() function is then used to convert this static plot into an animated SVG object.\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\nYou will notice that the show.legend = FALSE argument was used, but the legend still appears on the plot. To overcome this problem, theme(legend.position=none) should be used as shown in the plot and code below.\n\n\n\n\n\n3.4.2 Building an animated bubble plot: ggplotly() method - without legend\n\nPlot()Code()\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ngg &lt;- ggplot(globalPop, \n       aes(x = Old, \n           y = Young, \n           size = Population, \n           colour = Country)) +\n  geom_point(aes(size = Population,\n                 frame = Year),\n             alpha = 0.7) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  labs(x = '% Aged', \n       y = '% Young') + \n  theme(legend.position='none')\n\nggplotly(gg)\n\n\n\n\n\n\n\n\n3.5 Reference\n\nGetting Started\nVisit this [link]for a very interesting implementation of gganimate by your senior\nBuilding an animation step-by-step with gganimate.\nCreating a composite gif with multiple gganimate panels\n\n\n\n3.6 Overall reference\n\nKam, T.S. (2023).3 Programming Interactive Data Visualisation with R\n\n\n\n\n\n\n\nKey takeaways\n\n\n\n\nLearnt the Importance of Animated Graphics\nPackages and Tools used: gganimate, plotly, tidyr, and dplyr\nLearnt how to create animated visualizations - Static Bubble Plot, Animating with gganimate, and Animating with plotly\n\n\n\n\n\n5.0 Further exploration\n\nTo explore animated plot that shows how Singapore’s population has changed over the years.\n\n\nGraphCode\n\n\nObservations:\n\nReflect a society transitioning to an aging population\nSteady Population Growth Until 2030, but population decline after 2030.\nBy 2050, the population drops to 4,635.1, marking a decrease of approximately 9.6% from the peak.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Prepare the dataset and filter for 'Singapore'\nsingapore_data &lt;- globalPop %&gt;%\n  filter(Country == \"Singapore\") %&gt;%\n  mutate(Year = as.integer(Year), Population = as.numeric(Population))\n\np &lt;- ggplot(singapore_data, aes(x = Year, y = Population, group = 1)) +\n  # Line showing the trajectory of population over time\n  geom_line(color = \"blue\", linewidth = 1) +\n  # Moving dot to emphasize animation\n  geom_point(color = \"red\", size = 4) +\n  labs(title = \"Population Change in Singapore\", \n       subtitle = \"Year: {frame_time}\",\n       x = \"Year\", \n       y = \"Population\") +\n  theme_minimal() +\n  transition_reveal(Year) +  # Reveals the line over time\n  ease_aes('linear')\n\np\n\n\n\n\n\n\nTo explore static bubble plot for the sum of population across continent\n\n\nGraphCode\n\n\nObservations:\n\nAsia has the highest population - largest bubble\nAfrica has a significantly large population - second largest bubble\nOceania has the smallest population - smallest bubble\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(dplyr)\n\nglobalPop &lt;- read_xls(\"data/GlobalPopulation.xls\",\n                      sheet=\"Data\")\n\n\n\n\nCode\n# Process data for all continents\ndata_continent &lt;- globalPop %&gt;%\n  group_by(Year, Continent) %&gt;%\n  summarise(TotalPopulation = sum(Population, na.rm = TRUE), .groups = 'drop')\n\n\n\n\nCode\n# Create a static bubble plot\nggplot(data_continent, aes(x = Continent, y = TotalPopulation, size = TotalPopulation, color=Continent)) +\n  geom_point(alpha = 0.7) +\n  scale_size_area(max_size = 15) +\n  labs(\n    title = \"Total Population by Continent\",\n    x = \"Continent\",\n    y = \"Total Population (Thousands)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +  # Remove legend\n  coord_flip()  # Flip coordinates for better readability\n\n\n\n\n\n\nTo explore animated plot that visualizes the sum of population growth by continent over the years.\n\n\nGraphCode\n\n\nObservations:\n\nAsia has the highest population growth - trajectory is steep and significantly outpaces other continents\nAfrica’s population is also increasing rapidly, showing a strong upward trend.\nEurope, North America, South America show slow growth, with relatively flat trends\nOceania has the lowest population, maintaining a nearly constant trend.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Process data for all continents\ndata_continent &lt;- globalPop %&gt;%\n  group_by(Year, Continent) %&gt;%\n  summarise(TotalPopulation = sum(Population, na.rm = TRUE), .groups = 'drop')\n\n\n\n\nCode\n# Create an animated plot for population growth by continent\nggplot(data_continent, aes(x = Year, y = TotalPopulation, color = Continent, group = Continent)) +\n  geom_line(size = 1) +\n  geom_point(size = 3) +\n  labs(\n    title = \"Population Growth by Continent Over the Years\",\n    x = \"Year\",\n    y = \"Total Population (Thousands)\",\n    color = \"Continent\"\n  ) +\n  theme_minimal() +\n  transition_reveal(Year)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02.html",
    "href": "Hands-on_Ex/Hands-on_Ex02.html",
    "title": "Hands-on Exercise 02",
    "section": "",
    "text": "With the assistance of ChatGPT"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex02.html#beyond-ggplot2-fundamentals",
    "href": "Hands-on_Ex/Hands-on_Ex02.html#beyond-ggplot2-fundamentals",
    "title": "Hands-on Exercise 02",
    "section": "2. Beyond ggplot2 Fundamentals",
    "text": "2. Beyond ggplot2 Fundamentals\n\n2.1 Learning Outcome\nIn this chapter, we will be exploring several ggplot2 extensions to enhance the elegance and effectiveness of statistical graphics. The objectives will be to:\n\nUse the ggrepel package to control annotation placement on graphs.\nCreate publication-quality visuals with ggthemes and hrbrthemes.\nCombine multiple ggplot2 graphs into composite figures using the patchwork package.\n\n\n\n2.2 Getting started\n\n2.2.1 Installing and loading the required libraries\nBeside tidyverse, below are the four packages that will be used.\n\nggrepel: provides geoms for avoiding overlapping text labels in ggplot2.\nggthemes: offers additional themes, geoms, and scales for enhancing ggplot2 visuals.\nhrbrthemes: focuses on typography-centric themes and components for ggplot2.\npatchwork: allow for the creation of composite figures using ggplot2\n\nCode to check if the packages have been installed, and to load them into our R environment.\n\n\nCode\npacman::p_load(ggrepel, patchwork,\n               ggthemes, hrbrthemes,\n               tidyverse)\n\n\n\n\n2.2.2 Importing data\n\n\n\n\n\n\nNote\n\n\n\n\nThis section data is the same dataset used in Hands-on_Ex01\n\n\n\nWe will use a data file called Exam_data.csv which contains the year-end exam results of a group of Primary 3 students from a local school.\nThe code below will be used to import the “exam_data.csv” file into the R environment using the read_csv() function from the readr package, which is part of the tidyverse.\n\n\nCode\nexam_data &lt;- read_csv(\"data/Exam_data.csv\", show_col_types = FALSE)\n\n\nWe will check the dataset using below\n\nglimpse(): provides a transposed overview of a dataset, showing variables and their types in a concise format.\nhead(): displays the first few rows of a dataset (default is 6 rows) to give a quick preview of the data.\nsummary(): generates a statistical summary of each variable, including measures like mean, median, and range for numeric data.\nduplicated():returns a logical vector indicating which elements or rows in a vector or data frame are duplicates.\ncolSums(is.na()): counts the number of missing values (NA) in each column of the data frame.\nspec(): use spec() to quickly inspect the column\n\n\nglimpse()head()summary()duplicated()colSum(is.na(dataset))\n\n\n\n\nCode\nglimpse(exam_data)\n\n\nRows: 322\nColumns: 7\n$ ID      &lt;chr&gt; \"Student321\", \"Student305\", \"Student289\", \"Student227\", \"Stude…\n$ CLASS   &lt;chr&gt; \"3I\", \"3I\", \"3H\", \"3F\", \"3I\", \"3I\", \"3I\", \"3I\", \"3I\", \"3H\", \"3…\n$ GENDER  &lt;chr&gt; \"Male\", \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"Male\", \"M…\n$ RACE    &lt;chr&gt; \"Malay\", \"Malay\", \"Chinese\", \"Chinese\", \"Malay\", \"Malay\", \"Chi…\n$ ENGLISH &lt;dbl&gt; 21, 24, 26, 27, 27, 31, 31, 31, 33, 34, 34, 36, 36, 36, 37, 38…\n$ MATHS   &lt;dbl&gt; 9, 22, 16, 77, 11, 16, 21, 18, 19, 49, 39, 35, 23, 36, 49, 30,…\n$ SCIENCE &lt;dbl&gt; 15, 16, 16, 31, 25, 16, 25, 27, 15, 37, 42, 22, 32, 36, 35, 45…\n\n\n\n\n\n\nCode\nhead(exam_data)\n\n\n# A tibble: 6 × 7\n  ID         CLASS GENDER RACE    ENGLISH MATHS SCIENCE\n  &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Student321 3I    Male   Malay        21     9      15\n2 Student305 3I    Female Malay        24    22      16\n3 Student289 3H    Male   Chinese      26    16      16\n4 Student227 3F    Male   Chinese      27    77      31\n5 Student318 3I    Male   Malay        27    11      25\n6 Student306 3I    Female Malay        31    16      16\n\n\n\n\n\n\nCode\nsummary(exam_data)\n\n\n      ID               CLASS              GENDER              RACE          \n Length:322         Length:322         Length:322         Length:322        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n    ENGLISH          MATHS          SCIENCE     \n Min.   :21.00   Min.   : 9.00   Min.   :15.00  \n 1st Qu.:59.00   1st Qu.:58.00   1st Qu.:49.25  \n Median :70.00   Median :74.00   Median :65.00  \n Mean   :67.18   Mean   :69.33   Mean   :61.16  \n 3rd Qu.:78.00   3rd Qu.:85.00   3rd Qu.:74.75  \n Max.   :96.00   Max.   :99.00   Max.   :96.00  \n\n\n\n\n\n\nCode\nexam_data[duplicated(exam_data),]\n\n\n# A tibble: 0 × 7\n# ℹ 7 variables: ID &lt;chr&gt;, CLASS &lt;chr&gt;, GENDER &lt;chr&gt;, RACE &lt;chr&gt;,\n#   ENGLISH &lt;dbl&gt;, MATHS &lt;dbl&gt;, SCIENCE &lt;dbl&gt;\n\n\n\n\n\n\nCode\ncolSums(is.na(exam_data))\n\n\n     ID   CLASS  GENDER    RACE ENGLISH   MATHS SCIENCE \n      0       0       0       0       0       0       0 \n\n\n\n\nCode\nspec(exam_data)\n\n\ncols(\n  ID = col_character(),\n  CLASS = col_character(),\n  GENDER = col_character(),\n  RACE = col_character(),\n  ENGLISH = col_double(),\n  MATHS = col_double(),\n  SCIENCE = col_double()\n)\n\n\n\n\n\nThe exam_data tibble contains seven attributes, as shown above:\n\nCategorical attributes: ID, CLASS, GENDER, RACE\nContinuous attributes: MATHS, ENGLISH, SCIENCE\n\n\n\n\n2.3 Beyond ggplot2 Annotation: ggrel\nAnnotating statistical graphs can be challenging, particularly when dealing with a large number of data points.\n\n\nShow the code\n ggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              linewidth=0.5) +  \n  geom_label(aes(label = ID), \n             hjust = .5, \n             vjust = -.5) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nReplaced size=0.5 with linewidth-0.5 in the geom_smooth() function to align the code with the latest ggplot2 standards.\n\n\n\nggrepel is an extension of ggplot2 that provides geoms to prevent overlapping text labels.\nIt replaces geom_text() with geom_text_repel() and geom_label() with geom_label_repel(), ensuring labels repel from each other, data points, and plot edges.This enhances the clarity and readability of the chart.\n\n2.3.1 Working with ggrepel\n\n\nShow the code\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  geom_label_repel(aes(label = ID), \n                   fontface = \"bold\") +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThere is a warning message:\n\ngeom_smooth()` using formula = ‘y ~ x’\nWarning: ggrepel: 321 unlabeled data points (too many overlaps). Consider increasing max.overlaps\nthe warning message means that geom_label_repeal function was unable to place labels for 321 data points because they are too close to each other.\n\nResponse:\n\nOption 1: Increase max.overlaps\nOption 2: Label only specific points which involves creating a new column that marks points to label and passing the column into aes(label = ..)\nOption 3: Potentially to reduce the point size or transparency with geom_point()\n\n\n\n\n\n2.3.2 Further exploration with ggrepel\nLeveraging on Hands-on_Ex01, we added the below attributes:\n\ngeom_point:customize the color = “darkblue” and changed the size = 0.5, with a solid circle shape where shape = 16.\ngeom_smooth:changed the regression line, color = “red”, made thicker size = 1, and to display as a dashed line, linetype = “dashed”.\nRefer here for the different ggplot2 point shapes.\nRefer here for the different ggplot2 line types.\n\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(ggrepel)\n\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  # Scatter plot with customized point appearance\n  geom_point(color = \"darkblue\", size = 0.5, shape = 16) +  # change point color, size, and shape\n  \n  # Linear regression line with customized color and line type\n  geom_smooth(method=lm, \n              size=1, \n              color=\"red\",    # line color\n              linetype=\"dashed\") +  # line style\n  geom_label_repel(aes(label = ID), \n                   fontface = \"bold\") +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\n\n\n\n\n\n\n\n\n\n\n\n2.4 Beyond ggplot2 themes\nThere are 8 different built-in themes within ggplot2. They are:\n\ntheme_gray():default theme in ggplot2, with a gray background with white grid lines for readability.\ntheme_bw(): clean, minimalistic theme with a white background and black grid lines.\ntheme_classic():simple theme with a white background and no grid lines.\ntheme_dark():theme with a dark background and light-colored text and grid lines.\ntheme_light():light background theme with light-colored grid lines.\ntheme_linedraw():theme that uses black lines on a white background.\ntheme_minimal():minimalist theme that reduces non-data elements.\ntheme_void():clean theme with no background, grid lines, or axes.\n\n\ntheme_gray()theme_bw()theme_classic()theme_dark()theme_light()theme_linedraw()theme_minimal()theme_void()\n\n\n\n\nCode\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  theme_gray() +\n  ggtitle(\"Distribution of Maths scores\") \n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  theme_bw() +\n  ggtitle(\"Distribution of Maths scores\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  theme_classic() +\n  ggtitle(\"Distribution of Maths scores\") \n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  theme_dark() +\n  ggtitle(\"Distribution of Maths scores\") \n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  theme_light() +\n  ggtitle(\"Distribution of Maths scores\") \n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  theme_linedraw() +\n  ggtitle(\"Distribution of Maths scores\") \n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  theme_minimal() +\n  ggtitle(\"Distribution of Maths scores\") \n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  theme_void() +\n  ggtitle(\"Distribution of Maths scores\") \n\n\n\n\n\n\n\n\n\n\n\n\nRefer to this link to learn more about ggplot2 Themes\n\n2.4.1 Working with ggtheme package\nggthemes offers a collection of ggplot2 themes that replicate the style of plots from the likes of Edward Tufte, Stephen Few, FiveThirtyEight, The Economist, Stata, Excel, and The Wall Street Journal, among others.\nBelow are some of the examples of the different themes available.\nRefer here for the comprehensive list and descrptions of all available themes.\nInstall and load ggthemespackages\n\n\nCode\ninstall.packages(\"ggthemes\")\nlibrary(ggthemes)\n\n\n\ntheme_economist()theme_wsj()theme_fivethirtyeight()theme_tufte()theme_excel()\n\n\n\n\nCode\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_economist()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_wsj()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_fivethirtyeight()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_tufte()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_excel()\n\n\n\n\n\n\n\n\n\n\n\n\nRefer to this link to learn more about the ggplot2 themes\n\n\n2.4.2 Working with hrbthemes package\nThe hrbrthemes package provides typography centric themes and theme components for ggplot2. This includes where labels are placed and the fonts used.\n\n\nShow the code\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum()\n\n\n\n\n\n\n\n\n\nThe second goal is to boost productivity in a production workflow, which is the intended setting for using the elements of hrbrthemes.\nRefer here to learn more.\n\n\nShow the code\ninstall.packages(\"hrbrthemes\")\nlibrary(hrbrthemes)\n\nggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  ggtitle(\"Distribution of Maths scores\") +\n  theme_ipsum(axis_title_size = 18,\n              base_size = 15,\n              grid = \"Y\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat can we learn from the cde chunk above?\n\n\n\n\naxis_title_size: sets the font size of the axis title to 18\nbase_size: adjusts the default axis label size to 15\ngrid: remove the x-axis grid lines\n\n\n\n\n\n\n2.5 Beyond single graph\nCreating multiple graphs is often necessary to convey a compelling visual story. Several ggplot2 extensions offer functions for combining multiple graphs into a single figure.\nIn this section, we will learn how to create a composite/ combined plot by merging multiple graphs. First, let create three statistical graphics by using the codes below.\n\nHistogram 1 - distribution of Maths scoresHistogram 2 - distribution of English scoresScatterplot - English VS Maths scores\n\n\n\n\nShow the code\np1 &lt;- ggplot(data=exam_data, \n             aes(x = MATHS)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") + \n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of Maths scores\")\n\np1\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\np2 &lt;- ggplot(data=exam_data, \n             aes(x = ENGLISH)) +\n  geom_histogram(bins=20, \n                 boundary = 100,\n                 color=\"grey25\", \n                 fill=\"grey90\") +\n  coord_cartesian(xlim=c(0,100)) +\n  ggtitle(\"Distribution of English scores\")\n\np2\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\np3 &lt;- ggplot(data=exam_data, \n             aes(x= MATHS, \n                 y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              linewidth=0.5) +  \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100)) +\n  ggtitle(\"English scores versus Maths scores for Primary 3\")\n\np3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nReplaced size=0.5 with linewidth-0.5 in the geom_smooth() function to align the code with the latest ggplot2 standards.\n\n\n\n\n\n\n\n2.5.1 Creating composits graphics: pathwork methods\nThere are several ggplot2 extension’s functions that support the preparation of composite figures such as grid.arrange() of gridExtra package and plot_grid() of cowplot package.\nIn this section, we will be using Patchwork, a ggplot2 extension specifically designed to combine multiple ggplot2 graphs into a single figure.\nThe Patchwork package offers a straightforward synatx, making it easy to create custom layouts. Key features include\n\nTwo column layouts using + operator\nSubplot grouping using () operator\nTwo-row layouts using / operator\nOne-row layout using | operator\n\n\n\nCode\ninstall.packages(\"patchwork\")\nlibrary(patchwork)\n\n\n\n\n2.5.2 Combining two ggplot2 graphs\n\nUsing + operatorUsing () operatorUsing / operator\n\n\n+ combines plots into a single row\n\n\nCode\np1+p2\n\n\n\n\n\n\n\n\n\n\n\n() use for grouping and layout adjustments\n\n\nCode\np1 + p2 + plot_layout(ncol=2,widths=c(1,2))\n\n\n\n\n\n\n\n\n\n\n\n/ use to stack plots into a single column\n\n\nCode\np1/p2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.5.3 Combining 2 or more ggplot2 graphs\nWe can also combine 2 or more ggplots graph in one composite figure.\n\nUsing +&|&() operator\n\n\n\n\nCode\n(p1/p2) | p3\n\n\n\n\n\n\n\n\n\nTo learn more about Patchwork, refer here\n\n\n\n\n\n2.5.4 Creating a composite figure with tag\nPatchwork also includes auto-tagging capabilities to label subplots within a figure, making it easier to identify them in text.\n\nNumeralsDigitsLettersCustomizing tags\n\n\n\n\nShow the code\n((p1 / p2) | p3) + \n  plot_annotation(tag_levels = 'I')\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n((p1 / p2) | p3) + \n  plot_annotation(tag_levels = '1')\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n((p1 / p2) | p3) + \n  plot_annotation(tag_levels = 'A')\n\n\n\n\n\n\n\n\n\n\n\n\nUsing either tag_prefix and/ or tag_suffix to format tags\nAdded the word “Figure” before each Roman numeral\n\n\n\nCode\n((p1 / p2) | p3) + \n  plot_annotation(tag_levels = \"I\", tag_prefix = \"Figure \", tag_suffix = \"\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.5.5 Creating figure with insert\nIn addition to arranging plots side-by-side based on a specified layout, the Patchwork package provides the insert_element() function.\nThis allows user to freely place one or more plots or graphic elements over or below another plot, offering greater flexibility in design.\nRefer here for the insert_element documentation.\n\n\nCode\np3 + inset_element(p2, \n                   left = 0.02, \n                   bottom = 0.7, \n                   right = 0.5, \n                   top = 1)\n\n\n\n\n\n\n\n\n\n\n\n2.5.6 Creating a composite figure by using patchwork and ggtheme\nThe figure below is created by combining patchwork and the other ggthemes packages.\n\ntheme_econmist()theme_wsj()theme_fivethirtyeight()theme_tufte()theme_excel()\n\n\n\n\nCode\npatchwork &lt;- (p1 / p2) | p3\npatchwork & theme_economist() + \n  theme(plot.title=element_text(size =8),\n        axis.title.y=element_text(size = 9,\n                              angle = 0,\n                              vjust=0.9),\n        axis.title.x=element_text(size = 9))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npatchwork &lt;- (p1 / p2) | p3\npatchwork & theme_wsj() +\n  theme(plot.title=element_text(size =8),\n        axis.title.y=element_text(size = 9,\n                              angle = 0,\n                              vjust=0.9),\n        axis.title.x=element_text(size = 9))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npatchwork &lt;- (p1 / p2) | p3\npatchwork & theme_fivethirtyeight() +\n  theme(plot.title=element_text(size =8),\n        axis.title.y=element_text(size = 9,\n                              angle = 0,\n                              vjust=0.9),\n        axis.title.x=element_text(size = 9))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npatchwork &lt;- (p1 / p2) | p3\npatchwork & theme_tufte() +\n  theme(plot.title=element_text(size =8),\n        axis.title.y=element_text(size = 9,\n                              angle = 0,\n                              vjust=0.9),\n        axis.title.x=element_text(size = 9))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\npatchwork &lt;- (p1 / p2) | p3\npatchwork & theme_excel() +\n  theme(plot.title=element_text(size =8),\n        axis.title.y=element_text(size = 9,\n                              angle = 0,\n                              vjust=0.9),\n        axis.title.x=element_text(size = 9))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nReduced the font size of the plot. title to 8 to improve plot’s readability\nReduced the font size of the axis.title.y to 9 and rotated the y-axis title to vertical (90 degrees) to improve plot’s readability\nLikewise, reduced the font size of the axis.title.x to 9 to improve plot’s readability\n\n\n\n\n\n\n2.6 References\n\nKam, T.S(2023). 2 Beyond ggplot2 Fundamentals\nggrepel\nggthemes\nhrbrthemes\nggplot tips: Arranging plots\nggplot2 Theme Elements Demostration\nggplot2 Theme Elements Reference Sheet\n\n\n2.6.1 Additional references\n\nQuarto_HTML_Code_Blocks\n\n\n\n\n2.7 Takeaway\n\n\n\n\n\n\nKey takeaways\n\n\n\n\nLearnt about the code-fold and code-summary chunk attributes to hide executable source code. where:\nValid values for code-fold include:\n\n\n\nValue\nBehavior\n\n\n\n\nfalse\nNo folding (default)\n\n\ntrue\nFold code (initially hidden)\n\n\nshow\nFold code (initially shown)\n\n\n\nLearnt about the different R packages\n\ninstall.packages(“readr”) library(readr)\ninstall.packages(“ggplot2”) library(ggplot2)\ninstall.packages(“ggrepel”) library(ggrepel)\ninstall.packages(“ggthemes”) library(ggthemes)\n\nEnhanced annotations with the use of ggrepeal - help to repel overlapping text labels, ensuring clear and readbable annotations\nIntroducing of professional themes with ggthemes and hrbrthemes - offer additional themes , geoms, and improved asthetics and typography.\nCombining plots with patchwork - allows for the creation of composite figures by combining multiple ggplot2 graphs together."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex00.html",
    "href": "Hands-on_Ex/Hands-on_Ex00.html",
    "title": "Hands-on Exercise 00: Working with tidyverse",
    "section": "",
    "text": "Loading tidyverse onto r environment by using the code chunk below\n\n\nCode\npacman:: p_load(tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex00.html#getting-started",
    "href": "Hands-on_Ex/Hands-on_Ex00.html#getting-started",
    "title": "Hands-on Exercise 00: Working with tidyverse",
    "section": "",
    "text": "Loading tidyverse onto r environment by using the code chunk below\n\n\nCode\npacman:: p_load(tidyverse)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex00.html#importing-data",
    "href": "Hands-on_Ex/Hands-on_Ex00.html#importing-data",
    "title": "Hands-on Exercise 00: Working with tidyverse",
    "section": "Importing Data",
    "text": "Importing Data\nCode chunk below uses read_csv() of readr to import REALIS2019.csv into r environment as a tibble data.frame.\n\n\nCode\nrealis2019 &lt;- read_csv(\"data/REALIS2019.csv\")\npopdata_fat &lt;- read_csv(\"data/PopData2019_fat.csv\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex00.html#pivoting-data",
    "href": "Hands-on_Ex/Hands-on_Ex00.html#pivoting-data",
    "title": "Hands-on Exercise 00: Working with tidyverse",
    "section": "Pivoting Data",
    "text": "Pivoting Data\n\n\nCode\n#pivot_longer() \"lengthens\" data, increasing the number of rows and decreasing the number of columns\n\npopdata_long &lt;- popdata_fat %&gt;% #pipe: to combine diff functions tgt\n  pivot_longer(c(3:21), #column\n               names_to = \"Age Group\", #column headers to rows\n               values_to = \"Population\") #values to rows\n\n\n\n\nCode\nwrite_rds(popdata_long, \"rds/popdata_long.rds\") #rds: compact r file"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex00.html#selecting-data",
    "href": "Hands-on_Ex/Hands-on_Ex00.html#selecting-data",
    "title": "Hands-on Exercise 00: Working with tidyverse",
    "section": "Selecting Data",
    "text": "Selecting Data\n\n\nCode\nrealis2019_selected &lt;- realis2019 %&gt;%\n  select(`Project Name`,\n         `Transacted Price ($)`,\n         `Type of Sale`,\n         `Unit Price ($ psm)`,\n         `Property Type`)\nrealis2019_selected\n\n\n# A tibble: 19,515 × 5\n   `Project Name`     `Transacted Price ($)` `Type of Sale` `Unit Price ($ psm)`\n   &lt;chr&gt;                               &lt;dbl&gt; &lt;chr&gt;                         &lt;dbl&gt;\n 1 PEIRCE VIEW                        840000 Resale                         7434\n 2 FLORIDA PARK                      3040000 Resale                         9737\n 3 BULLION PARK                       860000 Resale                        11467\n 4 CASTLE GREEN                      1000000 Resale                         9346\n 5 HAPPY ESTATE                      7000000 Resale                        10183\n 6 TEACHER'S HOUSING…                2880000 Resale                        12659\n 7 THE PANORAMA                      1510000 Resale                        16064\n 8 THE PANORAMA                       710000 Resale                        16905\n 9 CHIP THYE GARDEN                  2800000 Resale                        13500\n10 TEACHER'S HOUSING…                2300000 Resale                         9935\n# ℹ 19,505 more rows\n# ℹ 1 more variable: `Property Type` &lt;chr&gt;"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex00.html#filtering-data-using-dplyr-package",
    "href": "Hands-on_Ex/Hands-on_Ex00.html#filtering-data-using-dplyr-package",
    "title": "Hands-on Exercise 00: Working with tidyverse",
    "section": "Filtering Data using dplyr package",
    "text": "Filtering Data using dplyr package\n\n\nCode\nrealis2019_filtered &lt;- realis2019_selected %&gt;%\n  filter (`Property Type` == \"Condominium\" |\n            `Property Type` == \"Apartment\") %&gt;%\n  filter (`Type of Sale` == \"New Sale\") %&gt;%\n  filter (`Unit Price ($ psm)` &lt;= 13000)\n\nrealis2019_filtered\n\n\n# A tibble: 87 × 5\n   `Project Name`     `Transacted Price ($)` `Type of Sale` `Unit Price ($ psm)`\n   &lt;chr&gt;                               &lt;dbl&gt; &lt;chr&gt;                         &lt;dbl&gt;\n 1 RIVERFRONT RESIDE…                1029000 New Sale                      12863\n 2 RIVERFRONT RESIDE…                 871000 New Sale                      12809\n 3 RIVERFRONT RESIDE…                1940000 New Sale                      12848\n 4 RIVERFRONT RESIDE…                1030000 New Sale                      12875\n 5 RIVERFRONT RESIDE…                2061000 New Sale                      12962\n 6 RIVERFRONT RESIDE…                 762000 New Sale                      12915\n 7 RIVERFRONT RESIDE…                1001000 New Sale                      12513\n 8 RIVERFRONT RESIDE…                1271000 New Sale                      12838\n 9 RIVERFRONT RESIDE…                1310000 New Sale                      12970\n10 RIVERFRONT RESIDE…                1339000 New Sale                      13000\n# ℹ 77 more rows\n# ℹ 1 more variable: `Property Type` &lt;chr&gt;\n\n\n\nPutting all together\n\n\nCode\nrealis2019_end &lt;- realis2019 %&gt;%\n  select(`Project Name`,\n         `Transacted Price ($)`,\n         `Type of Sale`,\n         `Unit Price ($ psm)`,\n         `Property Type`)   %&gt;%\n  filter (`Property Type` == \"Condominium\" |\n            `Property Type` == \"Apartment\") %&gt;%\n  filter (`Type of Sale` == \"New Sale\") %&gt;%\n  filter (`Unit Price ($ psm)` &lt;= 13000)\n\nrealis2019_end\n\n\n# A tibble: 87 × 5\n   `Project Name`     `Transacted Price ($)` `Type of Sale` `Unit Price ($ psm)`\n   &lt;chr&gt;                               &lt;dbl&gt; &lt;chr&gt;                         &lt;dbl&gt;\n 1 RIVERFRONT RESIDE…                1029000 New Sale                      12863\n 2 RIVERFRONT RESIDE…                 871000 New Sale                      12809\n 3 RIVERFRONT RESIDE…                1940000 New Sale                      12848\n 4 RIVERFRONT RESIDE…                1030000 New Sale                      12875\n 5 RIVERFRONT RESIDE…                2061000 New Sale                      12962\n 6 RIVERFRONT RESIDE…                 762000 New Sale                      12915\n 7 RIVERFRONT RESIDE…                1001000 New Sale                      12513\n 8 RIVERFRONT RESIDE…                1271000 New Sale                      12838\n 9 RIVERFRONT RESIDE…                1310000 New Sale                      12970\n10 RIVERFRONT RESIDE…                1339000 New Sale                      13000\n# ℹ 77 more rows\n# ℹ 1 more variable: `Property Type` &lt;chr&gt;\n\n\n\n\n\n\n\n\nTakeaway\n\n\n\n\necho: controls whether the code is displayed\n\nIf echo = TRUE (default), the code is shown in the rendered document.\nIf echo = FALSE, the code is hidden, but the output or results (if any) are still included.\n\neval: controls whether the code is executed\n\nIf eval = TRUE (default), the code in the chunk is executed, and its results (e.g., output, plots) are included in the document.\nIf eval = FALSE, the code is not executed, and no output or results are included, though the code may still be visible if echo = TRUE.\n\n%&gt;%: used to combine multiple functions together\nChange environmental variables if needed"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "In this webpage, you will find my learning journey and deliverables of ISSS608 Visual Analytics and Applications. I am Godzilla. This is the course page of ISSS608 whereby I share my Hands-on Exercises, In-class Exercises, Take-Home Exercises and Project\nDo Linkledin page, and brief introduction\n\n\n\n\n:::{#quarto-navigation-envelope .hidden}\n[ISSS608 Coursework]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1zaWRlYmFyLXRpdGxl\"}\n[ISSS608 Coursework]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXItdGl0bGU=\"}\n[Hands-on Exercise]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6SGFuZHMtb24gRXhlcmNpc2U=\"}\n[Hands-on Exercise 00]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6SGFuZHMtb24gRXhlcmNpc2UgMDA=\"}\n[/Hands-on_Ex/Hands-on_Ex00.html]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6L0hhbmRzLW9uX0V4L0hhbmRzLW9uX0V4MDAuaHRtbA==\"}\n[Hands-on Exercise 01]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6SGFuZHMtb24gRXhlcmNpc2UgMDE=\"}\n[/Hands-on_Ex/Hands-on_Ex01.html]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6L0hhbmRzLW9uX0V4L0hhbmRzLW9uX0V4MDEuaHRtbA==\"}\n[Hands-on Exercise 02]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6SGFuZHMtb24gRXhlcmNpc2UgMDI=\"}\n[/Hands-on_Ex/Hands-on_Ex02.html]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6L0hhbmRzLW9uX0V4L0hhbmRzLW9uX0V4MDIuaHRtbA==\"}\n[Hands-on Exercise 03a]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6SGFuZHMtb24gRXhlcmNpc2UgMDNh\"}\n[/Hands-on_Ex/Hands-on_Ex03a.html]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6L0hhbmRzLW9uX0V4L0hhbmRzLW9uX0V4MDNhLmh0bWw=\"}\n[Hands-on Exercise 03b]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6SGFuZHMtb24gRXhlcmNpc2UgMDNi\"}\n[/Hands-on_Ex/Hands-on_Ex03b.html]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6L0hhbmRzLW9uX0V4L0hhbmRzLW9uX0V4MDNiLmh0bWw=\"}\n[Hands-on Exercise 04a]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6SGFuZHMtb24gRXhlcmNpc2UgMDRh\"}\n[/Hands-on_Ex/Hands-on_Ex04a.html]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6L0hhbmRzLW9uX0V4L0hhbmRzLW9uX0V4MDRhLmh0bWw=\"}\n[Hands-on Exercise 04b]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6SGFuZHMtb24gRXhlcmNpc2UgMDRi\"}\n[/Hands-on_Ex/Hands-on_Ex04b.html]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6L0hhbmRzLW9uX0V4L0hhbmRzLW9uX0V4MDRiLmh0bWw=\"}\n[Hands-on Exercise 04c]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6SGFuZHMtb24gRXhlcmNpc2UgMDRj\"}\n[/Hands-on_Ex/Hands-on_Ex04c.html]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6L0hhbmRzLW9uX0V4L0hhbmRzLW9uX0V4MDRjLmh0bWw=\"}\n[Hands-on Exercise 04d]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6SGFuZHMtb24gRXhlcmNpc2UgMDRk\"}\n[/Hands-on_Ex/Hands-on_Ex04d.html]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6L0hhbmRzLW9uX0V4L0hhbmRzLW9uX0V4MDRkLmh0bWw=\"}\n[In-class Exercise]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6SW4tY2xhc3MgRXhlcmNpc2U=\"}\n[In-class Exercise Outline]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6SW4tY2xhc3MgRXhlcmNpc2UgT3V0bGluZQ==\"}\n[/In-class_Ex/In-class_Outline.html]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6L0luLWNsYXNzX0V4L0luLWNsYXNzX091dGxpbmUuaHRtbA==\"}\n[In-class Exercise 01]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6SW4tY2xhc3MgRXhlcmNpc2UgMDE=\"}\n[https://public.tableau.com/views/In-class_Ex01_17371867213230/Dashboard1?:language=en-US&:sid=&:redirect=auth&:display_count=n&:origin=viz_share_link]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6aHR0cHM6Ly9wdWJsaWMudGFibGVhdS5jb20vdmlld3MvSW4tY2xhc3NfRXgwMV8xNzM3MTg2NzIxMzIzMC9EYXNoYm9hcmQxPzpsYW5ndWFnZT1lbi1VUyY6c2lkPSY6cmVkaXJlY3Q9YXV0aCY6ZGlzcGxheV9jb3VudD1uJjpvcmlnaW49dml6X3NoYXJlX2xpbms=\"}\n[In-class Exercise 02]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6SW4tY2xhc3MgRXhlcmNpc2UgMDI=\"}\n[https://public.tableau.com/views/In-class_Ex02_17377896182080/Dashboard2?:language=en-US&:sid=&:redirect=auth&:display_count=n&:origin=viz_share_link]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6aHR0cHM6Ly9wdWJsaWMudGFibGVhdS5jb20vdmlld3MvSW4tY2xhc3NfRXgwMl8xNzM3Nzg5NjE4MjA4MC9EYXNoYm9hcmQyPzpsYW5ndWFnZT1lbi1VUyY6c2lkPSY6cmVkaXJlY3Q9YXV0aCY6ZGlzcGxheV9jb3VudD1uJjpvcmlnaW49dml6X3NoYXJlX2xpbms=\"}\n[In-class Exercise 03]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6SW4tY2xhc3MgRXhlcmNpc2UgMDM=\"}\n[https://public.tableau.com/views/In-class_Ex03_17383925295110/Hands-on_Ex03?:language=en-US&:sid=&:redirect=auth&:display_count=n&:origin=viz_share_link]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6aHR0cHM6Ly9wdWJsaWMudGFibGVhdS5jb20vdmlld3MvSW4tY2xhc3NfRXgwM18xNzM4MzkyNTI5NTExMC9IYW5kcy1vbl9FeDAzPzpsYW5ndWFnZT1lbi1VUyY6c2lkPSY6cmVkaXJlY3Q9YXV0aCY6ZGlzcGxheV9jb3VudD1uJjpvcmlnaW49dml6X3NoYXJlX2xpbms=\"}\n[In-class Exercise 03-Story]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6SW4tY2xhc3MgRXhlcmNpc2UgMDMtU3Rvcnk=\"}\n[https://public.tableau.com/views/In-class_Ex03_Story/Myfirststory?:language=en-US&:sid=&:redirect=auth&:display_count=n&:origin=viz_share_link]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6aHR0cHM6Ly9wdWJsaWMudGFibGVhdS5jb20vdmlld3MvSW4tY2xhc3NfRXgwM19TdG9yeS9NeWZpcnN0c3Rvcnk/Omxhbmd1YWdlPWVuLVVTJjpzaWQ9JjpyZWRpcmVjdD1hdXRoJjpkaXNwbGF5X2NvdW50PW4mOm9yaWdpbj12aXpfc2hhcmVfbGluaw==\"}\n[In-class Exercise 03-Animation]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6SW4tY2xhc3MgRXhlcmNpc2UgMDMtQW5pbWF0aW9u\"}\n[https://public.tableau.com/views/In-class_Ex03_Animation/Dashboard6?:language=en-US&:sid=&:redirect=auth&:display_count=n&:origin=viz_share_link]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6aHR0cHM6Ly9wdWJsaWMudGFibGVhdS5jb20vdmlld3MvSW4tY2xhc3NfRXgwM19BbmltYXRpb24vRGFzaGJvYXJkNj86bGFuZ3VhZ2U9ZW4tVVMmOnNpZD0mOnJlZGlyZWN0PWF1dGgmOmRpc3BsYXlfY291bnQ9biY6b3JpZ2luPXZpel9zaGFyZV9saW5r\"}\n[Take-home Exercise]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6VGFrZS1ob21lIEV4ZXJjaXNl\"}\n[Take Home Exercise 1]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6VGFrZSBIb21lIEV4ZXJjaXNlIDE=\"}\n[/Take-home_Ex/Take-home_Ex01.html]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6L1Rha2UtaG9tZV9FeC9UYWtlLWhvbWVfRXgwMS5odG1s\"}\n[Home]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6SG9tZQ==\"}\n[/index.html]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6L2luZGV4Lmh0bWw=\"}\n[About]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6QWJvdXQ=\"}\n[/about.html]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLWludC1uYXZiYXI6L2Fib3V0Lmh0bWw=\"}\n:::\n\n\n\n:::{#quarto-meta-markdown .hidden}\n[About – ISSS608 Coursework]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLW1ldGF0aXRsZQ==\"}\n[About – ISSS608 Coursework]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLXR3aXR0ZXJjYXJkdGl0bGU=\"}\n[About – ISSS608 Coursework]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLW9nY2FyZHRpdGxl\"}\n[ISSS608 Coursework]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLW1ldGFzaXRlbmFtZQ==\"}\n[]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLXR3aXR0ZXJjYXJkZGVzYw==\"}\n[]{.hidden .quarto-markdown-envelope-contents render-id=\"cXVhcnRvLW9nY2FyZGRkZXNj\"}\n:::\n\n\n\n\n&lt;!-- --&gt;\n\n::: {.quarto-embedded-source-code}\n```````````````````{.markdown shortcodes=\"false\"}\n---\ntitle: \"About\"\n---\n\nIn this webpage, you will find my learning journey and deliverables of ISSS608 Visual Analytics and Applications. I am Godzilla. This is the course page of ISSS608 whereby I share my Hands-on Exercises, In-class Exercises, Take-Home Exercises and Project\n\nDo Linkledin page, and brief introduction \n``````````````````` :::"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01.html",
    "href": "Hands-on_Ex/Hands-on_Ex01.html",
    "title": "Hands-on Exercise 01",
    "section": "",
    "text": "With the assistance of ChatGPT"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex01.html#a-layered-grammar-of-graphics-ggplot2-methods",
    "href": "Hands-on_Ex/Hands-on_Ex01.html#a-layered-grammar-of-graphics-ggplot2-methods",
    "title": "Hands-on Exercise 01",
    "section": "1. A Layered Grammar of Graphics: ggplot2 methods",
    "text": "1. A Layered Grammar of Graphics: ggplot2 methods\n\n1.1 Learning Outcome\nIn this chapter, we will learn the basic principles and key components of ggplot2. we will get hands-on experience using these components to create statistical graphics based on the principles of the Layered Grammar of Graphics. By the end, we will be able to apply ggplot2’s essential graphical elements to create elegant and functional statistical visualizations.\n\n\n1.2 Getting started\n\n1.2.1 Installing and loading the required libraries\nThe code chunk below uses p_load() from the pacman package to check if the tidyverse packages are installed on my computer. If they are, they will be loaded into the R environment.If not, we will proceed to install pacman first.\n\n\nCode\npacman::p_load(tidyverse)\n\n\n\n\n1.2.2 Importing data\nWe will import exam_data.csv into R environment by using the read.csv()function and assign it to exam_data\n\n\nCode\nexam_data &lt;- read_csv(\"data/Exam_data.csv\", show_col_types = FALSE)\n\n\nWe will check the dataset using below\n\nglimpse(): provides a transposed overview of a dataset, showing variables and their types in a concise format.\nhead(): displays the first few rows of a dataset (default is 6 rows) to give a quick preview of the data.\nsummary(): generates a statistical summary of each variable, including measures like mean, median, and range for numeric data.\nduplicated():Returns a logical vector indicating which elements or rows in a vector or data frame are duplicates.\ncolSums(is.na()): Counts the number of missing values (NA) in each column of the data frame.\n\n\nglimpse()head()summary()duplicated()colSum(is.na(dataset))\n\n\n\n\nCode\nglimpse(exam_data)\n\n\nRows: 322\nColumns: 7\n$ ID      &lt;chr&gt; \"Student321\", \"Student305\", \"Student289\", \"Student227\", \"Stude…\n$ CLASS   &lt;chr&gt; \"3I\", \"3I\", \"3H\", \"3F\", \"3I\", \"3I\", \"3I\", \"3I\", \"3I\", \"3H\", \"3…\n$ GENDER  &lt;chr&gt; \"Male\", \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"Male\", \"M…\n$ RACE    &lt;chr&gt; \"Malay\", \"Malay\", \"Chinese\", \"Chinese\", \"Malay\", \"Malay\", \"Chi…\n$ ENGLISH &lt;dbl&gt; 21, 24, 26, 27, 27, 31, 31, 31, 33, 34, 34, 36, 36, 36, 37, 38…\n$ MATHS   &lt;dbl&gt; 9, 22, 16, 77, 11, 16, 21, 18, 19, 49, 39, 35, 23, 36, 49, 30,…\n$ SCIENCE &lt;dbl&gt; 15, 16, 16, 31, 25, 16, 25, 27, 15, 37, 42, 22, 32, 36, 35, 45…\n\n\n\n\n\n\nCode\nhead(exam_data)\n\n\n# A tibble: 6 × 7\n  ID         CLASS GENDER RACE    ENGLISH MATHS SCIENCE\n  &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 Student321 3I    Male   Malay        21     9      15\n2 Student305 3I    Female Malay        24    22      16\n3 Student289 3H    Male   Chinese      26    16      16\n4 Student227 3F    Male   Chinese      27    77      31\n5 Student318 3I    Male   Malay        27    11      25\n6 Student306 3I    Female Malay        31    16      16\n\n\n\n\n\n\nCode\nsummary(exam_data)\n\n\n      ID               CLASS              GENDER              RACE          \n Length:322         Length:322         Length:322         Length:322        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n    ENGLISH          MATHS          SCIENCE     \n Min.   :21.00   Min.   : 9.00   Min.   :15.00  \n 1st Qu.:59.00   1st Qu.:58.00   1st Qu.:49.25  \n Median :70.00   Median :74.00   Median :65.00  \n Mean   :67.18   Mean   :69.33   Mean   :61.16  \n 3rd Qu.:78.00   3rd Qu.:85.00   3rd Qu.:74.75  \n Max.   :96.00   Max.   :99.00   Max.   :96.00  \n\n\n\n\n\n\nCode\nexam_data[duplicated(exam_data),]\n\n\n# A tibble: 0 × 7\n# ℹ 7 variables: ID &lt;chr&gt;, CLASS &lt;chr&gt;, GENDER &lt;chr&gt;, RACE &lt;chr&gt;,\n#   ENGLISH &lt;dbl&gt;, MATHS &lt;dbl&gt;, SCIENCE &lt;dbl&gt;\n\n\n\n\n\n\nCode\ncolSums(is.na(exam_data))\n\n\n     ID   CLASS  GENDER    RACE ENGLISH   MATHS SCIENCE \n      0       0       0       0       0       0       0 \n\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\n\nLearnt how to load libraries, import/ read excel files\n\n\n\n\n\n\n1.3 Introducing ggplot\nggplot2 is an R package for creating data-driven graphics based on The Grammar of Graphics. It is also part of the tidyverse family specially designed for visual exploration and communication.\nFor more information, visit ggplot2\n\n1.3.1 R Graphics VS ggplot\nLets compare how R Graphics, the core graphical functions of Base R and ggplot plot a simple histogram.\n\nR Graphicsggplot2\n\n\n\n\nCode\nhist(exam_data$MATHS,col = \"#4169e1\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(data=exam_data, aes(x = MATHS)) +\n  geom_histogram(bins=10, \n                 boundary = 100,\n                 color=\"black\", \n                 fill=\"#4169e1\") +\n  ggtitle(\"Distribution of Maths scores\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\n\nWhile R Graphics offers simpler syntax, Hadley Wickham emphasizes that the true value of ggplot2 lies not in its specific plotting syntax, but in the transformative way it approaches visualization. It enables users to think of visualizations as a method of mapping variables to the visual attributes of geometric objects, creating a powerful framework for understanding and perceiving data.\n\n\n\n\n\n\n1.4 Grammar of Graphics\nThe Grammar of Graphics, introduced by Leland Wilkinson in 1999, provides a structured approach to creating meaningful data visualizations. It breaks graphs into semantic components like scales and layers, offering a framework to answer the question: What is a statistical graphic?\nThe key principles I’ve learned are:\n\nGraphics are built from distinct layers of grammatical elements.\nMeaningful plots are achieved through aesthetic mapping.\n\nThis grammar helps me see how complex graphics are composed, uncover connections between seemingly different visuals, and understand the foundation of diverse visualizations. It also guides me toward recognizing well-formed, meaningful graphics while acknowledging that not all grammatically correct visuals will make sense.\n\n1.4.1 A layered grammar of graphics\nFigure shows the seven grammars of ggplot2: A layered grammar of graphics\n\n\n\nA layered grammar of graphics\n\n\nThe building blocks of the Grammar of Graphics are:\n\nData: The dataset being visualized.\nAesthetics: Attributes like position, color, size, shape, or transparency that map data to visual elements.\nGeometries: Visual elements representing data, such as points, bars, or lines.\nFacets: Subsets of data used to create multiple variations of a graph (e.g., panels).\nStatistics: Transformations summarizing data, such as means or confidence intervals.\nCoordinate Systems: The plane where data is plotted (e.g., Cartesian or polar).\nThemes: Non-data components like titles, axis labels, or legend formatting.\n\n\n\n\n1.5 Essential Grammatical Elements in ggplot2: data\nLet us call the ggplot( ) function using the code chunk below.\n\n\nCode\nggplot(data=exam_data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nA blank canvas is created with ggplot().\nggplot() initializes a ggplot object.\nThe data argument specifies the dataset for plotting.\nIf the dataset is not already a data.frame, it will be converted to one by fortify().\n\n\n\n\n\n1.6 Essential Grammatical Elements in ggplot2: aesthetic mappings\nAesthetic mappings link data attributes to visual characteristics like position, color, size, shape, or transparency, allowing each characteristic to represent information. These mappings are defined using the aes() function, and each geom layer can have its own aes() specification.\nThe code below adds the aesthetic element into the plot.\n\n\nCode\nggplot(data=exam_data,\n       aes(x=MATHS,\n           y=ENGLISH))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nggplot includes the x/y axis and the axis’s label\n\n\n\n\n\n1.7 Essential Grammatical Elements in ggplot2: geom\nGeometric objects are the visual marks on a plot. A layer combines data, aesthetic mappings, a geometric object (geom), statistical transformations (stat), and position adjustments. Layers are typically created using geom_ functions, with options to override default stat or position settings. Every plot requires at least one geom, and additional geoms can be added using the + operator.\nRefer here for a more comprehensive list.\nBelow are some examples of geom_:\n\ngeom_point: for drawing individual points (e.g., a scatter plot)\ngeom_line: for drawing lines (e.g., for a line charts)\ngeom_col: for drawing bars with heights mapped to values in the data (e.g., bar charts).\ngeom_boxplot: for drawing box-and-whisker plots to visualize data distribution and outliers.\ngeom_histogram: for drawing binned values to represent the distribution of a numeric variable (e.g., histograms).\ngeom_density: for drawing smoothed density estimates to visualize the distribution of a numeric variable.\n\n\n\n1.7.1 Geometric objects: geom_bar\nThe code below plots a bar chart by using geom_bar().\n\n\nCode\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n1.7.2 Geometric objects: geom_dotplot\nIn a dot plot, the dot width represents the bin width (or maximum width, depending on the binning method), and dots are stacked, with each dot signifying one observation.\nThe code below plots a dot plot chart by using geom_dotplot().\n\n\nCode\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot(dotsize = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nTo note that the y scale is not very useful, and can be misleading in this case.\n\n\nTo enhance the dotplot visualization, the below (non exhaustive) are some arguments that has been added:\n\nscale_y_continuous()- to turn off the y-axis by setting it to NULL\nbinwidth - specifies the width of the bins or interval used to group the data to - 3.0\ndotsize - adjusts the size of the dots in the plot to - 0.8\nfill - specifies the fill color of the dots to - red\ncolor - specifies the outline color of the dots to - yellow\nalpha - specifies the transparency level of the dots to - 0.5\n\n\n\nCode\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot(binwidth=3.0,         \n               dotsize = 0.8,\n               fill=\"#E15841\",\n               color=\"#E1B941\",\n               alpha=0.5 ) +      \n  scale_y_continuous(NULL,           \n                     breaks = NULL)  \n\n\n\n\n\n\n\n\n\n\n\n1.7.3 Geometric objects: geom_histogram\ngeom_histogram()in ggplot creates a bar plot that displays the distribution of a continuous variable by dividing the data into bins and counting the number of observations in each bin.\nThe code below plots a simple histogram by using values in MATHS field of exam_data.\n\n\nCode\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_histogram()  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe default number of bins is 30, however one can always adjust the number of bins manually by using the bins() argument.\n\n\n\n\n1.7.4 Modifying a geometric object by changing geom()\nTo enhance the histogram visualization, the below (non exhaustive) are some arguments that has been added:\n\nbins - to change the number of bins to - 20\nfill - specifies the fill color of the histogram to - red\ncolor - specifies the outline color of the histogram to - green\n\n\n\nCode\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20,            \n                 color=\"#A8E141\",      \n                 fill=\"#E15841\")\n\n\n\n\n\n\n\n\n\n\n\n1.7.5 Modifying a geometric object by changing aes()\nThe code below changes the interior colour of the histogram (i.e. fill) by using a sub-group of aes().\nIn this case, the fill is based on GENDER.\n\n\nCode\nggplot(data=exam_data, \n       aes(x= MATHS, \n           fill = GENDER)) +\n  geom_histogram(bins=20, \n                 color=\"grey30\")\n\n\n\n\n\n\n\n\n\nWe can also specify the color of the fill based on GENDER.\nIn this case, we used the scale_fill_manual() function to assign specific colors to each gender. E.g.:\n\n\nPink for Female\n\n\nBlue for Male\n\nwhile keeping the outline grey\n\n\n\nCode\nggplot(data = exam_data, \n       aes(x = MATHS, \n           fill = GENDER)) +\n  geom_histogram(bins = 20, \n                 color = \"grey30\") +\n  scale_fill_manual(values = c(\"Female\" = \"pink\", \"Male\" = \"blue\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis approach can be used to color, fill and alpha of the geometric.\n\n\n\n\n1.7.6 Geometric objects: geom_density\nThe geom_density() function computes and visualizes a kernel density estimate, which provides a smooth approximation of the data’s distribution.\nIt serves as a useful alternative to histograms for continuous data, particularly when the data originates from an underlying smooth distribution.\nThe code below plots the distribution of Maths scores in a kernel density plot.\n\n\nCode\nggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_density()\n\n\n\n\n\n\n\n\n\nThe code below plots two kernel density lines by using color or fill arguments of aes()\n\n\nCode\nggplot(data=exam_data, \n       aes(x = MATHS, \n           colour = GENDER)) +\n  geom_density()\n\n\n\n\n\n\n\n\n\nLikewise, by using scale_color_manual() function, we can map the colors of each gender explicitly, where\n\n\nPink for Female\n\n\nBlue for Male\n\n\n\n\nCode\nggplot(data = exam_data, \n       aes(x = MATHS, \n           colour = GENDER)) +\n  geom_density() +\n  scale_color_manual(values = c(\"Female\" = \"pink\", \"Male\" = \"blue\"))\n\n\n\n\n\n\n\n\n\n\n\n1.7.7 Geometric objects: geom_boxplot\ngeom_boxplot() function visualizes continuous data by displaying five key summary statistics: the median, the upper and lower hinges (quartiles), the whiskers, and individual outliers.\n\n\nCode\nggplot(data=exam_data, \n       aes(y = MATHS,       \n           x= GENDER)) +    \n  geom_boxplot()    \n\n\n\n\n\n\n\n\n\nTo enhance the boxplot visualization, the below (non exhaustive) are some arguments that has been added:\n\nfill - fills the boxplot with different colors based on a grouping variable - purple\ncolor - specifies the color of the boxplot borders to - green\noutlier.color - specifies the outlier color of the boxplot to - red\noutlier.shape - specifies the shape of the outlier points to - 16\nalpha - adjust the transparency of the boxplot fill - 0.7\nwidth - adjusts the width of the boxplots to - 0.7\n\n\n\nCode\nggplot(data = exam_data, \n       aes(y = MATHS,       \n           x = GENDER)) +    \n  geom_boxplot(fill = \"#6A5ACD\",      # Fills the boxes with a color\n               color = \"#ABE141\",     # Sets the border color of the boxes\n               outlier.color = \"red\",  # Colors the outliers\n               outlier.shape = 16,     # Sets the shape of the outliers\n               alpha = 0.7,            # Adjusts the transparency of the boxes\n               width = 0.7)            # Sets the width of the boxes\n\n\n\n\n\n\n\n\n\nLikewise, to specify different fill colors for female and male, we need to map the fill aesthetic to the Gender variable inside aes() and then define the specifc colors using scale_fill_manual().\n\n\nPink for Female\n\n\nBlue for Male\n\n\n\n\nCode\nggplot(data = exam_data, \n       aes(y = MATHS,       \n           x = GENDER, \n           fill = GENDER)) +   # Map fill aesthetic to GENDER\n  geom_boxplot(color = \"#ABE141\",      # Sets the border color of the boxes\n               outlier.color = \"red\",  # Colors the outliers\n               outlier.shape = 16,     # Sets the shape of the outliers\n               alpha = 0.7,            # Adjusts the transparency of the boxes\n               width = 0.7) +          # Sets the width of the boxes\n  scale_fill_manual(values = c(\"Female\" = \"pink\", \"Male\" = \"blue\"))  # Custom colors\n\n\n\n\n\n\n\n\n\nNotches in box plots help visually determine if the medians of distributions differ. Non-overlapping notches indicate distinct medians.\nThe code below plots the distribution of Maths scores by gender in notched plot instead of boxplot.\n\n\nCode\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_boxplot(notch=TRUE)\n\n\n\n\n\n\n\n\n\nLikewise, additional arguments can be added to improve the overall visualization of the notched plot\n\nfill - fills the notched plot with different colors based on a grouping variable\ncolor - specifies the color of the notched plot borders to - black\noutlier.color - specifies the outlier color of the boxplot to - red\noutlier.shape - specifies the shape of the outlier points to - 8\nalpha - adjust the transparency of the boxplot fill - 0.9\nnotchwidth - adjusts the width of the notches for improved clarity - 0.5\n\n\n\nCode\nggplot(data = exam_data, \n       aes(y = MATHS, \n           x = GENDER, \n           fill = GENDER)) +  # Add fill to differentiate genders\n  geom_boxplot(notch = TRUE, \n               outlier.colour = \"red\",   # Highlight outliers in red\n               outlier.shape = 8,       # Use a different shape for outliers\n               notchwidth = 0.5,        # Adjust the notch width for emphasis\n               color = \"black\",\n               alpha = 0.9) +      \n  scale_fill_manual(values = c(\"Female\" = \"pink\", \"Male\" = \"blue\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo refer to here for ggplot point shapes best tips and tricks\n\n\n\n\n1.7.8 Geometric objects: geom_violin\ngeom_violin creates violin plots, which are useful for comparing multiple data distributions. Unlike density curves, which can overlap and become hard to interpret, violin plots place distributions side by side, making comparisons clearer and more visually accessible.\nThe code below plots the distribution of Maths score by gender using violin plots.\n\n\nCode\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_violin()\n\n\n\n\n\n\n\n\n\nLikewise, additional arguments can be added to improve the overall visualization of the violin plot\n\nfill - maps the violin fill color to gender\nscale_fill_manual() - assigns custom colors for Female and Male\ntrim - ensures the plot displayes the full range of data, even for smaller distributions\ncolor - adds an outline to make the violins visually distinct\n\n\n\nCode\nggplot(data = exam_data, \n       aes(y = MATHS, \n           x = GENDER, \n           fill = GENDER)) +  # Map fill to GENDER\n  geom_violin(trim = FALSE,   # Show the full range of the data\n              scale = \"width\", # Adjust width for comparability\n              color = \"black\") +  # Add a black outline to the violins\n  scale_fill_manual(values = c(\"Female\" = \"pink\", \n                               \"Male\" = \"blue\"))\n\n\n\n\n\n\n\n\n\n\n\n1.7.9 Geometric objects: geom_point\ngeom_point() is useful for creating scatterplot.\nThe code below plots a scatterplot showing the Maths and English grades of pupils by using geom_point()\n\n\nCode\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() \n\n\n\n\n\n\n\n\n\nLikewise, additional arguments can be added to improve the overall visualization of the scatter plot\n\ncolor - set all points to a fixed blue color\nsize - set the size of the points to a fixed value (adjustable) to - 3\nalpha - adds transparency to reduce overlap if points are cluttered\nshape - set the apperance of the points in the scatter plot (e.g.: square, circle, triangle)\n\n\n\nCode\nggplot(data = exam_data, \n       aes(x = MATHS, \n           y = ENGLISH)) +\n  geom_point(color = \"blue\", size = 3, alpha = 0.5, shape = 20)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo refer to here for ggplot point shapes best tips and tricks\n\n\n\n\n1.7.10 Combining geometric objects\nThe code below plots the data points on the boxplots by using both geom_boxplot() and geom_point().\n\n\nCode\nggplot(data=exam_data, \n       aes(y = MATHS, \n           x= GENDER)) +\n  geom_boxplot() +                    \n  geom_point(position=\"jitter\", \n             size = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\n\nLearnt about the different geometric objects such as geom_bar,geom_dotplot,geom_histogram,geom_density, geom_boxplot,geom_violin,geom_point etc.\nLearnt about how to combine different geometric objects together in one chart - e.g.: geom_boxplot and geom_point\n\n\n\n\n\n\n1.8 Essential grammatical elements in ggplot2: stat\nThe Statistics functions statistically transforms data, as a form of summary.\nExample include: - calculating the frequency of variable values - calculating the mean - determing confidence limits\nThere are two ways to use these functions: - add a stat_() function and override the default geom, or - add a geom_() function and override the default stat.\n\n1.8.1 Working with stat()\nThe boxplots below are incomplete because the means are not shown.\n\n\nCode\nggplot(data=exam_data, \n       aes(y = MATHS, x= GENDER)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\n1.8.2 Working with stat_summary()\nThe code below uses the stat_summary() function to add mean values by replacing the default geometric layer (geom).\n\n\nCode\nggplot(data=exam_data, \n       aes(y = MATHS, x= GENDER)) +\n  geom_boxplot() +\n  stat_summary(geom = \"point\",       \n               fun = \"mean\",         \n               colour =\"blue\",        \n               size=4) \n\n\n\n\n\n\n\n\n\n\n\n1.8.3 Working with geom() method\nThe code below uses the geom() function to add mean values by replacing the default geometric layer (geom).\n\n\nCode\nggplot(data=exam_data, \n       aes(y = MATHS, x= GENDER)) +\n  geom_boxplot() +\n  geom_point(stat=\"summary\",        \n             fun=\"mean\",           \n             colour=\"blue\",          \n             size=4)          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\n\nBoth codes achieve the same result - boxplot with mean values as blue points overlaid on the plot\nThe first code uses stat_summary explicitly where summary is calculated fun = \"mean\", and then visualized using the geometry geom=\"point\". It provides more flexibility if one want to switch the geometry.\nThe second code uses geom_point with stat=\"summary\", where it computes the mean and plots it directly.\nOverall, I would prefer the second code as it feel more intuitive and focuses on using a geometry geom_point and overridding its default statistical transformation.\n\n\n\n\n\n1.8.4 Adding a best fit curve on a scatterplot?\nThe scatterplot below illustrates the relationship between Maths and English grades.\nAdding a best-fit curve can enhance its interpretability.\n\n\nCode\necho=FALSE\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() \n\n\n\n\n\n\n\n\n\ngeom_smooth() is used to plot a best fit curve on the scatterplot.\n\n\nCode\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(size=0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe default method used is LOESS - Locally Estimted Scatterplot Smoothing which refers to a regression method used for smoothing data points in a scatterplot, useful for capturing non-linear trends in the data.\n\n\nThe default method can be overriden as shown below:\n\n\nCode\nggplot(data=exam_data, \n       aes(x= MATHS, \n           y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              linewidth=0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn this case, method = \"lm\" specifies that a linear regression line should be fitted to the data.\nRefer here for more information on the different methods available in geom_smooth().\n\n\nLikewise, additional arguments can be added to improve the overall visualization of the scatter plot\n\ncolor - sets the color of the regression line\nsize - adjusts the thickness of the line\n\n\n\nCode\nggplot(data = exam_data, \n       aes(x = MATHS, y = ENGLISH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", color = \"red\", size = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\n\nLearnt the two ways to add statistical functions to ggplot2 - stat_() and geom_()\nLearnt how to incorporate a best fit curve on to the scatterplot geom_smooth()\nLearnt the different regression methods available ingeom_smooth()\n\n\n\n\n\n\n1.9 Essential grammatical elements in ggplot2: Facets\nFacetting in ggplot2 creates small multiples, or trellis plots, to display different subsets of data. This approach is an alternative to using aesthetics (such as color or shape) for displaying additional variables. There are two main types of faceting in ggplot2:\n\nfacet_wrap() : wraps multiple plots into a single panel, typically based on a single categorical variable. It will automatically arranges the plots into a grid.\nfacet_grid() : organizes plots into a grid based on the values of two categorical variables, one for rows and the other for columns.\n\n\n1.9.1 Working with facet_wrap()\nfacet_wrap arranges a 1D sequence of panels into a 2D layout, making better use of screen space, especially on rectangular displays. It is often more efficient than facet_grid() for displaying a variable across multiple categories\nThe code below produces a 2D matrix of ‘MATHS’ histograms grouped by variable ‘CLASS’ using facet_wrap(~CLASS).\n\n\nCode\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20) +\n    facet_wrap(~ CLASS)\n\n\n\n\n\n\n\n\n\n\n\n1.9.2 Working with facet_grid()\nfacet_grid creates a matrix of panels based on two discrete variables, one for rows and the other for columns. It is most useful when both variables have all possible combinations in the data.\nThe code below plots a trellis plot using facet_grid().\n\n\nCode\nggplot(data=exam_data, \n       aes(x= MATHS)) +\n  geom_histogram(bins=20) +\n    facet_grid(~ CLASS)\n\n\n\n\n\n\n\n\n\n\n\n\n1.10 Essential grammatical elements in ggplot2: Coordinates\nThe Coordinates functions map the position of objects onto the plane of the plot.\nThere are a number of different possible coordinate systems to use:\n\ncoord_cartesian(): the default cartesian coordinate systems, where you specify x and y values (e.g. allows you to zoom in or out).\ncoord_flip(): a cartesian system with the x and y flipped.\ncoord_fixed(): a cartesian system with a “fixed” aspect ratio (e.g. 1.78 for a “widescreen” plot).\ncoord_quickmap(): a coordinate system that approximates a good aspect ratio for maps.\n\n\n1.10.1 Working with Coordinate\nBy default the bar charts in ggplot2 are vertical, as shown:\n\n\n\n\n\n\n\n\n\nUsing coord_flip(), we can flip the vertical bar chart to a horizontal one.\n\n\nCode\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip()\n\n\n\n\n\n\n\n\n\n\n\n1.10.2 Changing the x and y axis range\nThe scatterplot below is misleading as the x and y axes are not equal. This can create confusion to the readers.\n\n\nCode\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, size=0.5)\n\n\n\n\n\n\n\n\n\nThe code below will set both the x and y axis range to 0-100.\n\n\nCode\nggplot(data=exam_data, \n       aes(x= MATHS, y=ENGLISH)) +\n  geom_point() +\n  geom_smooth(method=lm, \n              size=0.5) +  \n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\n\n\n\n\n\n\n\n\n\n\n\n1.11 Essential grammatical elements in ggplot2: Themes\nThemes control elements of the graph which are not related to the data such as\n\nbackground color\nfonts size\ngridlines\nlabels color\n\nSome of the Built-in themes include:\n\ntheme_gray(): default theme, with a light gray background and white gridlines.\ntheme_bw(): theme with a black-and-white color scheme - white background, black gridlines and axis lines\ntheme_classic(): theme with a minimalist look - white background and no gridlines\n\nThe list of all other themes available can be found at this link\n\n1.11.1 Working with Themes\nThe code below plot a horizontal bar chart using theme_gray()\n\n\nCode\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_gray()\n\n\n\n\n\n\n\n\n\nThe code below plot a horizontal bar chart using theme_bw()\n\n\nCode\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThe code below plot a horizontal bar chart using theme_classic()\n\n\nCode\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar() +\n  coord_flip() +\n  theme_classic()\n\n\n\n\n\n\n\n\n\nWe can further customize the chart by additional arguments using theme().\n\n\nCode\nggplot(data=exam_data, \n       aes(x=RACE)) +\n  geom_bar(fill=\"skyblue\", color=\"black\", size=0.7) +  \n  coord_flip() +\n  theme_minimal() +\n   theme(panel.background = element_rect(fill = \"lightgrey\",\n                                         color = \"white\"))\n\n\n\n\n\n\n\n\n\n\n\n\n1.12 References\n\nKam, T.S. (2023).A Layered Grammar of Graphics: ggplot2 methods.\nHadley Wickham (2023) ggplot2: Elegant Graphics for Data Analysis.Online 3rd edition.\nWinston Chang (2013) R Graphics Cookbook 2nd edition. Online version.\nHealy, Kieran (2019) Data Visualization: A practical introduction. Online version\nLearning ggplot2 on Paper – Components\nLearning ggplot2 on Paper – Layer\nLearning ggplot2 on Paper – Scale\n\n\n1.12.1 Additional references\n\nQuarto_overall_guide\nQuarto_markdown_basic\nQuarto_interactive_layout\n\n\n\n\n1.13 Takeaway\n\n\n\n\n\n\nTakeaway\n\n\n\n\nUnderstand the foundational principles of ggplot2 for creating graphics.\nLearn to set up the environment by installing necessary R packages. E.g.: library(ggplot2)\nUnderstand the concept of aesthetic mappings and geometric objects.\nExplore how to layer different elements like stats, facets, and coordinates.\nLearnt the importance of various themes in customizing the appearance of plots.\nApply practical examples to reinforce the concepts and improve visual analytics skills.\necho=FALSE: Hides the code while executing it to display the plot in the final output\n\n\n\n\n1.13.1 Further exploration\n1. Is it possible to add mean, median and a title on histogram?\nBefore:\n\n\nCode\nggplot(data=exam_data, aes(x = MATHS)) +\n  geom_histogram(bins=10, \n                 boundary = 100,\n                 color=\"black\", \n                 fill=\"#4169e1\") +\n  ggtitle(\"Distribution of Maths scores\")\n\n\n\n\n\n\n\n\n\nAfter:\n\ngeom_vline(): adds vertical lines for mean and median\nlabs(): used to set the title and add a label for the legend\nscale_color_manual(): set the colors for mean and median\n\n\n\nCode\n# Calculate mean and median\nmean_value &lt;- mean(exam_data$MATHS, na.rm = TRUE)\nmedian_value &lt;- median(exam_data$MATHS, na.rm = TRUE)\n\n# Create a data frame for the mean and median lines\nline_data &lt;- data.frame(\n  value = c(mean_value, median_value),\n  type = factor(c(\"Mean\", \"Median\"))\n)\n\n# Create the histogram with mean and median lines and a legend\nggplot(data = exam_data, aes(x = MATHS)) +\n  geom_histogram(bins = 10, \n                 boundary = 100, \n                 color = \"black\", \n                 fill = \"#4169e1\") +\n  geom_vline(data = line_data, aes(xintercept = value, color = type), \n             linetype = \"dashed\", \n             size = 1) +\n  scale_color_manual(values = c(\"Mean\" = \"red\", \"Median\" = \"green\")) +\n  labs(title = \"Distribution of Maths Scores\",\n       x = \"Maths Scores\",\n       y = \"Frequency\",\n       color = \"Statistics\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n2. Exploring other useful combinations of geometric objects in ggplot2?\n\nBox plot with jittered points (geom_boxplot()+ geom_jitter())\nUse case: To see the spread of ENGLISH scores across Gender, and identify any outliers.\n\n\n\nCode\nggplot(data=exam_data, aes(x=GENDER, y=ENGLISH)) +\n  geom_boxplot(fill=\"lightblue\") +   # Box plot for distribution\n  geom_jitter(width=0.1, size=1, alpha=0.7)  # Jittered points for individual data\n\n\n\n\n\n\n\n\n\nConclusion:\n\nFemale students tend to have a slightly higher median English score compared to male students.\nMale students exhibit greater variability in their English scores, with a wider range of scores observed compared to female students.\n\n\n\n\n\nlibrary(ggplot2)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03a.html",
    "href": "Hands-on_Ex/Hands-on_Ex03a.html",
    "title": "Hands-on Exercise 03a",
    "section": "",
    "text": "With the assistance of ChatGPT"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex03a.html#programming-interactive-data-visualisation-with-r",
    "href": "Hands-on_Ex/Hands-on_Ex03a.html#programming-interactive-data-visualisation-with-r",
    "title": "Hands-on Exercise 03a",
    "section": "3. Programming Interactive Data Visualisation with R",
    "text": "3. Programming Interactive Data Visualisation with R\n\n3.1 Learning outcome\nIn this exercise, we will be exploring how to create interactive data visualizations using the ggiraph and plotlyr packages.\n\n\n3.2 Getting started\nWe will check, install, and load the following R packages, if not already done so:\n\ngiraph: Adds interactivity to ggplot graphics.\nplotly: Enables interactive statistical plotting.\nDT: Provides an interface to the JavaScript library DataTables for interactive HTML tables.\ntidyverse: A collection of R packages for data science, including static graph creation.\npatchwork:Combines multiple ggplot2 graphs into a single figure.\n\n\n\nCode\npacman::p_load(ggiraph, plotly, \n               patchwork, DT, tidyverse) \n\n\n\n\n3.3 Importing data\nIn this section, we will use the read_csv() function from the readr package to import the Exam_data.csv file into R. The file will be loaded as a tibble data frame and saved as exam_data.\n\n\nCode\nexam_data &lt;- read_csv(\"data/Exam_data.csv\", show_col_types = FALSE)\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThis section data is the same dataset used in Hands-on_Ex01 and Hands-on_Ex02\n\n\n\n\n\n3.4 Interactive data visualisation - ggiraph\nggiraph is an HTML widget and an extension of ggplot2, enabling interactive ggplot graphics. Interactivity is achieved using three key arguments:\n\nTooltip: Displays information when hovering over elements.\nOnclick: Executes a JavaScript function when elements are clicked.\nData_id: Associates elements with unique IDs for further interaction.\n\nIn Shiny applications, elements with data_id can be selected and manipulated on both the client and server sides. For more details, refer here.\n\n3.4.1 Tooltip effect with tooltip aesthetic\nThe code shows how to create an interactive statistical graph using the ggiraph package.\nIt consists of two steps:\n\nFirst, a ggplot object is created, and\nSecond, the girafe() function of ggiraph is used to convert it into an interactive SVG object.\n\n\n\nCode\np &lt;- ggplot(data=exam_data, \n       aes(x = ENGLISH)) +\n  geom_dotplot_interactive(\n    aes(tooltip = ID),\n    stackgroups = TRUE, \n    binwidth = 1, \n    method = \"histodot\") +\n  scale_y_continuous(NULL, \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 6,\n  height_svg = 6*0.618\n)\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nUnderstand the 2 steps\n\n1. An interactive version of ggplot2 geom (i.e. geom_dotplot_interactive()) will be used to create the basic graph.\n2. girafe() will be used to generate an svg object to be displayed on an html page.\n\n\n\n\n\n\n\n\n\nExtra\n\n\n\n\nAn SVG object is a scalable, resolution-independent graphic format used in ggiraph to render interactive plots with tooltips and click events in web-based applications.\n\n\n\n\n\n\n3.5 Interactivity\nNotice that by hovering the mouse pointer on the chart, the student’s ID will be displayed.\n\n\n\n\n\n\n\n3.5.1 Displaying multiple information on tooltip\nThe tooltip information can be customizable by including a list object as shown below.\nThe code below shows that the tooltip will reflect the Name and Class information from the ID and Class table respectively from the exam_data table.\n\n\nCode\nexam_data$tooltip &lt;- c(paste0(     \n  \"Name = \", exam_data$ID,         \n  \"\\n Class = \", exam_data$CLASS)) \n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(\n    aes(tooltip = exam_data$tooltip), \n    stackgroups = TRUE,\n    binwidth = 1,\n    method = \"histodot\") +\n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 8,\n  height_svg = 8*0.618\n)\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe first three lines of code in the chunk create a new field named tooltip by combining the text from the ID and CLASS fields. This newly created tooltip field is then used to display tooltips in the plot.\n\n\n\n\n\n3.6 Interactivity with customizable tooltip\nNotice that by hovering the mouse pointer on the chart, the student’s ID and the classs details will be displayed.\n\n\n\n\n\n\n\n3.6.1 Customising tooltip style\nThe code below uses opts_tooltip() of ggiraph to customize the tooltip by adding css declarations.\n\nStyle 1Style 2Style 3\n\n\n\nTooltip aesthetic:Bold text with white background\nX axis: MATHS Score\nTooltip details: ID\n\n\n\nCode\ntooltip_css &lt;- \"background-color:white; #&lt;&lt;\nfont-style:bold; color:black;\" #&lt;&lt;\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = ID),                   \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(    #&lt;&lt;\n    opts_tooltip(    #&lt;&lt;\n      css = tooltip_css)) #&lt;&lt;\n)                                        \n\n\n\n\n\n\n\n\n\nTooltip aesthetic: Bold text with yellow background\nX axis: MATHS Score\nTooltip details: ID\n\n\n\nCode\ntooltip_css &lt;- \"background-color: yellow; \n                color: black; \n                font-weight: bold;\"\n\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = ID),                   \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(    #&lt;&lt;\n    opts_tooltip(    #&lt;&lt;\n      css = tooltip_css)) #&lt;&lt;\n)             \n\n\n\n\n\n\n\n\n\nTooltip aesthetic: Bold text with blue background\nX axis: ENGLISH Score\nTooltip details: Added “ID”, “CLASS”, “GENDER”, and “RACE” in tooltip\n\n\n\nCode\ntooltip_css &lt;- \"background-color: blue; \n                color: white; \n                font-weight: bold;\"\n\nexam_data$tooltip &lt;- c(paste0(     \n  \"ID = \", exam_data$ID,         \n  \"\\n Class = \", exam_data$CLASS,\n  \"\\n Gender = \", exam_data$GENDER,\n  \"\\n RACE = \", exam_data$RACE,\n  \"\\n English = \", exam_data$ENGLISH)) \n\np &lt;- ggplot(data=exam_data, \n       aes(x = ENGLISH)) +\n  geom_dotplot_interactive(\n    aes(tooltip = tooltip), \n    stackgroups = TRUE,\n    binwidth = 1,\n    method = \"histodot\") +\n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(\n  ggobj = p,\n  width_svg = 8,\n  height_svg = 8*0.618,\n  options = list(\n    opts_tooltip(css = tooltip_css) # Pass the custom CSS for the tooltip\n  )\n)\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe paste0() function ensures that the column has no missing or has NA value, if not R raises the “argument is missing, with no default” error.\nAs we are using the same dataset as in Hands-on_Ex01 and Hands-on_Ex02, we have ensured that there are no NA values.\n\n\n\n\n\nRefer to “Customizing girafe objects” for instructions on customizing ggiraph visualizations.\n\n\n3.6.2 Displaying statistics on tooltip\nThe code chunk below shows an advanced method for customizing tooltips. It uses a function to calculate the 90% confidence interval of the mean, and the computed statistics are displayed in the tooltip.\n\nMaths score across racesMaths score across CLASSMaths score across GENDER\n\n\n\n\nCode\ntooltip &lt;- function(y, ymax, accuracy = .01) {\n  mean &lt;- scales::number(y, accuracy = accuracy)\n  sem &lt;- scales::number(ymax - y, accuracy = accuracy)\n  paste(\"Mean maths scores:\", mean, \"+/-\", sem)\n}\n\ngg_point &lt;- ggplot(data=exam_data, \n                   aes(x = RACE),\n) +\n  stat_summary(aes(y = MATHS, \n                   tooltip = after_stat(  \n                     tooltip(y, ymax))),  \n    fun.data = \"mean_se\", \n    geom = GeomInteractiveCol,  \n    fill = \"pink\"\n  ) +\n  stat_summary(aes(y = MATHS),\n    fun.data = mean_se,\n    geom = \"errorbar\", width = 0.2, linewidth = 0.2\n  )\n\ngirafe(ggobj = gg_point,\n       width_svg = 8,\n       height_svg = 8*0.618)\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nReplaced size=0.5 with linewidth-0.5 in the geom_smooth() function to align the code with the latest ggplot2 standards.\n\n\n\n\n\n\n\n\n\nConclusion\n\n\n\n\nThe Chinese group has the highest average Maths score compared to other groups.\nThe Malay group has the lowest average Maths score among the groups.\nNoticeable performance gap between the Chinese group (highest) and the Malay group (lowest), suggesting potential differences in educational performance across these groups.\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(ggiraph)\nlibrary(scales)\n\ntooltip &lt;- function(y, ymax, accuracy = .01) {\n  mean &lt;- number(y, accuracy = accuracy)\n  sem &lt;- number(ymax - y, accuracy = accuracy)\n  paste(\"Mean maths scores:\", mean, \"+/-\", sem)\n}\n\ngg_point &lt;- ggplot(data = exam_data, \n                   aes(x = CLASS)) +  \n  stat_summary(aes(y = MATHS, \n                   tooltip = after_stat(\n                     tooltip(y, ymax))),  \n               fun.data = \"mean_se\", \n               geom = GeomInteractiveCol,  \n               fill = \"lightblue\") +  \n  stat_summary(aes(y = MATHS),\n               fun.data = mean_se,\n               geom = \"errorbar\", \n               width = 0.2, \n               linewidth = 0.2)\n\ngirafe(ggobj = gg_point,\n       width_svg = 8,\n       height_svg = 8 * 0.618)\n\n\n\n\n\n\n\n\n\n\n\n\nConclusion\n\n\n\n\nHighest Performance: Class 3A has the highest average Maths score among all classes.\nNoticeable decline in average Maths scores as you move from Class 3A to Class 3I.\nClass 3I has the lowest average Maths score, with a significant gap compared to Class 3A.\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(ggiraph)\nlibrary(scales)\n\ntooltip &lt;- function(y, ymax, accuracy = .01) {\n  mean &lt;- number(y, accuracy = accuracy)\n  sem &lt;- number(ymax - y, accuracy = accuracy)\n  paste(\"Mean maths scores:\", mean, \"+/-\", sem)\n}\n\ngg_point &lt;- ggplot(data = exam_data, \n                   aes(x = GENDER)) +  \n  stat_summary(aes(y = MATHS, \n                   tooltip = after_stat(\n                     tooltip(y, ymax))),  \n               fun.data = \"mean_se\", \n               geom = GeomInteractiveCol,  \n               fill = \"lightgreen\") +  \n  stat_summary(aes(y = MATHS),\n               fun.data = mean_se,\n               geom = \"errorbar\", \n               width = 0.2, \n               linewidth = 0.2)\n\ngirafe(ggobj = gg_point,\n       width_svg = 8,\n       height_svg = 8 * 0.618)\n\n\n\n\n\n\n\n\n\n\n\n\nConclusion\n\n\n\n\nSimilar Performance: Both Female and Male groups have very similar average Maths scores, with minimal difference.\nNo Significant Gap: There is no significant performance gap between females and males in Maths scores.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe same analysis can be applied to the “ENGLISH” and “SCIENCE” scores as well.\n\n\n\n\n\n\n\n\n3.6.3 Hover effect with data_id aesthetic\nThe code below shows another interactive feature of ggiraph, data_id.\n\n\nCode\np4 &lt;- ggplot(data=exam_data, \n       aes(x = SCIENCE)) +\n  geom_dotplot_interactive(           \n    aes(data_id = CLASS),             \n    stackgroups = TRUE,               \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p4,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618                      \n)                                        \n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe default value of the hover css is *hover_css = “fill:orange;”.\n\n\n\n\nAdded tooltip with more information: CLASS; SCIENCE score\nChanged histodots to lightgrey\nUsing opts_hover(css=\"fill:green;\"), changed the fill color of the dots to green when hovered over\n\n\n\n\n\n\n\nInteractivity: Elements associated with a data_id (i.e CLASS) will be highlighted upon mouse over.\n\n\n3.6.4 Styling hover effect\nAnother highlighting effect: - Using opts_hover(css = \"fill: #202020;\"), changed the fill color of the dots to black when hovered over - Using opts_hover_inv(css = \"opacity:0.2;\") create the opacity of the dots\n\n\nCode\np &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #202020;\"),  \n    opts_hover_inv(css = \"opacity:0.2;\") \n  )                                        \n) \n\n\n\n\n\n\nInteractivity: Elements associated with a data_id (i.e CLASS) will be highlighted upon mouse over.\n\n\n\n\n\n\nNote\n\n\n\n\nDifferent from previous example, in this example the ccs customisation request are encoded directly.\n\n\n\n\n\n3.6.5 Combining tooltip and hover effect\nBelow code will combine tooltip and hover effect on the interactive statistical graph:\n\n\nCode\n#| echo: true\n#| eval: false\n\np5 &lt;- ggplot(data=exam_data, \n       aes(x = SCIENCE)) +\n  geom_dotplot_interactive(              \n    aes(tooltip = CLASS, \n        data_id = CLASS),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\ngirafe(                                  \n  ggobj = p5,                             \n  width_svg = 6,                         \n  height_svg = 6*0.618,\n  options = list(                        \n    opts_hover(css = \"fill: #E15841;\"),  \n    opts_hover_inv(css = \"opacity:0.1;\") \n  )                                      \n)\n\n\n\n\n\n\nInteractivity: Elements associated with a data_id (i.e CLASS) will be highlighted upon mouse over. At the same time, the tooltip will show the CLASS.\n\n\n3.6.6 Click effect with onclick\nonclick argument of ggiraph provides hotlink interactivity on the web.\nThe code below shows an example of onclick.\nI have also incorprated both onclick and hover functionality together in an interactive plot using the ggiraph package.\n\n\nCode\nexam_data$onclick &lt;- sprintf(\n  \"window.open(\\\"%s%s\\\")\",\n  \"https://www.moe.gov.sg/schoolfinder?journey=Primary%20school\",\n  as.character(exam_data$ID)\n)\n\np &lt;- ggplot(data = exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(\n      onclick = onclick,                 # Onclick functionality\n      tooltip = paste(\"ID:\", ID)         # Tooltip for hover functionality\n    ),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\",\n    fill = \"lightgreen\"                  # Default dot color\n  ) +               \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\n# Generate the interactive plot with hover and onclick\ngirafe(\n  ggobj = p,                             \n  width_svg = 6,                         \n  height_svg = 6 * 0.618,\n  options = list(\n    opts_hover(css = \"fill: blue;\"),     # Change fill color on hover\n    opts_hover_inv(css = \"opacity:0.1;\") # Dim non-hovered elements\n  )\n)                                  \n\n\n\n\n\n\nInteractivity: Web document link with a data object will be displayed on the web browser upon mouse click.\nIn this case, upon clicking on the chart, it will bring user to the MOE SchoolFinder webpage.\n\n\n\n\n\n\nWarning\n\n\n\n\nNote that click actions must be a string column in the data set containing valid javascript instructions\n\n\n\n\n\n3.6.7 Coordinated multiple views with ggiraph\nThe data visualization below shows coordinated multiple views techniques.\nThe example demonstrates coordinated multiple views, where selecting a data point in one dotplot highlights the corresponding data point ID in the second visualization.\nTo implement this, the following strategy will be used: - Interactive functions from the ggiraph package will enable the creation of interative multiple views. - The patchwork function will be integrated within the girafe function to build the interactive coordinated multiple views.\n\n\nCode\np_1 &lt;- ggplot(data=exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +  \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\np_2 &lt;- ggplot(data=exam_data, \n       aes(x = SCIENCE)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID),              \n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") + \n  coord_cartesian(xlim=c(0,100)) + \n  scale_y_continuous(NULL,               \n                     breaks = NULL)\n\ngirafe(code = print(p_1 + p_2), \n       width_svg = 6,\n       height_svg = 3,\n       options = list(\n         opts_hover(css = \"fill: #800080;\"),\n         opts_hover_inv(css = \"opacity:0.2;\")\n         )\n       ) \n\n\n\n\n\n\nIn this case, the date_id is critical to link observations between plots and the tooltip aesthetic is optional but nice to have when we mouse over a point.\nAdded tooltip displaying the scores of MATHS and SCIENCE for each ID:\n\n\nCode\n# Plot 1: Maths Dotplot\np_1 &lt;- ggplot(data = exam_data, \n       aes(x = MATHS)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID, \n        tooltip = paste(\"ID:\", ID, \"&lt;br&gt;Maths:\", MATHS)),  # Add tooltip for Maths\n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") +  \n  coord_cartesian(xlim = c(0, 100)) + \n  scale_y_continuous(NULL, breaks = NULL)\n\n# Plot 2: Science Dotplot\np_2 &lt;- ggplot(data = exam_data, \n       aes(x = SCIENCE)) +\n  geom_dotplot_interactive(              \n    aes(data_id = ID, \n        tooltip = paste(\"ID:\", ID, \"&lt;br&gt;Science:\", SCIENCE)),  # Add tooltip for Science\n    stackgroups = TRUE,                  \n    binwidth = 1,                        \n    method = \"histodot\") + \n  coord_cartesian(xlim = c(0, 100)) + \n  scale_y_continuous(NULL, breaks = NULL)\n\n# Combine plots and add interactivity\ngirafe(code = print(p_1 + p_2), \n       width_svg = 6,\n       height_svg = 3,\n       options = list(\n         opts_hover(css = \"fill: #800080;\"), \n         opts_hover_inv(css = \"opacity:0.2;\")\n       )\n)\n\n\n\n\n\n\n\n\n\n3.7 Interactive data visualization - plotly method\nPlotly’s R graphing library enables the creation of interactive web graphics using ggplot2 or a custom interface inspired by the grammar of graphics. Unlike other Plotly platforms, plotly for R is free and open-source, built on the MIT-licensed plotly.js JavaScript library.\nThere are two ways to create interactive graph by using plotly:\n\nplot_ly():A function in Plotly’s R library for building interactive visualizations from scratch.\nggplotly():A function that converts static ggplot2 visualizations into interactive Plotly graphics.\n\n\n3.7.1 Creating an interactive scatter plot: plot_ly() method\n\nPlot()Code()\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot_ly(data = exam_data, \n             x = ~MATHS, \n             y = ~ENGLISH)\n\n\n\n\n\n\n\n3.7.2 Working with visual variable: plot_ly() method\nThe color argument is added to a qualitative visual variable (e.g.: RACE)\n\nPlot()Code()\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot_ly(data = exam_data, \n        x = ~ENGLISH, \n        y = ~MATHS, \n        color = ~RACE)\n\n\n\n\n\n\n\n3.7.3 Creating an interactive scatter plot: ggplotly() method\nThe code plots an interactive scatter plot by using ggplotly().\n\nPlot()Code()\n\n\n\n\n\n\n\n\n\n\n\n\nCode\np &lt;- ggplot(data=exam_data, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nggplotly(p)\n\n\nThe only additional line required in the code chunk is ggplotly().\n\n\n\n\n\n3.7.4 Coordinated multiple views with plotly\nCreating a coordinated linked plot with plotly involves three key steps:\n\nShare Data: Use highlight_key]() from the Plotly package to create shared data.\nCreate Scatterplots: Generate two scatterplots using the ggplot2 functions.\nCombine Plots: Use subplot() from the Plotly package to arrange the plots side by side.\n\n\nPlot()Code()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nClick on a data point of one of the scatterplot and see how the corresponding point on the other scatterplot is selected.\n\n\n\n\n\n\n\nCode\nd &lt;- highlight_key(exam_data)\np1 &lt;- ggplot(data=d, \n            aes(x = MATHS,\n                y = ENGLISH)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\np2 &lt;- ggplot(data=d, \n            aes(x = MATHS,\n                y = SCIENCE)) +\n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\nsubplot(ggplotly(p1),\n        ggplotly(p2))\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nhighlight_key() creates an object of class crosstalk::SharedData\nVisit this link to learn more about crosstalk.\ncrosstalk::SharedData is a function provided by the Crosstalk package in R, which enables shared interactivity between multiple visualizations or widgets.\n\n\n\n\n\n\n\n\n\n3.8 Interactive data visualization - crosstalk methods!\nCrosstalk is an extension for the htmlwidgets package that provides classes, functions, and conventions to enable cross-widget interactions, such as linked brushing and filtering.\n\n3.8.1 Interactive data table: DT package\n\nDataTables is a wrapper for the JavaScript library DataTables, allowing R data objects to be displayed as interactive HTML tables.\nIt is commonly used in R Markdown or Shiny applications.\n\n\n\nCode\nDT::datatable(exam_data, class= \"compact\")\n\n\n\n\n\n\n\n\n3.8.2 Linked brushing: crosstalk method\n\nPlot()Code()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode below is used to implement the coordinated brushing shown:\n\n\nCode\nd &lt;- highlight_key(exam_data) \np &lt;- ggplot(d, \n            aes(ENGLISH, \n                MATHS)) + \n  geom_point(size=1) +\n  coord_cartesian(xlim=c(0,100),\n                  ylim=c(0,100))\n\ngg &lt;- highlight(ggplotly(p),        \n                \"plotly_selected\")  \n\ncrosstalk::bscols(gg,               \n                  DT::datatable(d), \n                  widths = 5)        \n\n\n\n\n\n\n\n\nNote\n\n\n\nThings to learn: - [*highlight()]*(https://www.rdocumentation.org/packages/plotly/versions/4.10.4/topics/highlight) is a function of plotly package. It sets a variety of options for brushing (i.e., highlighting) multiple plots. These options are primarily designed for linking multiple plotly graphs, and may not behave as expected when linking plotly to another htmlwidget package via crosstalk. In some cases, other htmlwidgets will respect these options, such as persistent selection in leaflet.\n\nbscols() is a helper function of crosstalk package. It makes it easy to put HTML elements side by side. It can be called directly from the console but is especially designed to work in an R Markdown document. Warning: This will bring in all of Bootstrap!\n\n\n\n\n\n\n\n\n\n3.9 Reference\n\n3.9.1 ggiraph\nRefer here for the online version and useful articles of ggiraph. This link to download the pdf version of the reference guide.\n\nHow to Plot With Ggiraph\nInteractive map of France with ggiraph\nCustom interactive sunbursts with ggplot in R\nLink provides code example on how ggiraph is used to interactive graphs for Swiss Olympians - the solo specialists.\n\n\n\n3.9.2 plotly for R\n\nGetting Started with Plotly in R\nCollection of plotly R graphs are available via this link\nCarson Sievert (2020) Interactive web-based data visualization with R, plotly, and shiny, Chapman and Hall/CRC is the best resource to learn plotly for R. The online version is available via this link\nPlotly R Figure Reference provides a comprehensive discussion of each visual representations.\nPlotly R Library Fundamentals is a good place to learn the fundamental features of Plotly’s R API.\nGetting Started\nVisit this link for a very interesting implementation of gganimate by your senior.\nBuilding an animation step-by-step with gganimate.\nCreating a composite gif with multiple gganimate panels\n\n\n\n3.9.3 Overall reference\n\nKam, T.S. (2023).3 Programming Interactive Data Visualisation with R\n\n\n\n\n4.0 Takeaway\n\n\n\n\n\n\nKey takeaways\n\n\n\n\nKey packages used - ggiraph, plotly, DT,tidyverse,patchwork\nLearnt about interactive visualizations with ggiraph such as - Tooltip,Hover effects,Onclick actions, and coordinated views\nLearnt about interactive visualizations with plotly such as - using highlight_key()for shared data and subplot() to display coordinated views\nLearnt about crosstalk for cross-widget interaction such as - linked brushing and filtering\n\n\n\n\n\n5.0 Further exploration\n\nExploration of different types of interactive plots, apart from geom_dotplot_interative():\n\n\ngeom_bar_interactivegeom_density_interactive\n\n\nObservations:\n\nScore distribution:shows a right-skewed distribution, with a higher concentration of students scoring between 60 and 90 in MATHS.\nClass variability: With the tooltip tied to the CLASS variable, while hovering over the bars in the interactive chart, we can see that CLASS 3A dominate a high score ranges (i.e.: they excel in MATHS). Whereas, CLASS 3I students struggle in MATHS.\n\n\n\nCode\np &lt;- ggplot(data = exam_data, \n            aes(x = MATHS)) +\n  geom_bar_interactive(\n    aes(tooltip = CLASS, \n        data_id = CLASS), \n    width = 0.8\n  ) +\n  scale_y_continuous(NULL, breaks = NULL)\n\ngirafe(\n  ggobj = p,\n  width_svg = 6,\n  height_svg = 6 * 0.618,\n  options = list(\n    opts_hover(css = \"fill: #202020;\"),\n    opts_hover_inv(css = \"opacity:0.2;\")\n  )\n)\n\n\n\n\n\n\n\n\nObservations:\n\nClass performance: Some classes, such as 3A and 3B, seem to have consistently higher scores compared to others like 3I and 3H.\nSpread of scores: Classes such as 3F and 3G show a broader spread of scores indicating a wider variation in student performance, as compared to 3B with a tighter range.\n\n\n\nCode\np &lt;- ggplot(data = exam_data, \n            aes(x = MATHS)) +\n  geom_density_interactive(\n    aes(tooltip = CLASS, \n        data_id = CLASS),\n    fill = \"#69b3a2\", \n    alpha = 0.5\n  ) +\n  scale_y_continuous(NULL, breaks = NULL)\n\ngirafe(\n  ggobj = p,\n  width_svg = 6,\n  height_svg = 6 * 0.618,\n  options = list(\n    opts_hover(css = \"fill: #202020;\"),\n    opts_hover_inv(css = \"opacity:0.2;\")\n  )\n)\n\n\n\n\n\n\n\n\n\n\nUsing plot_ly method to explore setting of colors for interactive plot\n\n\nGraphCode\n\n\nObservations:\n\nPositive correlation: Positive relationship between MATHS and ENGLISH scores, as higher MATHS scores generally correspond to higher ENGLISH scores.\nClass grouping: Class 3A generally score higher in both subjects, Class 3I tend to have lower scores in both subjects.\nOutliers: A few students score relatively low in one subject while doing better in other.\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot_ly(\n  data = exam_data,\n  x = ~MATHS,\n  y = ~ENGLISH,\n  type = 'scatter',\n  mode = 'markers',\n  color = ~CLASS,  # Dynamically color points based on CLASS\n  colors = colorRampPalette(c(\"blue\", \"green\", \"yellow\"))(length(unique(exam_data$CLASS))), \n  marker = list(\n    size = 10,  # Set marker size\n    opacity = 0.8  # Adjust transparency for better visual clarity\n  )\n) %&gt;%\n  layout(\n    title = \"Interactive Plot: MATHS vs ENGLISH Scores\",\n    xaxis = list(title = \"MATHS Scores\"),\n    yaxis = list(title = \"ENGLISH Scores\")\n  )\n\n\n\n\n\n\nUsing plot_ly method to explore setting of shapes for interactive plot\n\n\nGraphCode\n\n\nObservations:\n\nPositive Correlation: noticeable positive correlation between English and Math scores\nRacial Groups:\n\nChinese (green circles): Appears to be clustered among the top right corner, which suggests that most of the Chinese students performed well in both ENGLISH and MATHS scores\nMalay (blue squares): Appears to be concentrated around mid to lower ranges of ENGLISH and MATHS scores\nIndian (orange triangles): Appears less frequent, but appear in the middle range of scores\nOthers (pink crosses): Sparse, and distributed across the score ranges without clear clustering\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nTo assign different shapes for the points based on RACE variable in plot_ly, we used the symbol argument in addition to color.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot_ly(data = exam_data, \n        x = ~ENGLISH, \n        y = ~MATHS, \n        color = ~RACE,       # Different colors for RACE\n        symbol = ~RACE)      # Different shapes for RACE\n\n\n\n\n\n\nUsing plot_ly method to explore coordinated multiple views with plotly - Maths score and Race\n\n\nGraphCode\n\n\nObservations:\nHistogram\n\nOverall distribution: Most students score between 60 to 80, indicating average performance\nRace-specific observations:\n\nChinese students dominate the higher score range\nMalay students fared broadly in score\nIndian students are concentrated in the middle range\nOthers contribute mostly in the mid-to-high ranges, and less frequent overall\n\n\nBox plot\n\nMedian score: Chinese students have the highest median, followed by Others, Malay, while Indian fared slightly lower.\nVariability in score: Malay students show the widest spread, while others have the most consistent performance.\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Load required libraries\nlibrary(crosstalk)\nlibrary(plotly)\n\n# Ensure exam_data is a data frame\nexam_data &lt;- as.data.frame(exam_data)\n\n# Create a shared data object for interactivity\nshared_data &lt;- SharedData$new(exam_data)\n\n# Histogram for Overall Maths Scores (Colored by Race)\np1 &lt;- plot_ly(\n  data = shared_data,\n  x = ~MATHS,\n  color = ~RACE,\n  type = \"histogram\",\n  text = ~paste(\"Score:\", MATHS, \"&lt;br&gt;Race:\", RACE),  # Tooltip showing Maths Score and Race\n  hoverinfo = \"text\",  # Use the custom text for the tooltip\n  opacity = 0.7\n) %&gt;%\n  layout(\n    title = \"Overall Maths Scores Distribution\",\n    xaxis = list(\n      title = \"Maths Scores\",  # X-axis title\n      titlefont = list(size = 16),  # Larger font for better visibility\n      tickfont = list(size = 12)  # Tick label size\n    ),\n    yaxis = list(\n      title = \"Frequency\",  # Y-axis title\n      titlefont = list(size = 16),  # Larger font for better visibility\n      tickfont = list(size = 12)  # Tick label size\n    ),\n    barmode = \"stack\",  # Ensure legend is visible\n    showlegend = TRUE  # Enable legend\n  )\n\n# Box Plot for Maths Scores by Race\np2 &lt;- plot_ly(\n  data = shared_data,\n  x = ~RACE,\n  y = ~MATHS,\n  color = ~RACE,\n  type = \"box\",\n  text = ~paste(\"Race:\", RACE, \"&lt;br&gt;Score:\", MATHS),  # Tooltip showing Race and Score\n  hoverinfo = \"text\",  # Use the custom text for the tooltip\n  boxpoints = FALSE  # Removes points beside the boxplot\n) %&gt;%\n  layout(\n    title = \"Maths Scores Distribution by Race\",\n    xaxis = list(\n      title = \"Race\",  # X-axis title\n      titlefont = list(size = 16),  # Larger font for better visibility\n      tickfont = list(size = 12)  # Tick label size\n    ),\n    yaxis = list(\n      title = \"Maths Scores\",  # Y-axis title\n      titlefont = list(size = 16),  # Larger font for better visibility\n      tickfont = list(size = 12)  # Tick label size\n    ),\n    showlegend = FALSE  # Disable legend for the box plot\n  )\n\n# Combine the plots into a subplot with interactivity\nsubplot(p1, p2, nrows = 1, margin = 0.05) %&gt;%\n  highlight(on = \"plotly_click\", dynamic = TRUE)\n\n\n\n\n\ninstall.packages(“crosstalk”) library(crosstalk)\nlibrary(magrittr)"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04a.html",
    "href": "Hands-on_Ex/Hands-on_Ex04a.html",
    "title": "Hands-on Exercise 04a",
    "section": "",
    "text": "With the assistance of ChatGPT"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04a.html#visualising-distribution",
    "href": "Hands-on_Ex/Hands-on_Ex04a.html#visualising-distribution",
    "title": "Hands-on Exercise 04a",
    "section": "4. Visualising Distribution",
    "text": "4. Visualising Distribution\n\n4.1 Learning outcome\nVisualizing distributions is a fundamental aspect of statistical analysis. In Chapter 1, we introduced common methods such as histograms, probability density curves (PDFs), boxplots, notch plots, and violin plots using ggplot2.\nIn this chapter, we explore two newer techniques\n\nridgeline plots: display multiple density plots stacked vertically to compare distributions across groups,\nraincloud plots: combine a density plot, boxplot, and scatter plot to provide a comprehensive view of data\n\nby utilizing ggplot2 and its extensions.\n\n\n4.2 Getting started\n\n4.2.1 Installing and loading the packages\nThe following R packages will be used,\n\nggridges:ggplot2 extension for creating ridgeline plots.\nggdist:ggplot2 extension for visualizing distribution and uncertainty.\ntidyverse:collection of R packages designed for modern data science and visual communication.\nggthemes:ggplot2 extension offering additional themes, scales, and geoms for enhanced visualizations.\ncolorspace:R package for selecting, manipulating, and applying color palettes in visualizations.\n\n\n\nCode\npacman::p_load(ggdist, ggridges, ggthemes,\n               colorspace, tidyverse)\n\n\n\n\n4.2.2 Importing data\nIn this exercise, Exam_data.csv will be used. The `read_csv() function from the readr package is used to import the dataset into R and store it as a tibble data frame.\n\n\nCode\nexam &lt;- read_csv(\"data/Exam_data.csv\", show_col_types = FALSE)\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThis section data is the same dataset used in Hands-on_Ex01, Hands-on_Ex02 Hands-on_Ex03a\n\n\n\n\n\n\n4.3 Visualising distribution with ridgeline plot\nA ridgeline plot (also known as a Joyplot) visualizes the distribution of a numeric variable across multiple groups. It uses overlapping histograms or density plots aligned on the same horizontal scale.\nThe figure below illustrates the distribution of English scores by class using a ridgeline plot.\n\n\nCode\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS)) +\n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    bandwidth = 3.4,\n    fill = lighten(\"pink\", .3),\n    color = \"blue\"\n  ) +\n  scale_x_continuous(\n    name = \"ENGLISH\",\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = \"CLASS\", expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen to use a ridgeline plot\n\n\n\n\nRidgeline plots are ideal for visualizing medium to large groups, as overlapping distributions help conserve space compared to separate plots. However, for fewer than five groups, other distribution plots may be more effective.\nThey work best when there is a clear pattern or ranking among groups; otherwise, excessive overlap can make the plot cluttered and less informative.\n\n\n\n\n4.3.1 Plotting ridgeline graph: ggridges method\nIn R, ridgeline plots can be created using the ggridges package, which provides two key functions:\n\ngeom_ridgeline(): Uses direct height values to draw ridgelines.\ngeom_density_ridges(): Estimates data densities before plotting ridgelines.\n\nThe example below demonstrates a ridgeline plot created with geom_density_ridges().\n\nPlot()Code()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS)) +\n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    bandwidth = 3.4,\n    fill = lighten(\"white\", .3),\n    color = \"#7097BB\"\n  ) +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n    ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\n4.3.2 Varying fill colors along the x axis\nTo add gradient fills to ridgeline plots, use geom_ridgeline_gradient() or geom_density_ridges_gradient(), which function similarly to their non-gradient counterparts - geom_ridgeline(), and geom_density_ridges() but allow varying colors along the x-axis.\nHowever, they do not support alpha transparency—only color variation or transparency can be applied, but not both.\n\nPlot()Code()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS,\n           fill = stat(x))) +\n  geom_density_ridges_gradient(\n    scale = 3,\n    rel_min_height = 0.01) +\n  scale_fill_viridis_c(name = \"Temp. [F]\",\n                       option = \"C\") +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\n4.3.3 Mapping the probabilities directly onto color\nThe ggridges package extends ggplot2 by providing stat_density_ridges(), that replaces stat_density() for ridgeline plots.\nThe figure below uses stat(ecdf) to compute and visualize the empirical cumulative density function (ECDF) for the distribution of English scores.\n\nPlot()Code()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(exam,\n       aes(x = ENGLISH, \n           y = CLASS, \n           fill = 0.5 - abs(0.5-stat(ecdf)))) +\n  stat_density_ridges(geom = \"density_ridges_gradient\", \n                      calc_ecdf = TRUE) +\n  scale_fill_viridis_c(name = \"Tail probability\",\n                       direction = -1) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\nIt is important to include the argument calc_ecdf = TRUE in stat_density_ridges()\n\n\n\n\n\n4.3.4 Ridgeline plots with quantile lines\nUsing geom_density_ridges_gradient(), ridgeline plots can be colored by quantiles through the stat(quantile) aesthetic, as shown in the figure below.\n\nPlot()Code()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(exam,\n       aes(x = MATHS, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = 4,\n    quantile_lines = TRUE) +\n  scale_fill_viridis_d(name = \"Quartiles\") +\n  theme_ridges()\n\n\n\n\n\nInstead of using number to define the quantiles, we can use cut points (e.g., 2.5% and 97.5% tails), as shown in the figure below.\n\nPlot()Code()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(exam,\n       aes(x = MATHS, \n           y = CLASS, \n           fill = factor(stat(quantile))\n           )) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE, \n    quantiles = c(0.025, 0.975)\n    ) +\n  scale_fill_manual(\n    name = \"Probability\",\n    values = c(\"#FF0000A0\", \"#A0A0A0A0\", \"#0000FFA0\"),\n    labels = c(\"(0, 0.025]\", \"(0.025, 0.975]\", \"(0.975, 1]\")\n  ) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n4.4 Visualising distribution with Raincloud plot\nA Raincloud Plot is a visualization technique that combines a half-density plot with a boxplot, resembling a “raincloud” shape. It improves traditional boxplots by revealing multiple modes in the data, indicating potential group structures. Unlike boxplots, raincloud plots show where densities are clustered.\nIn this section, we will be learning how to create a raincloud plot to visualize English scores by race using functions from the ggdist and ggplot2 packages.\n\n4.4.1 Plotting a half eye graph\nFirst, by using stat_halfeye() from the ggdist package to create a Half-Eye plot, which combines a half-density plot with a slab-interval for visualizing distributions.\nThis produces a Half Eye visualization, which is contains a half-density and a slab-interval.\n\nGraphCode()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA)\n\n\n\n\n\n\n\n\nThings to learn from the code chunk above:\n\n\n\n\nWe remove the slab interval by setting .width = 0 and point_colour = NA.\n\n\n\n\n\n\n\n\n4.4.2 Adding the boxplot with geom_boxplot()\nNext, the code will add a narrow boxplot using geom_boxplot() from ggplot2, reducing its width and adjusting opacity for better visualization.\n\nGraphCode()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA)\n\n\n\n\n\n\n\n4.4.3 Adding the dot plots with stat_dots()\nNext, we’ll use stat_dots() from ggdist to add a half-dot plot, similar to a histogram, showing sample counts with dots.\nSetting side = “left” positions it on the left-hand side.\n\nGraphCode()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 2)\n\n\n\n\n\n\n\n4.4.4 Finishing touch\nLastly, coord_flip() from ggplot2 flips the chart horizontally, creating the raincloud effect, while theme_economist() from ggthemes enhances its appearance.\n\nGraphCode()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(exam, \n       aes(x = RACE, \n           y = ENGLISH)) +\n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA) +\n  geom_boxplot(width = .20,\n               outlier.shape = NA) +\n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = .5,\n            dotsize = 1.5) +\n  coord_flip() +\n  theme_economist()\n\n\n\n\n\n\n\n\n4.5 Reference\n\nIntroducing Ridgeline Plots (formerly Joyplots)\nClaus O. Wilke Fundamentals of Data Visualization especially Chapter 6, 7, 8, 9 and 10.\nAllen M, Poggiali D, Whitaker K et al. “Raincloud plots: a multi-platform tool for robust data. visualization” [version 2; peer review: 2 approved]. Welcome Open Res 2021, pp. 4:63.\nDots + interval stats and geoms\n\n\n\n4.6 Takeaways\n\n\n\n\n\n\nKey takeaways\n\n\n\n\nLearnt about the two distribution visualization methods\n\nRidgeline Plot ggridges and\nRaincloud Plot ggdist\n\nPackages and Tools used: ggridges, ggdist, tidyverse, ggthemes and colorspace\nRidgeline plots work best when there’s a clear ranking or pattern.\nRaincloud plots provide richer insights than boxplots by showing density clusters.\n\n\n\n\n\n4.7 Further exploration\n\nTo explore the different ways to change the gradient color in the ggplot2 ridgeline plot:\n\n\nChange Virdis Platte Option()Use scale_fill_distiller() for diverging or sequential colorsUse a custom color gradient\n\n\nModify the option argument in scale_fill_viridis_c(), which supports different palettes like “A”, “B”, “C”, “D”, and “E”.\n\n\nCode\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS,\n           fill = stat(x))) +\n  geom_density_ridges_gradient(\n    scale = 3,\n    rel_min_height = 0.01) +\n  scale_fill_viridis_c(name = \"English Score\",\n                       option = \"E\") +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n\n\nTo change palette argument to options to “Blues”, “Reds”, “PuBu”, or “RdYlGn” etc.\n\nRed refer to low score\nGreen refer to high score\n\n\n\nCode\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS,\n           fill = stat(x))) +\n  geom_density_ridges_gradient(\n    scale = 3,\n    rel_min_height = 0.01) +\n  scale_fill_distiller(name = \"English Score\",\n                       palette = \"RdYlGn\",\n                       direction = 1) +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n\n\nUse scale_fill_gradient() or scale_fill_gradientn() to define custom colors. scale_fill_gradient(): Using two colors to define colors. scale_fill_gradientn(): Using multiple colors, creates a multi-color gradient.\n\nRed refer to low score\nGreen refer to high score\n\n\n\nCode\nggplot(exam, \n       aes(x = ENGLISH, \n           y = CLASS,\n           fill = stat(x))) +\n  geom_density_ridges_gradient(\n    scale = 3,\n    rel_min_height = 0.01) +\n  scale_fill_gradient(name = \"English Score\", low = \"red\", high = \"green\")+\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges()\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo explore the creation of two ridgeline plot side by side\n\nObservations:\n\nEnglish scores show wider distributions, indicating more variation, while Maths scores have sharper peaks, suggesting consistent performance.\nHigher-level classes (3I, 3H, 3G) have more spread-out distributions, while lower-level classes (3A, 3B, 3C) are more concentrated at higher scores.\n\n\nGraphCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(exam_long, \n       aes(x = Score, \n           y = CLASS, \n           fill = Subject)) +  \n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    bandwidth = 3.4,\n    fill = lighten(\"#7097BB\", .3),  # Adjust color\n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    name = \"Grades\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(name = NULL, expand = expansion(add = c(0.2, 2.6))) +\n  theme_ridges() +\n  facet_wrap(~ Subject, scales = \"free_x\")  # Facet by Subject (English vs Maths)\n\n\n\n\n\n\nTo explore the creation of raincloud plot - GENDER against MATHS\n\nObservations:\n\nMales show a slight right-skewed distribution with more students scoring higher.\nFemales have a more balanced and evenly spread distribution across score levels.\nMedian scores are comparable, but Females show more variation.\n\n\nGraphCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(exam, \n       aes(x = MATHS, \n           y = GENDER, \n           fill = GENDER)) +  \n  stat_halfeye(adjust = 0.5,\n               justification = -0.2,\n               .width = 0,\n               point_colour = NA,\n               alpha = 0.6) +  \n  geom_boxplot(width = 0.2,\n               outlier.shape = NA,\n               alpha = 0.7, \n               color = \"black\") +  \n  stat_dots(side = \"left\", \n            justification = 1.2, \n            binwidth = 0.5,\n            dotsize = 1.5) +\n  scale_fill_manual(values = c(\"Male\" = \"blue\", \"Female\" = \"pink\")) + \n  coord_flip() +\n  labs(title = \"Raincloud Plot of MATHS Scores by Gender\",\n       x = \"MATHS Scores\",\n       y = \"Gender\") +\n  theme_classic() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04c.html",
    "href": "Hands-on_Ex/Hands-on_Ex04c.html",
    "title": "Hands-on Exercise 04c",
    "section": "",
    "text": "With the assistance of ChatGPT"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex04c.html#visualising-uncertainty",
    "href": "Hands-on_Ex/Hands-on_Ex04c.html#visualising-uncertainty",
    "title": "Hands-on Exercise 04c",
    "section": "4. Visualising uncertainty",
    "text": "4. Visualising uncertainty\n\n4.1 Learning outcome\nIn this chapter, we will be learning how to create statistical graphics to visualize uncertainty. By the end of the chapter, we will be able to:\n\nCreate statistical error bar plots using ggplot2.\nDevelop interactive error bar plots by integrating ggplot2, plotly, and DT.\nDesign advanced visualizations with the ggdist package.\nGenerate Hypothetical Outcome Plots (HOPs) using the ungeviz package.\n\n\n\n4.2 Getting started\n\n4.2.1 Installing and loading the packages\nFor this exercise the following R packages will be used:\n\ntidyverse: A collection of R packages for data science workflows\nplotly: Used for creating interactive plots.\ngganimate:Enables the creation of animated plots.\nDT:Displays interactive HTML tables.\ncrosstalk:Facilitates cross-widget interactions, including linked brushing and filtering.\nggdist:Helps visualize distributions and uncertainty.\n\n\n\nCode\ndevtools::install_github(\"wilkelab/ungeviz\")\n\n\n\n\nCode\npacman::p_load(ungeviz, plotly, crosstalk,\n               DT, ggdist, ggridges,\n               colorspace, gganimate, tidyverse)\n\n\n\n\n4.2.2 Data import\n\n\nCode\nexam &lt;- read_csv(\"data/Exam_data.csv\", show_col_types = FALSE)\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThis section data is the same dataset used in Hands-on_Ex01, Hands-on_Ex02, Hands-on_Ex03a, Hands-on_Ex04a, and Hands-on_Ex04b\n\n\n\n\n\n\n4.3 Visualizing the uncertainty of point estimates: ggplot2 methods\nA point estimate is a single number, such as a mean. Uncertainty is expressed as standard error, confidence interval, or credible interval\n\n\n\n\n\n\nImportant\n\n\n\nWe should not confuse the uncertainty of a point estimate with the variation in the sample.\n\n\nIn this section, we will be plotting error bars for MATHS scores based on race using the exam tibble data frame.\nThe following functions from the dplyr package and code will be used to calculate the required summary statistics.\n\ngroup_by():Groups the observations by RACE.\nsummarise(): Calculates the count of observations, along with the mean and standard deviation of MATHS scores.\nmutate(): Computes the standard error of MATHS scores for each race.\nThe output is stored as a tibble data frame named my_sum.\n\n\n\nCode\nmy_sum &lt;- exam %&gt;%\n  group_by(RACE) %&gt;%\n  summarise(\n    n=n(),\n    mean=mean(MATHS),\n    sd=sd(MATHS)\n    ) %&gt;%\n  mutate(se=sd/sqrt(n-1))\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nTo refer to Slide 20 of Lesson 4 for the mathematical explanation\n\n\n\nThe code below will be used to display my_sum tibble data frame in a html table format.\n\nCode()Table()\n\n\n\n\nCode\nknitr::kable(head(my_sum), format = 'html')\n\n\n\n\n\n\n\n\n\nRACE\nn\nmean\nsd\nse\n\n\n\n\nChinese\n193\n76.50777\n15.69040\n1.132357\n\n\nIndian\n12\n60.66667\n23.35237\n7.041005\n\n\nMalay\n108\n57.44444\n21.13478\n2.043177\n\n\nOthers\n9\n69.66667\n10.72381\n3.791438\n\n\n\n\n\n\n\n\n\n\n\n4.3.1 Plotting standard error bars of point estimates\nNow, we will plot the standard error bars for the mean MATHS scores by race, as shown below.\n\nThe error bars are computed by using the formula mean+/-se.\nFor geom_point(), it is important to indicate stat=“identity”\n\n\nCode()Graph()\n\n\n\n\nCode\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=RACE, \n        ymin=mean-se, \n        ymax=mean+se), \n    width=0.2, \n    colour=\"blue\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  ggtitle(\"Standard error of mean maths score by rac\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.3.2 Plotting confidence interval of point estimates\nIn addition to plotting standard error bars for point estimates, we can also visualize the confidence intervals of the mean MATHS scores by race.\n\nThe confidence intervals are calculated using the formula: mean ± 1.96 × standard error (SE).\nThe error bars are arranged based on the average MATHS scores.\nThe labs() function from ggplot2 is used to modify the x-axis label.\n\n\nCode()Graph()\n\n\n\n\nCode\nggplot(my_sum) +\n  geom_errorbar(\n    aes(x=reorder(RACE, -mean), \n        ymin=mean-1.96*se, \n        ymax=mean+1.96*se), \n    width=0.2, \n    colour=\"blue\", \n    alpha=0.9, \n    size=0.5) +\n  geom_point(aes\n           (x=RACE, \n            y=mean), \n           stat=\"identity\", \n           color=\"red\",\n           size = 1.5,\n           alpha=1) +\n  labs(x = \"Maths score\",\n       title = \"95% confidence interval of mean maths score by race\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.3.3 Visualizing the uncertainty of point estimates with interactive error bars\nWe will learn how to create interactive error bar plots to display the 99% confidence intervals of the mean MATHS scores by race, as shown in the figure below.\n\nCode()Graph()\n\n\n\n\nCode\nshared_df = SharedData$new(my_sum)\n\nbscols(widths = c(4,8),\n       ggplotly((ggplot(shared_df) +\n                   geom_errorbar(aes(\n                     x=reorder(RACE, -mean),\n                     ymin=mean-2.58*se, \n                     ymax=mean+2.58*se), \n                     width=0.2, \n                     colour=\"blue\", \n                     alpha=0.9, \n                     size=0.5) +\n                   geom_point(aes(\n                     x=RACE, \n                     y=mean, \n                     text = paste(\"Race:\", `RACE`, \n                                  \"&lt;br&gt;N:\", `n`,\n                                  \"&lt;br&gt;Avg. Scores:\", round(mean, digits = 2),\n                                  \"&lt;br&gt;95% CI:[\", \n                                  round((mean-2.58*se), digits = 2), \",\",\n                                  round((mean+2.58*se), digits = 2),\"]\")),\n                     stat=\"identity\", \n                     color=\"red\", \n                     size = 1.5, \n                     alpha=1) + \n                   xlab(\"Race\") + \n                   ylab(\"Average Scores\") + \n                   theme_minimal() + \n                   theme(axis.text.x = element_text(\n                     angle = 45, vjust = 0.5, hjust=1)) +\n                   ggtitle(\"99% Confidence interval of average /&lt;br&gt;maths scores by race\")), \n                tooltip = \"text\"), \n       DT::datatable(shared_df, \n                     rownames = FALSE, \n                     class=\"compact\", \n                     width=\"100%\", \n                     options = list(pageLength = 10,\n                                    scrollX=T), \n                     colnames = c(\"No. of pupils\", \n                                  \"Avg Scores\",\n                                  \"Std Dev\",\n                                  \"Std Error\")) %&gt;%\n         formatRound(columns=c('mean', 'sd', 'se'),\n                     digits=2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.4 Visualizing uncertainty: ggdist methods\n\nggdist is an R package that offers flexible ggplot2 geoms and stats specifically designed for visualizing distributions and uncertainty.\nIt supports both frequentist and Bayesian uncertainty visualization by focusing on distribution-based approaches:\n\nFor frequentist models, it visualizes confidence or bootstrap distributions (see vignette(“freq-uncertainty-vis”)).\nFor Bayesian models, it visualizes probability distributions, with extended functionality available through the tidybayes package.\n\n\n\n4.4.1 Visualizing the uncertainty of point estimates: ggdist methods\nstat_pointinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\nCode()Graph()\n\n\n\n\nCode\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval() +\n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis function comes with many arguments. In the code below, the following arguments are used:\n\nwidth = 0.95\npoint = median\ninterval = qi\n\nFor more information on the arguments available, please refer to this link.\n\nCode()Graph()\n\n\n\n\nCode\nexam %&gt;%\n  ggplot(aes(x = RACE, y = MATHS)) +\n  stat_pointinterval(.width = 0.95,\n  .point = median,\n  .interval = qi) +\n  labs(\n    title = \"Visualising confidence intervals of median math score\",\n    subtitle = \"Median Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.4.2 Visualizing the uncertainty of point estimates: ggdist methods\nWe will makeover the previous plot by showing 95% and 99% confidence intervals.\n\n.width = 0.95 and 0.99\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe .width argument in the stat_pointinterval function defines the coverage probability of the confidence interval around the summary statistic, such as the median.\nIt controls the visual width of the confidence interval displayed in the plot.\n\n\n\n\nCode()Graph()\n\n\n\n\nCode\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_pointinterval(\n    .width = c(0.95, 0.99), \n    show.legend = FALSE) +   \n  labs(\n    title = \"Visualising confidence intervals of mean Math score\",\n    subtitle = \"Mean Point + Multiple-interval plot\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.4.3 Visualizing the uncertainty of point estimates: ggdist methods\nstat_gradientinterval() of ggdist is used to build a visual for displaying distribution of maths scores by race.\n\nCode()Graph()\n\n\n\n\nCode\nexam %&gt;%\n  ggplot(aes(x = RACE, \n             y = MATHS)) +\n  stat_gradientinterval(   \n    fill = \"#E1B941\",      \n    show.legend = TRUE     \n  ) +                        \n  labs(\n    title = \"Visualising confidence intervals of mean math score\",\n    subtitle = \"Gradient + interval plot\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.5 Visualizing uncertainity with hypothetical outcome plots (HOPs)\nWhat are HOPs and when are they used?\n\nHOPs are visualizations that display multiple simulated outcomes to help users understand uncertainty in data or predictions. Instead of showing a static confidence interval, HOPs animate or present a series of possible outcomes to illustrate the variability in the data.\nUsed to help audiences intuitively understand variability and potential results in decision-making and forecasting.\n\n\nCode()Graph()\n\n\n\n\nCode\ndevtools::install_github(\"wilkelab/ungeviz\")\nlibrary(ungeviz)\n\nggplot(data = exam, \n       (aes(x = factor(RACE), y = MATHS))) +\n  geom_point(position = position_jitter(\n    height = 0.3, width = 0.05), \n    size = 0.4, color = \"#E1B941\", alpha = 1/2) +\n  geom_hpline(data = sampler(25, group = RACE), height = 0.6, color = \"#4169E1\") +\n  theme_bw() + \n  # `.draw` is a generated column indicating the sample draw\n  transition_states(.draw, 1, 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.7 References\n\nKam, T.S(2024). Visualising Uncertainty\n\n\n\n4.8 Takeaway\n\n\n\n\n\n\nKey takeaways\n\n\n\n\nUnderstand uncertainty in statistical graphics: uncertainty is expressed using standard errors/ confidence intervals.\nLearnt about plotting error bars with ggplot2: use geom_errorbar to represent standard errors around point estimates\nLearnt about creating interactive error bars with plotly and DT\nLearnt about visualizing uncertainty with Hypothetical Outcome Plots (HOPs): displaying multiple stimulated outcomes to show data variability.\nLearnt the difference about the uncertainty of point estimates with same variation, where uncertainty reflects estimation confidence, while variation shows data spread.\n\n\n\n\n\n4.9 Further exploration\n\nTo further explore Hypothetical Outcome Plots (HOPs) using exam dataset - By increasing the number of stimulated outcomes\n\n\nIncrease the number of samples from 25 to 150.\n\n\n\n\n\n\n\nNote\n\n\n\n\nBy increasing the number of samples, it shows more potential outcomes, providing a richer picture of uncertainty.\n\n\n\n\nCode()Graph()\n\n\n\n\nCode\ndevtools::install_github(\"wilkelab/ungeviz\")\nlibrary(ungeviz)\n\nggplot(data = exam, \n       aes(x = factor(RACE), y = MATHS)) +\n  geom_point(position = position_jitter(height = 0.3, width = 0.05), \n             size = 0.4, color = \"#E1B941\", alpha = 1/2) +\n  geom_hpline(data = sampler(150, group = RACE), height = 0.6, color = \"#4169E1\") +  # Increased from 25 to 150\n  theme_minimal() + \n  transition_states(.draw, transition_length = 2, state_length = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo further visualize how confidence intervals for average MATHS scores shift across class (using `ggdist)\n\n\n\n\n\n\n\nNote\n\n\n\n\nHelps to track how uncertainty (confidence intervals) widens or narrows across different classes\n\n\n\n\nCode()Animated Graph()Static Graph()\n\n\n\n\nCode\nlibrary(ggdist)\nlibrary(gganimate)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Clean data to remove any missing CLASS values\nexam_clean &lt;- exam %&gt;% filter(!is.na(CLASS))\n\n# Animated plot\nggplot(exam_clean, aes(x = factor(CLASS), y = MATHS)) +\n  stat_pointinterval(aes(color = RACE), \n                      .width = c(0.66, 0.95), \n                      position = position_dodge(width = 0.5)) +\n  labs(title = \"Confidence Intervals of Maths Scores Across Classes\",\n       subtitle = \"Class: {closest_state}\",  # Dynamic subtitle reflecting the current class\n       y = \"Maths Score\") +\n  theme_minimal() +\n  transition_states(CLASS, transition_length = 2, state_length = 1) +  # Handles categorical data\n  ease_aes('cubic-in-out')  # Smooth animation transitions"
  },
  {
    "objectID": "In-class_Ex/In-class_Outline.html",
    "href": "In-class_Ex/In-class_Outline.html",
    "title": "In-class Exercise 01",
    "section": "",
    "text": "Welcome to my Tableau-tastic adventure!🚀, where raw data meets creativity and transforms into stunning dashboards! From bar charts to pie charts, we will be exploring the colorful world of Tableau, turning numbers into insights and dashboards into works of art.\nLeveraging on VizQL, the proprietary query language behind the Tableau product, with a few clicks, and simple drag-and-drop features, we can translates raw data into powerful visuals in just a few clicks!\nJoin me in the journey of turning data into art and numbers into stories worth sharing!"
  },
  {
    "objectID": "In-class_Ex/In-class_Outline.html#welcome-to-my-tableau-page",
    "href": "In-class_Ex/In-class_Outline.html#welcome-to-my-tableau-page",
    "title": "In-class Exercise 01",
    "section": "",
    "text": "Welcome to my Tableau-tastic adventure!🚀, where raw data meets creativity and transforms into stunning dashboards! From bar charts to pie charts, we will be exploring the colorful world of Tableau, turning numbers into insights and dashboards into works of art.\nLeveraging on VizQL, the proprietary query language behind the Tableau product, with a few clicks, and simple drag-and-drop features, we can translates raw data into powerful visuals in just a few clicks!\nJoin me in the journey of turning data into art and numbers into stories worth sharing!"
  },
  {
    "objectID": "New.html",
    "href": "New.html",
    "title": "New",
    "section": "",
    "text": "Data science hoepfully is easy"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#getting-started",
    "href": "Take-home_Ex/Take-home_Ex01.html#getting-started",
    "title": "Take-home Exercise 01",
    "section": "",
    "text": "The following R packages will be loaded for this exercise using pacman::p_load():\n\nreadr: Part of a tidyverse package for fast and efficient reading of rectangular data (CSV, TSV, and other delimited files) into R\ndplyr: Part of a tidyverse package for efficient data manipulation, including filtering, selecting, mutating, summarizing, and grouping data in R\nggplot2: Part of a tidyverse package allowing for flexible and layered creation of complex plots\ntidyverse: A collection of R packages for data manipulation, visualization, and analysis\nknitr: Enables dynamic report generation with R Markdown\npatchwork: Combines multiple ggplot2 plots into a single layout\nggthemes: Provides additional themes and scales for ggplot2\nggridges: Creates ridge plots for density visualization\nFunnelPlotrR:Used to create funnel plots for visualizing and comparing proportions, rates, or ratios across different groups while accounting for statistical variation.\nperformance: Provides tools for assessing and comparing statistical models, including metrics for model quality, diagnostics, and visualization.\nstats: Provides essential statistical functions for modeling, hypothesis testing, regression, and probability distributions.\nparameters: Provides tools for processing, reporting, and visualizing model parameters in a tidy and user-friendly format.\nggstatsplot: Creates visualizations with statistical details embedded, combining ggplot2 with statistical tests for easy interpretation.\npatchwork: Allows easy combination and arrangement of multiple ggplot2 plots into a single cohesive layout.\n\n\n\nCode\npacman::p_load(tidyverse, knitr,\n               patchwork, ggthemes, scales,\n               ggridges, ggdist, ggtext, ggalt,\n               cowplot, ggnewscale,FunnelPlotR, performance, parameters, ggstatsplot)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#data-wrangling",
    "href": "Take-home_Ex/Take-home_Ex01.html#data-wrangling",
    "title": "Take-home Exercise 01",
    "section": "",
    "text": "The code chunk below imports the Heart Attack in Japan: Youth vs. Adult dataset, downloaded from Kaggle, using the read_csv() function from the readr package.\n\n\nCode\nheart_attack &lt;- read_csv(\"data/japan_heart_attack_dataset.csv\", show_col_types = FALSE)\n\nheart_attack\n\n\n# A tibble: 30,000 × 32\n     Age Gender Region Smoking_History Diabetes_History Hypertension_History\n   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;           &lt;chr&gt;            &lt;chr&gt;               \n 1    56 Male   Urban  Yes             No               No                  \n 2    69 Male   Urban  No              No               No                  \n 3    46 Male   Rural  Yes             No               No                  \n 4    32 Female Urban  No              No               No                  \n 5    60 Female Rural  No              No               No                  \n 6    25 Female Rural  No              No               No                  \n 7    78 Male   Urban  No              Yes              Yes                 \n 8    38 Female Urban  Yes             No               No                  \n 9    56 Male   Rural  No              No               Yes                 \n10    75 Male   Urban  No              No               No                  \n# ℹ 29,990 more rows\n# ℹ 26 more variables: Cholesterol_Level &lt;dbl&gt;, Physical_Activity &lt;chr&gt;,\n#   Diet_Quality &lt;chr&gt;, Alcohol_Consumption &lt;chr&gt;, Stress_Levels &lt;dbl&gt;,\n#   BMI &lt;dbl&gt;, Heart_Rate &lt;dbl&gt;, Systolic_BP &lt;dbl&gt;, Diastolic_BP &lt;dbl&gt;,\n#   Family_History &lt;chr&gt;, Heart_Attack_Occurrence &lt;chr&gt;, Extra_Column_1 &lt;dbl&gt;,\n#   Extra_Column_2 &lt;dbl&gt;, Extra_Column_3 &lt;dbl&gt;, Extra_Column_4 &lt;dbl&gt;,\n#   Extra_Column_5 &lt;dbl&gt;, Extra_Column_6 &lt;dbl&gt;, Extra_Column_7 &lt;dbl&gt;, …\n\n\nThe dataset is structured as a tibble dataframe, containing 30,000 rows and 32 columns. Each observation represents an individual case, and the variables capture key medical and demographic information relevant to heart attack incidents across different age groups in Japan.\n\n\n\nWe will check the dataset using below\n\nglimpse(): provides a transposed overview of a dataset, showing variables and their types in a concise format.\nhead(): displays the first few rows of a dataset (default is 6 rows) to give a quick preview of the data.\nsummary(): generates a statistical summary of each variable, including measures like mean, median, and range for numeric data.\nduplicated():returns a logical vector indicating which elements or rows in a vector or data frame are duplicates.\ncolSums(is.na()): counts the number of missing values (NA) in each column of the data frame.\nspec(): use spec() to quickly inspect the column\n\n\nglimpse()head()summary()duplicated()colSum(is.na())spec())\n\n\n\n\nCode\nglimpse(heart_attack)\n\n\nRows: 30,000\nColumns: 32\n$ Age                     &lt;dbl&gt; 56, 69, 46, 32, 60, 25, 78, 38, 56, 75, 36, 40…\n$ Gender                  &lt;chr&gt; \"Male\", \"Male\", \"Male\", \"Female\", \"Female\", \"F…\n$ Region                  &lt;chr&gt; \"Urban\", \"Urban\", \"Rural\", \"Urban\", \"Rural\", \"…\n$ Smoking_History         &lt;chr&gt; \"Yes\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"No\", \"Y…\n$ Diabetes_History        &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No…\n$ Hypertension_History    &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No…\n$ Cholesterol_Level       &lt;dbl&gt; 186.4002, 185.1367, 210.6966, 211.1655, 223.81…\n$ Physical_Activity       &lt;chr&gt; \"Moderate\", \"Low\", \"Low\", \"Moderate\", \"High\", …\n$ Diet_Quality            &lt;chr&gt; \"Poor\", \"Good\", \"Average\", \"Good\", \"Good\", \"Go…\n$ Alcohol_Consumption     &lt;chr&gt; \"Low\", \"Low\", \"Moderate\", \"High\", \"High\", \"Hig…\n$ Stress_Levels           &lt;dbl&gt; 3.644786, 3.384056, 3.810911, 6.014878, 6.8068…\n$ BMI                     &lt;dbl&gt; 33.96135, 28.24287, 27.60121, 23.71729, 19.771…\n$ Heart_Rate              &lt;dbl&gt; 72.30153, 57.45764, 64.65870, 55.13147, 76.667…\n$ Systolic_BP             &lt;dbl&gt; 123.90209, 129.89331, 145.65490, 131.78522, 10…\n$ Diastolic_BP            &lt;dbl&gt; 85.68281, 73.52426, 71.99481, 68.21133, 92.902…\n$ Family_History          &lt;chr&gt; \"No\", \"Yes\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No…\n$ Heart_Attack_Occurrence &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n$ Extra_Column_1          &lt;dbl&gt; 0.40498852, 0.03627815, 0.85297888, 0.39085280…\n$ Extra_Column_2          &lt;dbl&gt; 0.43330004, 0.51256694, 0.21959083, 0.29684675…\n$ Extra_Column_3          &lt;dbl&gt; 0.62871236, 0.66839275, 0.61343656, 0.15572404…\n$ Extra_Column_4          &lt;dbl&gt; 0.70160955, 0.11552874, 0.50800995, 0.87025144…\n$ Extra_Column_5          &lt;dbl&gt; 0.49814235, 0.42381938, 0.90066981, 0.39035591…\n$ Extra_Column_6          &lt;dbl&gt; 0.007901312, 0.083932768, 0.227205241, 0.40318…\n$ Extra_Column_7          &lt;dbl&gt; 0.79458257, 0.68895108, 0.49634358, 0.74140891…\n$ Extra_Column_8          &lt;dbl&gt; 0.29077922, 0.83016364, 0.75210679, 0.22396813…\n$ Extra_Column_9          &lt;dbl&gt; 0.49719307, 0.63449028, 0.18150125, 0.32931387…\n$ Extra_Column_10         &lt;dbl&gt; 0.52199452, 0.30204337, 0.62918031, 0.14319054…\n$ Extra_Column_11         &lt;dbl&gt; 0.79965663, 0.04368285, 0.01827617, 0.90778075…\n$ Extra_Column_12         &lt;dbl&gt; 0.72239788, 0.45166789, 0.06322702, 0.54232201…\n$ Extra_Column_13         &lt;dbl&gt; 0.1487387, 0.8786714, 0.1465122, 0.9224606, 0.…\n$ Extra_Column_14         &lt;dbl&gt; 0.8340099, 0.5356022, 0.9972962, 0.6262165, 0.…\n$ Extra_Column_15         &lt;dbl&gt; 0.061632229, 0.617825340, 0.974455410, 0.22860…\n\n\n\n\n\n\nCode\nhead(heart_attack)\n\n\n# A tibble: 6 × 32\n    Age Gender Region Smoking_History Diabetes_History Hypertension_History\n  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;           &lt;chr&gt;            &lt;chr&gt;               \n1    56 Male   Urban  Yes             No               No                  \n2    69 Male   Urban  No              No               No                  \n3    46 Male   Rural  Yes             No               No                  \n4    32 Female Urban  No              No               No                  \n5    60 Female Rural  No              No               No                  \n6    25 Female Rural  No              No               No                  \n# ℹ 26 more variables: Cholesterol_Level &lt;dbl&gt;, Physical_Activity &lt;chr&gt;,\n#   Diet_Quality &lt;chr&gt;, Alcohol_Consumption &lt;chr&gt;, Stress_Levels &lt;dbl&gt;,\n#   BMI &lt;dbl&gt;, Heart_Rate &lt;dbl&gt;, Systolic_BP &lt;dbl&gt;, Diastolic_BP &lt;dbl&gt;,\n#   Family_History &lt;chr&gt;, Heart_Attack_Occurrence &lt;chr&gt;, Extra_Column_1 &lt;dbl&gt;,\n#   Extra_Column_2 &lt;dbl&gt;, Extra_Column_3 &lt;dbl&gt;, Extra_Column_4 &lt;dbl&gt;,\n#   Extra_Column_5 &lt;dbl&gt;, Extra_Column_6 &lt;dbl&gt;, Extra_Column_7 &lt;dbl&gt;,\n#   Extra_Column_8 &lt;dbl&gt;, Extra_Column_9 &lt;dbl&gt;, Extra_Column_10 &lt;dbl&gt;, …\n\n\n\n\n\n\nCode\nsummary(heart_attack)\n\n\n      Age           Gender             Region          Smoking_History   \n Min.   :18.00   Length:30000       Length:30000       Length:30000      \n 1st Qu.:33.00   Class :character   Class :character   Class :character  \n Median :48.00   Mode  :character   Mode  :character   Mode  :character  \n Mean   :48.49                                                           \n 3rd Qu.:64.00                                                           \n Max.   :79.00                                                           \n Diabetes_History   Hypertension_History Cholesterol_Level Physical_Activity \n Length:30000       Length:30000         Min.   : 80.02    Length:30000      \n Class :character   Class :character     1st Qu.:179.55    Class :character  \n Mode  :character   Mode  :character     Median :199.77    Mode  :character  \n                                         Mean   :199.90                      \n                                         3rd Qu.:220.16                      \n                                         Max.   :336.86                      \n Diet_Quality       Alcohol_Consumption Stress_Levels         BMI       \n Length:30000       Length:30000        Min.   : 0.000   Min.   : 5.58  \n Class :character   Class :character    1st Qu.: 3.644   1st Qu.:21.63  \n Mode  :character   Mode  :character    Median : 4.993   Median :24.96  \n                                        Mean   : 5.002   Mean   :25.00  \n                                        3rd Qu.: 6.353   3rd Qu.:28.36  \n                                        Max.   :10.000   Max.   :46.10  \n   Heart_Rate      Systolic_BP      Diastolic_BP    Family_History    \n Min.   : 30.03   Min.   : 56.23   Min.   : 39.95   Length:30000      \n 1st Qu.: 63.25   1st Qu.:109.79   1st Qu.: 73.26   Class :character  \n Median : 69.95   Median :119.90   Median : 80.12   Mode  :character  \n Mean   : 69.98   Mean   :119.91   Mean   : 80.03                     \n 3rd Qu.: 76.66   3rd Qu.:130.02   3rd Qu.: 86.76                     \n Max.   :108.78   Max.   :178.77   Max.   :117.67                     \n Heart_Attack_Occurrence Extra_Column_1     Extra_Column_2     \n Length:30000            Min.   :0.000007   Min.   :0.0000052  \n Class :character        1st Qu.:0.253308   1st Qu.:0.2473606  \n Mode  :character        Median :0.500820   Median :0.4961980  \n                         Mean   :0.501939   Mean   :0.4978940  \n                         3rd Qu.:0.750529   3rd Qu.:0.7473954  \n                         Max.   :0.999965   Max.   :0.9999894  \n Extra_Column_3      Extra_Column_4      Extra_Column_5     Extra_Column_6     \n Min.   :0.0000227   Min.   :0.0000934   Min.   :0.000105   Min.   :0.0000531  \n 1st Qu.:0.2483093   1st Qu.:0.2522110   1st Qu.:0.251803   1st Qu.:0.2559989  \n Median :0.4976104   Median :0.4976175   Median :0.501987   Median :0.5017726  \n Mean   :0.4981949   Mean   :0.5005952   Mean   :0.501410   Mean   :0.5027631  \n 3rd Qu.:0.7476807   3rd Qu.:0.7505662   3rd Qu.:0.753657   3rd Qu.:0.7511886  \n Max.   :0.9999694   Max.   :0.9999869   Max.   :0.999995   Max.   :0.9998892  \n Extra_Column_7      Extra_Column_8      Extra_Column_9     \n Min.   :0.0000678   Min.   :0.0000449   Min.   :0.0000305  \n 1st Qu.:0.2482839   1st Qu.:0.2509790   1st Qu.:0.2502452  \n Median :0.4988157   Median :0.4985698   Median :0.4984491  \n Mean   :0.4980753   Mean   :0.5003557   Mean   :0.5002292  \n 3rd Qu.:0.7456378   3rd Qu.:0.7507293   3rd Qu.:0.7512186  \n Max.   :0.9999900   Max.   :0.9999300   Max.   :0.9999852  \n Extra_Column_10     Extra_Column_11     Extra_Column_12    \n Min.   :0.0000133   Min.   :0.0000008   Min.   :0.0000713  \n 1st Qu.:0.2484256   1st Qu.:0.2538092   1st Qu.:0.2505341  \n Median :0.5031040   Median :0.5067589   Median :0.5038609  \n Mean   :0.5010694   Mean   :0.5044949   Mean   :0.5008624  \n 3rd Qu.:0.7522686   3rd Qu.:0.7556257   3rd Qu.:0.7511780  \n Max.   :0.9999928   Max.   :0.9999578   Max.   :0.9999484  \n Extra_Column_13     Extra_Column_14     Extra_Column_15    \n Min.   :0.0000204   Min.   :0.0000025   Min.   :0.0000241  \n 1st Qu.:0.2473108   1st Qu.:0.2482152   1st Qu.:0.2482573  \n Median :0.5041162   Median :0.4943841   Median :0.5009406  \n Mean   :0.5004557   Mean   :0.4976507   Mean   :0.4999634  \n 3rd Qu.:0.7497094   3rd Qu.:0.7456212   3rd Qu.:0.7487379  \n Max.   :0.9999451   Max.   :0.9999779   Max.   :0.9999913  \n\n\n\n\n\n\nCode\nheart_attack[duplicated(heart_attack),]\n\n\n# A tibble: 0 × 32\n# ℹ 32 variables: Age &lt;dbl&gt;, Gender &lt;chr&gt;, Region &lt;chr&gt;, Smoking_History &lt;chr&gt;,\n#   Diabetes_History &lt;chr&gt;, Hypertension_History &lt;chr&gt;,\n#   Cholesterol_Level &lt;dbl&gt;, Physical_Activity &lt;chr&gt;, Diet_Quality &lt;chr&gt;,\n#   Alcohol_Consumption &lt;chr&gt;, Stress_Levels &lt;dbl&gt;, BMI &lt;dbl&gt;,\n#   Heart_Rate &lt;dbl&gt;, Systolic_BP &lt;dbl&gt;, Diastolic_BP &lt;dbl&gt;,\n#   Family_History &lt;chr&gt;, Heart_Attack_Occurrence &lt;chr&gt;, Extra_Column_1 &lt;dbl&gt;,\n#   Extra_Column_2 &lt;dbl&gt;, Extra_Column_3 &lt;dbl&gt;, Extra_Column_4 &lt;dbl&gt;, …\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nEnsure that there are no duplicated columns, if not will have to investigate further.\n\n\n\n\n\n\n\nCode\ncolSums(is.na(heart_attack))\n\n\n                    Age                  Gender                  Region \n                      0                       0                       0 \n        Smoking_History        Diabetes_History    Hypertension_History \n                      0                       0                       0 \n      Cholesterol_Level       Physical_Activity            Diet_Quality \n                      0                       0                       0 \n    Alcohol_Consumption           Stress_Levels                     BMI \n                      0                       0                       0 \n             Heart_Rate             Systolic_BP            Diastolic_BP \n                      0                       0                       0 \n         Family_History Heart_Attack_Occurrence          Extra_Column_1 \n                      0                       0                       0 \n         Extra_Column_2          Extra_Column_3          Extra_Column_4 \n                      0                       0                       0 \n         Extra_Column_5          Extra_Column_6          Extra_Column_7 \n                      0                       0                       0 \n         Extra_Column_8          Extra_Column_9         Extra_Column_10 \n                      0                       0                       0 \n        Extra_Column_11         Extra_Column_12         Extra_Column_13 \n                      0                       0                       0 \n        Extra_Column_14         Extra_Column_15 \n                      0                       0 \n\n\n\n\n\n\n\n\nNote\n\n\n\n\nEnsure that there are no NA values, if not will have to investigate further.\nPossibility to use drop_na() function to drop rows where any column specified contains a missing value.\n\n\n\n\n\n\n\nCode\nspec(heart_attack)\n\n\ncols(\n  Age = col_double(),\n  Gender = col_character(),\n  Region = col_character(),\n  Smoking_History = col_character(),\n  Diabetes_History = col_character(),\n  Hypertension_History = col_character(),\n  Cholesterol_Level = col_double(),\n  Physical_Activity = col_character(),\n  Diet_Quality = col_character(),\n  Alcohol_Consumption = col_character(),\n  Stress_Levels = col_double(),\n  BMI = col_double(),\n  Heart_Rate = col_double(),\n  Systolic_BP = col_double(),\n  Diastolic_BP = col_double(),\n  Family_History = col_character(),\n  Heart_Attack_Occurrence = col_character(),\n  Extra_Column_1 = col_double(),\n  Extra_Column_2 = col_double(),\n  Extra_Column_3 = col_double(),\n  Extra_Column_4 = col_double(),\n  Extra_Column_5 = col_double(),\n  Extra_Column_6 = col_double(),\n  Extra_Column_7 = col_double(),\n  Extra_Column_8 = col_double(),\n  Extra_Column_9 = col_double(),\n  Extra_Column_10 = col_double(),\n  Extra_Column_11 = col_double(),\n  Extra_Column_12 = col_double(),\n  Extra_Column_13 = col_double(),\n  Extra_Column_14 = col_double(),\n  Extra_Column_15 = col_double()\n)\n\n\n\n\n\nThe heart_attack tibble contains 32 attributes, as shown above.\n\n\n\nThe table below presents the metadata for the dataset, classifying each variable as either categorical or continuous based on its nature and data type.\n\nCategorical attributes:\n\n\n\n\n\n\n\n\nVariable Names\nType\nDescription\n\n\n\n\nGender\nCHAR\nBiological sex of the individual (e.g., Male, Female)\n\n\nRegion\nCHAR\nGeographic location where the individual resides (e.g., Urban, Rural)\n\n\nSmoking_History\nCHAR\nPast or current smoking habits (e.g., Yes, No)\n\n\nDiabetes_History\nCHAR\nHistory of diabetes diagnosis (Yes/No)\n\n\nHypertension_History\nCHAR\nHistory of high blood pressure diagnosis (Yes/No)\n\n\nPhysical_Activity\nCHAR\nLevel of physical activity (e.g., Low, Moderate, High)\n\n\nDiet_Quality\nCHAR\nDietary habits and nutritional intake assessment (e.g., Poor, Average, Good)\n\n\nAlcohol_Consumption\nCHAR\nFrequency or amount of alcohol intake (e.g., Low, Moderate, High, None)\n\n\nFamily_History\nCHAR\nPresence of heart disease in close relatives (Yes/No)\n\n\nHeart_Attack_Occurrence\nCHAR\nWhether the individual has experienced a heart attack (Yes/No)\n\n\n\nContinuous attributes:\n\n\n\n\n\n\n\n\nVariable Names\nType\nDescription\n\n\n\n\nAge\nNUM\nAge of the individual in years\n\n\nCholesterol_Level\nNUM\nMeasured cholesterol level in the blood\n\n\nStress_Levels\nNUM\nMeasured or self-reported stress level\n\n\nBMI\nNUM\nBody Mass Index, calculated from height and weight\n\n\nHeart_Rate\nNUM\nResting heart rate in beats per minute (bpm)\n\n\nSystolic_BP\nNUM\nSystolic blood pressure measurement (mmHg)\n\n\nDiastolic_BP\nNUM\nDiastolic blood pressure measurement (mmHg)\n\n\nExtra_Column_1\nNUM\n\n\n\nExtra_Column_2\nNUM\n\n\n\nExtra_Column_3\nNUM\n\n\n\nExtra_Column_4\nNUM\n\n\n\nExtra_Column_5\nNUM\n\n\n\nExtra_Column_6\nNUM\n\n\n\nExtra_Column_7\nNUM\n\n\n\nExtra_Column_8\nNUM\n\n\n\nExtra_Column_9\nNUM\n\n\n\nExtra_Column_10\nNUM\n\n\n\nExtra_Column_11\nNUM\n\n\n\nExtra_Column_12\nNUM\n\n\n\nExtra_Column_13\nNUM\n\n\n\nExtra_Column_14\nNUM\n\n\n\nExtra_Column_15\nNUM\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nNotice that Extra_Column_1 to Extra_Column_15 are additional numeric figures without clear definition.\nThey are most likely derived calculations from the variables."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#data-wrangling-data-prepration",
    "href": "Take-home_Ex/Take-home_Ex01.html#data-wrangling-data-prepration",
    "title": "Take-home Exercise 01",
    "section": "",
    "text": "The code chunk below imports the Heart Attack in Japan: Youth vs. Adult dataset, downloaded from Kaggle, using the read_csv() function from the readr package.\n\n\nCode\nheart_attack &lt;- read_csv(\"data/japan_heart_attack_dataset.csv\", show_col_types = FALSE)\n\nheart_attack\n\n\n# A tibble: 30,000 × 32\n     Age Gender Region Smoking_History Diabetes_History Hypertension_History\n   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;           &lt;chr&gt;            &lt;chr&gt;               \n 1    56 Male   Urban  Yes             No               No                  \n 2    69 Male   Urban  No              No               No                  \n 3    46 Male   Rural  Yes             No               No                  \n 4    32 Female Urban  No              No               No                  \n 5    60 Female Rural  No              No               No                  \n 6    25 Female Rural  No              No               No                  \n 7    78 Male   Urban  No              Yes              Yes                 \n 8    38 Female Urban  Yes             No               No                  \n 9    56 Male   Rural  No              No               Yes                 \n10    75 Male   Urban  No              No               No                  \n# ℹ 29,990 more rows\n# ℹ 26 more variables: Cholesterol_Level &lt;dbl&gt;, Physical_Activity &lt;chr&gt;,\n#   Diet_Quality &lt;chr&gt;, Alcohol_Consumption &lt;chr&gt;, Stress_Levels &lt;dbl&gt;,\n#   BMI &lt;dbl&gt;, Heart_Rate &lt;dbl&gt;, Systolic_BP &lt;dbl&gt;, Diastolic_BP &lt;dbl&gt;,\n#   Family_History &lt;chr&gt;, Heart_Attack_Occurrence &lt;chr&gt;, Extra_Column_1 &lt;dbl&gt;,\n#   Extra_Column_2 &lt;dbl&gt;, Extra_Column_3 &lt;dbl&gt;, Extra_Column_4 &lt;dbl&gt;,\n#   Extra_Column_5 &lt;dbl&gt;, Extra_Column_6 &lt;dbl&gt;, Extra_Column_7 &lt;dbl&gt;, …\n\n\nThe dataset is structured as a tibble dataframe, containing 30,000 rows and 32 columns. Each observation represents an individual case, and the variables capture key medical and demographic information relevant to heart attack incidents across different age groups in Japan.\n\n\n\nWe will check the dataset using below\n\nglimpse(): provides a transposed overview of a dataset, showing variables and their types in a concise format.\nhead(): displays the first few rows of a dataset (default is 6 rows) to give a quick preview of the data.\nsummary(): generates a statistical summary of each variable, including measures like mean, median, and range for numeric data.\nduplicated():returns a logical vector indicating which elements or rows in a vector or data frame are duplicates.\ncolSums(is.na()): counts the number of missing values (NA) in each column of the data frame.\nspec(): use spec() to quickly inspect the column\n\n\nglimpse()head()summary()duplicated()colSum(is.na())spec())\n\n\n\n\nCode\nglimpse(heart_attack)\n\n\nRows: 30,000\nColumns: 32\n$ Age                     &lt;dbl&gt; 56, 69, 46, 32, 60, 25, 78, 38, 56, 75, 36, 40…\n$ Gender                  &lt;chr&gt; \"Male\", \"Male\", \"Male\", \"Female\", \"Female\", \"F…\n$ Region                  &lt;chr&gt; \"Urban\", \"Urban\", \"Rural\", \"Urban\", \"Rural\", \"…\n$ Smoking_History         &lt;chr&gt; \"Yes\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"No\", \"Y…\n$ Diabetes_History        &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No…\n$ Hypertension_History    &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No…\n$ Cholesterol_Level       &lt;dbl&gt; 186.4002, 185.1367, 210.6966, 211.1655, 223.81…\n$ Physical_Activity       &lt;chr&gt; \"Moderate\", \"Low\", \"Low\", \"Moderate\", \"High\", …\n$ Diet_Quality            &lt;chr&gt; \"Poor\", \"Good\", \"Average\", \"Good\", \"Good\", \"Go…\n$ Alcohol_Consumption     &lt;chr&gt; \"Low\", \"Low\", \"Moderate\", \"High\", \"High\", \"Hig…\n$ Stress_Levels           &lt;dbl&gt; 3.644786, 3.384056, 3.810911, 6.014878, 6.8068…\n$ BMI                     &lt;dbl&gt; 33.96135, 28.24287, 27.60121, 23.71729, 19.771…\n$ Heart_Rate              &lt;dbl&gt; 72.30153, 57.45764, 64.65870, 55.13147, 76.667…\n$ Systolic_BP             &lt;dbl&gt; 123.90209, 129.89331, 145.65490, 131.78522, 10…\n$ Diastolic_BP            &lt;dbl&gt; 85.68281, 73.52426, 71.99481, 68.21133, 92.902…\n$ Family_History          &lt;chr&gt; \"No\", \"Yes\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No…\n$ Heart_Attack_Occurrence &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\"…\n$ Extra_Column_1          &lt;dbl&gt; 0.40498852, 0.03627815, 0.85297888, 0.39085280…\n$ Extra_Column_2          &lt;dbl&gt; 0.43330004, 0.51256694, 0.21959083, 0.29684675…\n$ Extra_Column_3          &lt;dbl&gt; 0.62871236, 0.66839275, 0.61343656, 0.15572404…\n$ Extra_Column_4          &lt;dbl&gt; 0.70160955, 0.11552874, 0.50800995, 0.87025144…\n$ Extra_Column_5          &lt;dbl&gt; 0.49814235, 0.42381938, 0.90066981, 0.39035591…\n$ Extra_Column_6          &lt;dbl&gt; 0.007901312, 0.083932768, 0.227205241, 0.40318…\n$ Extra_Column_7          &lt;dbl&gt; 0.79458257, 0.68895108, 0.49634358, 0.74140891…\n$ Extra_Column_8          &lt;dbl&gt; 0.29077922, 0.83016364, 0.75210679, 0.22396813…\n$ Extra_Column_9          &lt;dbl&gt; 0.49719307, 0.63449028, 0.18150125, 0.32931387…\n$ Extra_Column_10         &lt;dbl&gt; 0.52199452, 0.30204337, 0.62918031, 0.14319054…\n$ Extra_Column_11         &lt;dbl&gt; 0.79965663, 0.04368285, 0.01827617, 0.90778075…\n$ Extra_Column_12         &lt;dbl&gt; 0.72239788, 0.45166789, 0.06322702, 0.54232201…\n$ Extra_Column_13         &lt;dbl&gt; 0.1487387, 0.8786714, 0.1465122, 0.9224606, 0.…\n$ Extra_Column_14         &lt;dbl&gt; 0.8340099, 0.5356022, 0.9972962, 0.6262165, 0.…\n$ Extra_Column_15         &lt;dbl&gt; 0.061632229, 0.617825340, 0.974455410, 0.22860…\n\n\n\n\n\n\nCode\nhead(heart_attack)\n\n\n# A tibble: 6 × 32\n    Age Gender Region Smoking_History Diabetes_History Hypertension_History\n  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;           &lt;chr&gt;            &lt;chr&gt;               \n1    56 Male   Urban  Yes             No               No                  \n2    69 Male   Urban  No              No               No                  \n3    46 Male   Rural  Yes             No               No                  \n4    32 Female Urban  No              No               No                  \n5    60 Female Rural  No              No               No                  \n6    25 Female Rural  No              No               No                  \n# ℹ 26 more variables: Cholesterol_Level &lt;dbl&gt;, Physical_Activity &lt;chr&gt;,\n#   Diet_Quality &lt;chr&gt;, Alcohol_Consumption &lt;chr&gt;, Stress_Levels &lt;dbl&gt;,\n#   BMI &lt;dbl&gt;, Heart_Rate &lt;dbl&gt;, Systolic_BP &lt;dbl&gt;, Diastolic_BP &lt;dbl&gt;,\n#   Family_History &lt;chr&gt;, Heart_Attack_Occurrence &lt;chr&gt;, Extra_Column_1 &lt;dbl&gt;,\n#   Extra_Column_2 &lt;dbl&gt;, Extra_Column_3 &lt;dbl&gt;, Extra_Column_4 &lt;dbl&gt;,\n#   Extra_Column_5 &lt;dbl&gt;, Extra_Column_6 &lt;dbl&gt;, Extra_Column_7 &lt;dbl&gt;,\n#   Extra_Column_8 &lt;dbl&gt;, Extra_Column_9 &lt;dbl&gt;, Extra_Column_10 &lt;dbl&gt;, …\n\n\n\n\n\n\nCode\nsummary(heart_attack)\n\n\n      Age           Gender             Region          Smoking_History   \n Min.   :18.00   Length:30000       Length:30000       Length:30000      \n 1st Qu.:33.00   Class :character   Class :character   Class :character  \n Median :48.00   Mode  :character   Mode  :character   Mode  :character  \n Mean   :48.49                                                           \n 3rd Qu.:64.00                                                           \n Max.   :79.00                                                           \n Diabetes_History   Hypertension_History Cholesterol_Level Physical_Activity \n Length:30000       Length:30000         Min.   : 80.02    Length:30000      \n Class :character   Class :character     1st Qu.:179.55    Class :character  \n Mode  :character   Mode  :character     Median :199.77    Mode  :character  \n                                         Mean   :199.90                      \n                                         3rd Qu.:220.16                      \n                                         Max.   :336.86                      \n Diet_Quality       Alcohol_Consumption Stress_Levels         BMI       \n Length:30000       Length:30000        Min.   : 0.000   Min.   : 5.58  \n Class :character   Class :character    1st Qu.: 3.644   1st Qu.:21.63  \n Mode  :character   Mode  :character    Median : 4.993   Median :24.96  \n                                        Mean   : 5.002   Mean   :25.00  \n                                        3rd Qu.: 6.353   3rd Qu.:28.36  \n                                        Max.   :10.000   Max.   :46.10  \n   Heart_Rate      Systolic_BP      Diastolic_BP    Family_History    \n Min.   : 30.03   Min.   : 56.23   Min.   : 39.95   Length:30000      \n 1st Qu.: 63.25   1st Qu.:109.79   1st Qu.: 73.26   Class :character  \n Median : 69.95   Median :119.90   Median : 80.12   Mode  :character  \n Mean   : 69.98   Mean   :119.91   Mean   : 80.03                     \n 3rd Qu.: 76.66   3rd Qu.:130.02   3rd Qu.: 86.76                     \n Max.   :108.78   Max.   :178.77   Max.   :117.67                     \n Heart_Attack_Occurrence Extra_Column_1      Extra_Column_2     \n Length:30000            Min.   :0.0000071   Min.   :0.0000052  \n Class :character        1st Qu.:0.2533084   1st Qu.:0.2473606  \n Mode  :character        Median :0.5008204   Median :0.4961980  \n                         Mean   :0.5019388   Mean   :0.4978940  \n                         3rd Qu.:0.7505286   3rd Qu.:0.7473954  \n                         Max.   :0.9999654   Max.   :0.9999894  \n Extra_Column_3      Extra_Column_4      Extra_Column_5     \n Min.   :0.0000227   Min.   :0.0000934   Min.   :0.0001051  \n 1st Qu.:0.2483093   1st Qu.:0.2522110   1st Qu.:0.2518029  \n Median :0.4976104   Median :0.4976175   Median :0.5019871  \n Mean   :0.4981949   Mean   :0.5005952   Mean   :0.5014100  \n 3rd Qu.:0.7476807   3rd Qu.:0.7505662   3rd Qu.:0.7536569  \n Max.   :0.9999694   Max.   :0.9999869   Max.   :0.9999949  \n Extra_Column_6      Extra_Column_7      Extra_Column_8     \n Min.   :0.0000531   Min.   :0.0000678   Min.   :0.0000449  \n 1st Qu.:0.2559989   1st Qu.:0.2482839   1st Qu.:0.2509790  \n Median :0.5017726   Median :0.4988157   Median :0.4985698  \n Mean   :0.5027631   Mean   :0.4980753   Mean   :0.5003557  \n 3rd Qu.:0.7511886   3rd Qu.:0.7456378   3rd Qu.:0.7507293  \n Max.   :0.9998892   Max.   :0.9999900   Max.   :0.9999300  \n Extra_Column_9      Extra_Column_10     Extra_Column_11    \n Min.   :0.0000305   Min.   :0.0000133   Min.   :0.0000008  \n 1st Qu.:0.2502452   1st Qu.:0.2484256   1st Qu.:0.2538092  \n Median :0.4984491   Median :0.5031040   Median :0.5067589  \n Mean   :0.5002292   Mean   :0.5010694   Mean   :0.5044949  \n 3rd Qu.:0.7512186   3rd Qu.:0.7522686   3rd Qu.:0.7556257  \n Max.   :0.9999852   Max.   :0.9999928   Max.   :0.9999578  \n Extra_Column_12     Extra_Column_13     Extra_Column_14    \n Min.   :0.0000713   Min.   :0.0000204   Min.   :0.0000025  \n 1st Qu.:0.2505341   1st Qu.:0.2473108   1st Qu.:0.2482152  \n Median :0.5038609   Median :0.5041162   Median :0.4943841  \n Mean   :0.5008624   Mean   :0.5004557   Mean   :0.4976507  \n 3rd Qu.:0.7511780   3rd Qu.:0.7497094   3rd Qu.:0.7456212  \n Max.   :0.9999484   Max.   :0.9999451   Max.   :0.9999779  \n Extra_Column_15    \n Min.   :0.0000241  \n 1st Qu.:0.2482573  \n Median :0.5009406  \n Mean   :0.4999634  \n 3rd Qu.:0.7487379  \n Max.   :0.9999913  \n\n\n\n\n\n\nCode\nheart_attack[duplicated(heart_attack),]\n\n\n# A tibble: 0 × 32\n# ℹ 32 variables: Age &lt;dbl&gt;, Gender &lt;chr&gt;, Region &lt;chr&gt;, Smoking_History &lt;chr&gt;,\n#   Diabetes_History &lt;chr&gt;, Hypertension_History &lt;chr&gt;,\n#   Cholesterol_Level &lt;dbl&gt;, Physical_Activity &lt;chr&gt;, Diet_Quality &lt;chr&gt;,\n#   Alcohol_Consumption &lt;chr&gt;, Stress_Levels &lt;dbl&gt;, BMI &lt;dbl&gt;,\n#   Heart_Rate &lt;dbl&gt;, Systolic_BP &lt;dbl&gt;, Diastolic_BP &lt;dbl&gt;,\n#   Family_History &lt;chr&gt;, Heart_Attack_Occurrence &lt;chr&gt;, Extra_Column_1 &lt;dbl&gt;,\n#   Extra_Column_2 &lt;dbl&gt;, Extra_Column_3 &lt;dbl&gt;, Extra_Column_4 &lt;dbl&gt;, …\n\n\n\n\n\n\n\n\nLimitations of pie charts in data visualization\n\n\n\n\nDistored perception: Pie chart makes comparisons difficult due to the reliance on angles and areas rather than a common baseline.\nDifficult to compare similar data slices: Can be misleading when there are many segments/ similar-sized portions making it hard to interpret differences accurately.\nSpace constraints: Pie chart can take up more space than necessary and can clutter dashboards or reports.\nPoor for trend analysis: Pie charts only show a single point in time and do not help in comparing trends over multiple years.\n\n\n\n\nRefer to this page to find out why the use of pie charts are discouraged.\n\n\n\n\n\n\n\n\nCode\ncolSums(is.na(heart_attack))\n\n\n                    Age                  Gender                  Region \n                      0                       0                       0 \n        Smoking_History        Diabetes_History    Hypertension_History \n                      0                       0                       0 \n      Cholesterol_Level       Physical_Activity            Diet_Quality \n                      0                       0                       0 \n    Alcohol_Consumption           Stress_Levels                     BMI \n                      0                       0                       0 \n             Heart_Rate             Systolic_BP            Diastolic_BP \n                      0                       0                       0 \n         Family_History Heart_Attack_Occurrence          Extra_Column_1 \n                      0                       0                       0 \n         Extra_Column_2          Extra_Column_3          Extra_Column_4 \n                      0                       0                       0 \n         Extra_Column_5          Extra_Column_6          Extra_Column_7 \n                      0                       0                       0 \n         Extra_Column_8          Extra_Column_9         Extra_Column_10 \n                      0                       0                       0 \n        Extra_Column_11         Extra_Column_12         Extra_Column_13 \n                      0                       0                       0 \n        Extra_Column_14         Extra_Column_15 \n                      0                       0 \n\n\n\n\n\n\n\n\nNote\n\n\n\n\nEnsure that there are no NA values, if not will have to investigate further.\nPossibility to use drop_na() function to drop rows where any specified column contains a missing value.\n\n\n\n\n\n\n\nCode\nspec(heart_attack)\n\n\ncols(\n  Age = col_double(),\n  Gender = col_character(),\n  Region = col_character(),\n  Smoking_History = col_character(),\n  Diabetes_History = col_character(),\n  Hypertension_History = col_character(),\n  Cholesterol_Level = col_double(),\n  Physical_Activity = col_character(),\n  Diet_Quality = col_character(),\n  Alcohol_Consumption = col_character(),\n  Stress_Levels = col_double(),\n  BMI = col_double(),\n  Heart_Rate = col_double(),\n  Systolic_BP = col_double(),\n  Diastolic_BP = col_double(),\n  Family_History = col_character(),\n  Heart_Attack_Occurrence = col_character(),\n  Extra_Column_1 = col_double(),\n  Extra_Column_2 = col_double(),\n  Extra_Column_3 = col_double(),\n  Extra_Column_4 = col_double(),\n  Extra_Column_5 = col_double(),\n  Extra_Column_6 = col_double(),\n  Extra_Column_7 = col_double(),\n  Extra_Column_8 = col_double(),\n  Extra_Column_9 = col_double(),\n  Extra_Column_10 = col_double(),\n  Extra_Column_11 = col_double(),\n  Extra_Column_12 = col_double(),\n  Extra_Column_13 = col_double(),\n  Extra_Column_14 = col_double(),\n  Extra_Column_15 = col_double()\n)\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nEnsure that all variables are correctly classified by data type; recast variable types if needed.\nVariables are correctly classified - where categorical variables are classified as character, while continuous variables are classified as double.\n\n\n\n\n\n\nThe heart_attack tibble contains 32 attributes, as shown above.\nThe following preprocessing checks were conducted as part of data preparation:\n\n\n\n\n\n\nPreprocessing Checks\n\n\n\n\nVerified that the correct data types were loaded in the heart_attack dataset using glimpse() and spec()\nEnsured there were no duplicate variable names using duplicated() in the dataset\nChecked for missing values using colSums(is.na())\n\n\n\n\n\n\nThe table below presents the metadata for the dataset, classifying each variable as either categorical or continuous based on its nature and data type.\n\nCategorical attributes:\n\n\n\n\n\n\n\n\nVariable Names\nType\nDescription\n\n\n\n\nGender\nCHAR\nBiological sex of the individual (e.g., Male, Female)\n\n\nRegion\nCHAR\nGeographic location where the individual resides (e.g., Urban, Rural)\n\n\nSmoking_History\nCHAR\nPast or current smoking habits (e.g., Yes, No)\n\n\nDiabetes_History\nCHAR\nHistory of diabetes diagnosis (Yes/No)\n\n\nHypertension_History\nCHAR\nHistory of high blood pressure diagnosis (Yes/No)\n\n\nPhysical_Activity\nCHAR\nLevel of physical activity (e.g., Low, Moderate, High)\n\n\nDiet_Quality\nCHAR\nDietary habits and nutritional intake assessment (e.g., Poor, Average, Good)\n\n\nAlcohol_Consumption\nCHAR\nFrequency or amount of alcohol intake (e.g., Low, Moderate, High, None)\n\n\nFamily_History\nCHAR\nPresence of heart disease in close relatives (Yes/No)\n\n\nHeart_Attack_Occurrence\nCHAR\nWhether the individual has experienced a heart attack (Yes/No)\n\n\n\nContinuous attributes:\n\n\n\n\n\n\n\n\nVariable Names\nType\nDescription\n\n\n\n\nAge\nNUM\nAge of the individual in years\n\n\nCholesterol_Level\nNUM\nMeasured cholesterol level in the blood\n\n\nStress_Levels\nNUM\nMeasured or self-reported stress level\n\n\nBMI\nNUM\nBody Mass Index, calculated from height and weight\n\n\nHeart_Rate\nNUM\nResting heart rate in beats per minute (bpm)\n\n\nSystolic_BP\nNUM\nSystolic blood pressure measurement (mmHg)\n\n\nDiastolic_BP\nNUM\nDiastolic blood pressure measurement (mmHg)\n\n\nExtra_Column_1\nNUM\n\n\n\nExtra_Column_2\nNUM\n\n\n\nExtra_Column_3\nNUM\n\n\n\nExtra_Column_4\nNUM\n\n\n\nExtra_Column_5\nNUM\n\n\n\nExtra_Column_6\nNUM\n\n\n\nExtra_Column_7\nNUM\n\n\n\nExtra_Column_8\nNUM\n\n\n\nExtra_Column_9\nNUM\n\n\n\nExtra_Column_10\nNUM\n\n\n\nExtra_Column_11\nNUM\n\n\n\nExtra_Column_12\nNUM\n\n\n\nExtra_Column_13\nNUM\n\n\n\nExtra_Column_14\nNUM\n\n\n\nExtra_Column_15\nNUM\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nNotice that Extra_Column_1 to Extra_Column_15 are additional numeric figures without clear definition.\nThey are most likely derived calculations from the variables.\n\n\n\n\n\n\nOf the 32 variables (columns), only 17 variables(columns) are selected for analysis\n\nAll columns are selected except for Extra_Column_1 to Extra_Column_15\n\nThe select() function in the dplyr package is used to obtain these rows, and stored as the R object, heart_attack_1.\n\n\nCode\nheart_attack_1 &lt;- heart_attack %&gt;% \n  select(\"Age\", \"Gender\", \"Region\", \"Smoking_History\", \"Diabetes_History\", \n         \"Hypertension_History\", \"Cholesterol_Level\", \"Physical_Activity\", \n         \"Diet_Quality\", \"Alcohol_Consumption\", \"Stress_Levels\", \"BMI\", \n         \"Heart_Rate\", \"Systolic_BP\", \"Diastolic_BP\", \"Family_History\", \n         \"Heart_Attack_Occurrence\")\n\nheart_attack_1\n\n\n# A tibble: 30,000 × 17\n     Age Gender Region Smoking_History Diabetes_History Hypertension_History\n   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;           &lt;chr&gt;            &lt;chr&gt;               \n 1    56 Male   Urban  Yes             No               No                  \n 2    69 Male   Urban  No              No               No                  \n 3    46 Male   Rural  Yes             No               No                  \n 4    32 Female Urban  No              No               No                  \n 5    60 Female Rural  No              No               No                  \n 6    25 Female Rural  No              No               No                  \n 7    78 Male   Urban  No              Yes              Yes                 \n 8    38 Female Urban  Yes             No               No                  \n 9    56 Male   Rural  No              No               Yes                 \n10    75 Male   Urban  No              No               No                  \n# ℹ 29,990 more rows\n# ℹ 11 more variables: Cholesterol_Level &lt;dbl&gt;, Physical_Activity &lt;chr&gt;,\n#   Diet_Quality &lt;chr&gt;, Alcohol_Consumption &lt;chr&gt;, Stress_Levels &lt;dbl&gt;,\n#   BMI &lt;dbl&gt;, Heart_Rate &lt;dbl&gt;, Systolic_BP &lt;dbl&gt;, Diastolic_BP &lt;dbl&gt;,\n#   Family_History &lt;chr&gt;, Heart_Attack_Occurrence &lt;chr&gt;\n\n\nThe output shows a tibble dataframe with 30,000 rows and 17 columns.\n\n\n\n\n\nIn the following section, we will recode specific continuous variables into categorical groups for better interpretability.\nWhile variables like Cholesterol_Level, Stress_Levels, BMI, Heart_Rate, Systolic_BP, and Diastolic_BP provide valuable insights in their continuous form, categorizing them into meaningful groups can enhance our ability to analyze trends and risk factors more effectively.\n\n\n\n\n\n\n\n\nVariable Names\nMeasurement\nCategorical ranges Approximated classification\n\n\n\n\nCholesterol_Level\nmg/dL\n\nLow: ≤ 150\nModerate: 151–200\nHigh: &gt; 200\n\n\n\nStress_Levels\nSelf-reported scale\n\nMiniminal_Stress: 0\nLow_Stress: 1–3\nModerate_Stress: 4–7\nHigh_Stress: 8–10\n\n\n\nBMI\nBody Mass Index\n\nUnderweight: &lt; 18.5\nNormal_Weight: 18.5–24.9\nOverweight: 25–29.9\nObese: ≥ 30\n\n\n\nHeart_Rate\nbeats per minute\n\nLow: &lt; 60\nNormal: 60–100\nHigh: &gt; 100\n\n\n\nSystolic_BP\nmmHg\n\nNormal: &lt; 120\nElevated: 120–129\nHypertension_Stage_1: 130–139\nHypertension_Stage_2: ≥ 140\n\n\n\nDiastolic_BP\nmmHg\n\nNormal: &lt; 80\nElevated: 80–89\nHypertension_Stage_1: 90–99\nHypertension_Stage_2: ≥ 100\n\n\n\n\n\n\nCode\nlibrary(dplyr)\n\nheart_attack_1 &lt;- heart_attack %&gt;%\n  select(\"Age\", \"Gender\", \"Region\", \"Smoking_History\", \"Diabetes_History\", \n         \"Hypertension_History\", \"Cholesterol_Level\", \"Physical_Activity\", \n         \"Diet_Quality\", \"Alcohol_Consumption\", \"Stress_Levels\", \"BMI\", \n         \"Heart_Rate\", \"Systolic_BP\", \"Diastolic_BP\", \"Family_History\", \n         \"Heart_Attack_Occurrence\") %&gt;%\n  mutate(\n    Cholesterol_Level_Category = case_when(\n      Cholesterol_Level &lt;= 150 ~ \"Low\",\n      Cholesterol_Level &lt;= 200 ~ \"Moderate\",\n      TRUE ~ \"High\"\n    ),\n    \n    # **ROUND Stress_Levels before categorization**\n    Rounded_Stress_Levels = round(Stress_Levels), \n    \n    Stress_Levels_Category = case_when(\n      is.na(Rounded_Stress_Levels) ~ \"Unknown\",  # Handle missing values\n      Rounded_Stress_Levels == 0 ~ \"Miniminal_Stress\",\n      Rounded_Stress_Levels %in% 1:3 ~ \"Low_Stress\",\n      Rounded_Stress_Levels %in% 4:7 ~ \"Moderate_Stress\",\n      Rounded_Stress_Levels %in% 8:10 ~ \"High_Stress\",\n      TRUE ~ \"Unknown\"\n    ),\n    \n    BMI_Category = case_when(\n      BMI &lt; 18.5 ~ \"Underweight\",\n      BMI &gt;= 18.5 & BMI &lt; 25 ~ \"Normal_Weight\",\n      BMI &gt;= 25 & BMI &lt; 30 ~ \"Overweight\",\n      BMI &gt;= 30 ~ \"Obese\",\n      TRUE ~ \"Unknown\"\n    ),\n    \n    Heart_Rate_Category = case_when(\n      Heart_Rate &lt; 60 ~ \"Low\",\n      Heart_Rate &gt;= 60 & Heart_Rate &lt;= 100 ~ \"Normal\",\n      Heart_Rate &gt; 100 ~ \"High\",\n      TRUE ~ \"Unknown\"\n    ),\n    \n    Systolic_BP_Category = case_when(\n      Systolic_BP &lt; 120 ~ \"Normal\",\n      Systolic_BP &gt;= 120 & Systolic_BP &lt; 130 ~ \"Elevated\",\n      Systolic_BP &gt;= 130 & Systolic_BP &lt; 140 ~ \"Hypertension_Stage_1\",\n      Systolic_BP &gt;= 140 ~ \"Hypertension_Stage_2\",\n      TRUE ~ \"Unknown\"\n    ),\n    \n    Diastolic_BP_Category = case_when(\n      Diastolic_BP &lt; 80 ~ \"Normal\",\n      Diastolic_BP &gt;= 80 & Diastolic_BP &lt; 90 ~ \"Elevated\",\n      Diastolic_BP &gt;= 90 & Diastolic_BP &lt; 100 ~ \"Hypertension_Stage_1\",\n      Diastolic_BP &gt;= 100 ~ \"Hypertension_Stage_2\",\n      TRUE ~ \"Unknown\"\n    )\n  )\n\n# View the modified dataframe with new categorical variables\nheart_attack_1\n\n\n# A tibble: 30,000 × 24\n     Age Gender Region Smoking_History Diabetes_History Hypertension_History\n   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;           &lt;chr&gt;            &lt;chr&gt;               \n 1    56 Male   Urban  Yes             No               No                  \n 2    69 Male   Urban  No              No               No                  \n 3    46 Male   Rural  Yes             No               No                  \n 4    32 Female Urban  No              No               No                  \n 5    60 Female Rural  No              No               No                  \n 6    25 Female Rural  No              No               No                  \n 7    78 Male   Urban  No              Yes              Yes                 \n 8    38 Female Urban  Yes             No               No                  \n 9    56 Male   Rural  No              No               Yes                 \n10    75 Male   Urban  No              No               No                  \n# ℹ 29,990 more rows\n# ℹ 18 more variables: Cholesterol_Level &lt;dbl&gt;, Physical_Activity &lt;chr&gt;,\n#   Diet_Quality &lt;chr&gt;, Alcohol_Consumption &lt;chr&gt;, Stress_Levels &lt;dbl&gt;,\n#   BMI &lt;dbl&gt;, Heart_Rate &lt;dbl&gt;, Systolic_BP &lt;dbl&gt;, Diastolic_BP &lt;dbl&gt;,\n#   Family_History &lt;chr&gt;, Heart_Attack_Occurrence &lt;chr&gt;,\n#   Cholesterol_Level_Category &lt;chr&gt;, Rounded_Stress_Levels &lt;dbl&gt;,\n#   Stress_Levels_Category &lt;chr&gt;, BMI_Category &lt;chr&gt;, …\n\n\n\n\n\nWhile analyzing age as a continuous variable provides detailed insights, categorizing the Age variable into distinct age groups allows us to explore how age influences the likelihood of heart attack incidents.\nThe table below are the proposed age categories for this analysis:\n\n\n\nCategories\nAge\n\n\n\n\nYouth\n≤25 years\n\n\nYoung adults\n26–40 years\n\n\nMiddle-aged adults\n41–55 years\n\n\nOlder adults\n56–70 years\n\n\nElderly\n≥71 years\n\n\n\n\n\nCode\nheart_attack_2 &lt;- heart_attack_1 %&gt;%\n  mutate(\n    Age_Category = case_when(\n      Age &lt;= 25 ~ \"Youth\",\n      Age &gt;= 26 & Age &lt;= 40 ~ \"Young Adult\",\n      Age &gt;= 41 & Age &lt;= 55 ~ \"Middle-Aged Adult\",\n      Age &gt;= 56 & Age &lt;= 70 ~ \"Older Adult\",\n      Age &gt;= 71 ~ \"Elderly\",\n      TRUE ~ \"Unknown\"\n    )\n  )\n\n# View the updated dataset\nheart_attack_2\n\n\n# A tibble: 30,000 × 25\n     Age Gender Region Smoking_History Diabetes_History Hypertension_History\n   &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;           &lt;chr&gt;            &lt;chr&gt;               \n 1    56 Male   Urban  Yes             No               No                  \n 2    69 Male   Urban  No              No               No                  \n 3    46 Male   Rural  Yes             No               No                  \n 4    32 Female Urban  No              No               No                  \n 5    60 Female Rural  No              No               No                  \n 6    25 Female Rural  No              No               No                  \n 7    78 Male   Urban  No              Yes              Yes                 \n 8    38 Female Urban  Yes             No               No                  \n 9    56 Male   Rural  No              No               Yes                 \n10    75 Male   Urban  No              No               No                  \n# ℹ 29,990 more rows\n# ℹ 19 more variables: Cholesterol_Level &lt;dbl&gt;, Physical_Activity &lt;chr&gt;,\n#   Diet_Quality &lt;chr&gt;, Alcohol_Consumption &lt;chr&gt;, Stress_Levels &lt;dbl&gt;,\n#   BMI &lt;dbl&gt;, Heart_Rate &lt;dbl&gt;, Systolic_BP &lt;dbl&gt;, Diastolic_BP &lt;dbl&gt;,\n#   Family_History &lt;chr&gt;, Heart_Attack_Occurrence &lt;chr&gt;,\n#   Cholesterol_Level_Category &lt;chr&gt;, Rounded_Stress_Levels &lt;dbl&gt;,\n#   Stress_Levels_Category &lt;chr&gt;, BMI_Category &lt;chr&gt;, …\n\n\n\n\n\nWe will re-check the dataset after filtering and recoding of continuous variables using below:\n\nglimpse()head()summary()duplicated()colSum(is.na())spec())\n\n\n\n\nCode\nglimpse(heart_attack_2)\n\n\nRows: 30,000\nColumns: 25\n$ Age                        &lt;dbl&gt; 56, 69, 46, 32, 60, 25, 78, 38, 56, 75, 36,…\n$ Gender                     &lt;chr&gt; \"Male\", \"Male\", \"Male\", \"Female\", \"Female\",…\n$ Region                     &lt;chr&gt; \"Urban\", \"Urban\", \"Rural\", \"Urban\", \"Rural\"…\n$ Smoking_History            &lt;chr&gt; \"Yes\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"No\",…\n$ Diabetes_History           &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", …\n$ Hypertension_History       &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", …\n$ Cholesterol_Level          &lt;dbl&gt; 186.4002, 185.1367, 210.6966, 211.1655, 223…\n$ Physical_Activity          &lt;chr&gt; \"Moderate\", \"Low\", \"Low\", \"Moderate\", \"High…\n$ Diet_Quality               &lt;chr&gt; \"Poor\", \"Good\", \"Average\", \"Good\", \"Good\", …\n$ Alcohol_Consumption        &lt;chr&gt; \"Low\", \"Low\", \"Moderate\", \"High\", \"High\", \"…\n$ Stress_Levels              &lt;dbl&gt; 3.644786, 3.384056, 3.810911, 6.014878, 6.8…\n$ BMI                        &lt;dbl&gt; 33.96135, 28.24287, 27.60121, 23.71729, 19.…\n$ Heart_Rate                 &lt;dbl&gt; 72.30153, 57.45764, 64.65870, 55.13147, 76.…\n$ Systolic_BP                &lt;dbl&gt; 123.90209, 129.89331, 145.65490, 131.78522,…\n$ Diastolic_BP               &lt;dbl&gt; 85.68281, 73.52426, 71.99481, 68.21133, 92.…\n$ Family_History             &lt;chr&gt; \"No\", \"Yes\", \"No\", \"No\", \"No\", \"No\", \"No\", …\n$ Heart_Attack_Occurrence    &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"…\n$ Cholesterol_Level_Category &lt;chr&gt; \"Moderate\", \"Moderate\", \"High\", \"High\", \"Hi…\n$ Rounded_Stress_Levels      &lt;dbl&gt; 4, 3, 4, 6, 7, 8, 5, 5, 7, 4, 3, 2, 2, 3, 8…\n$ Stress_Levels_Category     &lt;chr&gt; \"Moderate_Stress\", \"Low_Stress\", \"Moderate_…\n$ BMI_Category               &lt;chr&gt; \"Obese\", \"Overweight\", \"Overweight\", \"Norma…\n$ Heart_Rate_Category        &lt;chr&gt; \"Normal\", \"Low\", \"Normal\", \"Low\", \"Normal\",…\n$ Systolic_BP_Category       &lt;chr&gt; \"Elevated\", \"Elevated\", \"Hypertension_Stage…\n$ Diastolic_BP_Category      &lt;chr&gt; \"Elevated\", \"Normal\", \"Normal\", \"Normal\", \"…\n$ Age_Category               &lt;chr&gt; \"Older Adult\", \"Older Adult\", \"Middle-Aged …\n\n\n\n\n\n\nCode\nhead(heart_attack_2)\n\n\n# A tibble: 6 × 25\n    Age Gender Region Smoking_History Diabetes_History Hypertension_History\n  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;           &lt;chr&gt;            &lt;chr&gt;               \n1    56 Male   Urban  Yes             No               No                  \n2    69 Male   Urban  No              No               No                  \n3    46 Male   Rural  Yes             No               No                  \n4    32 Female Urban  No              No               No                  \n5    60 Female Rural  No              No               No                  \n6    25 Female Rural  No              No               No                  \n# ℹ 19 more variables: Cholesterol_Level &lt;dbl&gt;, Physical_Activity &lt;chr&gt;,\n#   Diet_Quality &lt;chr&gt;, Alcohol_Consumption &lt;chr&gt;, Stress_Levels &lt;dbl&gt;,\n#   BMI &lt;dbl&gt;, Heart_Rate &lt;dbl&gt;, Systolic_BP &lt;dbl&gt;, Diastolic_BP &lt;dbl&gt;,\n#   Family_History &lt;chr&gt;, Heart_Attack_Occurrence &lt;chr&gt;,\n#   Cholesterol_Level_Category &lt;chr&gt;, Rounded_Stress_Levels &lt;dbl&gt;,\n#   Stress_Levels_Category &lt;chr&gt;, BMI_Category &lt;chr&gt;,\n#   Heart_Rate_Category &lt;chr&gt;, Systolic_BP_Category &lt;chr&gt;, …\n\n\n\n\n\n\nCode\nsummary(heart_attack_2)\n\n\n      Age           Gender             Region          Smoking_History   \n Min.   :18.00   Length:30000       Length:30000       Length:30000      \n 1st Qu.:33.00   Class :character   Class :character   Class :character  \n Median :48.00   Mode  :character   Mode  :character   Mode  :character  \n Mean   :48.49                                                           \n 3rd Qu.:64.00                                                           \n Max.   :79.00                                                           \n Diabetes_History   Hypertension_History Cholesterol_Level Physical_Activity \n Length:30000       Length:30000         Min.   : 80.02    Length:30000      \n Class :character   Class :character     1st Qu.:179.55    Class :character  \n Mode  :character   Mode  :character     Median :199.77    Mode  :character  \n                                         Mean   :199.90                      \n                                         3rd Qu.:220.16                      \n                                         Max.   :336.86                      \n Diet_Quality       Alcohol_Consumption Stress_Levels         BMI       \n Length:30000       Length:30000        Min.   : 0.000   Min.   : 5.58  \n Class :character   Class :character    1st Qu.: 3.644   1st Qu.:21.63  \n Mode  :character   Mode  :character    Median : 4.993   Median :24.96  \n                                        Mean   : 5.002   Mean   :25.00  \n                                        3rd Qu.: 6.353   3rd Qu.:28.36  \n                                        Max.   :10.000   Max.   :46.10  \n   Heart_Rate      Systolic_BP      Diastolic_BP    Family_History    \n Min.   : 30.03   Min.   : 56.23   Min.   : 39.95   Length:30000      \n 1st Qu.: 63.25   1st Qu.:109.79   1st Qu.: 73.26   Class :character  \n Median : 69.95   Median :119.90   Median : 80.12   Mode  :character  \n Mean   : 69.98   Mean   :119.91   Mean   : 80.03                     \n 3rd Qu.: 76.66   3rd Qu.:130.02   3rd Qu.: 86.76                     \n Max.   :108.78   Max.   :178.77   Max.   :117.67                     \n Heart_Attack_Occurrence Cholesterol_Level_Category Rounded_Stress_Levels\n Length:30000            Length:30000               Min.   : 0.000       \n Class :character        Class :character           1st Qu.: 4.000       \n Mode  :character        Mode  :character           Median : 5.000       \n                                                    Mean   : 4.998       \n                                                    3rd Qu.: 6.000       \n                                                    Max.   :10.000       \n Stress_Levels_Category BMI_Category       Heart_Rate_Category\n Length:30000           Length:30000       Length:30000       \n Class :character       Class :character   Class :character   \n Mode  :character       Mode  :character   Mode  :character   \n                                                              \n                                                              \n                                                              \n Systolic_BP_Category Diastolic_BP_Category Age_Category      \n Length:30000         Length:30000          Length:30000      \n Class :character     Class :character      Class :character  \n Mode  :character     Mode  :character      Mode  :character  \n                                                              \n                                                              \n                                                              \n\n\n\n\n\n\nCode\nheart_attack_2[duplicated(heart_attack_2),]\n\n\n# A tibble: 0 × 25\n# ℹ 25 variables: Age &lt;dbl&gt;, Gender &lt;chr&gt;, Region &lt;chr&gt;, Smoking_History &lt;chr&gt;,\n#   Diabetes_History &lt;chr&gt;, Hypertension_History &lt;chr&gt;,\n#   Cholesterol_Level &lt;dbl&gt;, Physical_Activity &lt;chr&gt;, Diet_Quality &lt;chr&gt;,\n#   Alcohol_Consumption &lt;chr&gt;, Stress_Levels &lt;dbl&gt;, BMI &lt;dbl&gt;,\n#   Heart_Rate &lt;dbl&gt;, Systolic_BP &lt;dbl&gt;, Diastolic_BP &lt;dbl&gt;,\n#   Family_History &lt;chr&gt;, Heart_Attack_Occurrence &lt;chr&gt;,\n#   Cholesterol_Level_Category &lt;chr&gt;, Rounded_Stress_Levels &lt;dbl&gt;, …\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nEnsure that there are no duplicated columns, if not will have to investigate further.\n\n\n\n\n\n\n\nCode\ncolSums(is.na(heart_attack_2))\n\n\n                       Age                     Gender \n                         0                          0 \n                    Region            Smoking_History \n                         0                          0 \n          Diabetes_History       Hypertension_History \n                         0                          0 \n         Cholesterol_Level          Physical_Activity \n                         0                          0 \n              Diet_Quality        Alcohol_Consumption \n                         0                          0 \n             Stress_Levels                        BMI \n                         0                          0 \n                Heart_Rate                Systolic_BP \n                         0                          0 \n              Diastolic_BP             Family_History \n                         0                          0 \n   Heart_Attack_Occurrence Cholesterol_Level_Category \n                         0                          0 \n     Rounded_Stress_Levels     Stress_Levels_Category \n                         0                          0 \n              BMI_Category        Heart_Rate_Category \n                         0                          0 \n      Systolic_BP_Category      Diastolic_BP_Category \n                         0                          0 \n              Age_Category \n                         0 \n\n\n\n\n\n\n\n\nNote\n\n\n\n\nEnsure that there are no NA values, if not will have to investigate further.\nPossibility to use drop_na() function to drop rows where any specified column contains a missing value.\n\n\n\n\n\n\n\nCode\nspec(heart_attack_2)\n\n\nNULL\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nEnsure that all variables are correctly classified by data type; recast variable types if needed.\nVariables are correctly classified - where categorical variables are classified as character, while continuous variables are classified as double.\n\n\n\n\n\n\n\n\n\n\n\n\nPreprocessing Checks\n\n\n\n\nVerified that the correct data types were loaded in the heart_attack dataset using glimpse() and spec()\nEnsured there were no duplicate variable names using duplicated() in the dataset\nChecked for missing values using colSums(is.na())\n\n\n\nThe final output - heart_attack_2 shows a tibble dataframe with 30,000 rows and 17 columns."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04.html",
    "href": "In-class_Ex/In-class_Ex04.html",
    "title": "In-class Exercise 04",
    "section": "",
    "text": "Code\npacman::p_load(haven, SmartEDA,tidyverse, tidymodels,ggridges, colorspace)\n\n\n\n\n\n\nCode\nexam &lt;- read_csv(\"data/Exam_data.csv\", show_col_types = FALSE)\n\n\n\n\n\n\n\n\nCode\nggplot(data = exam,\n       aes(x = ENGLISH,\n           y = CLASS)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(data = exam, aes(x = ENGLISH, y = CLASS)) +\n  geom_boxplot(fill = \"#7097BB\", color = \"black\", alpha = 0.6, outlier.shape = NA) +  # Boxplot with color\n  geom_jitter(aes(color = CLASS), width = 0.2, alpha = 0.7, size = 2) +  # Add jittered points\n  scale_color_brewer(palette = \"Dark2\") +  # Use Brewer palette for distinct colors\n  labs(\n    title = \"Distribution of English Grades by Class\",\n    x = \"English Grades\",\n    y = \"Class\"\n  ) +\n  theme_minimal() +  # Modern theme\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),  # Centered title\n    legend.position = \"none\"  # Hide legend if not needed\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(data = exam, aes(x = ENGLISH, y = CLASS)) +\n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    bandwidth = 3.4,\n    fill = lighten(\"#7097BB\", .3),\n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(\n    name = NULL, \n    expand = expansion(add = c(0.2, 2.6))\n  ) +\n  theme_ridges()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex04.html#getting-started",
    "href": "In-class_Ex/In-class_Ex04.html#getting-started",
    "title": "In-class Exercise 04",
    "section": "",
    "text": "Code\npacman::p_load(haven, SmartEDA,tidyverse, tidymodels,ggridges, colorspace)\n\n\n\n\n\n\nCode\nexam &lt;- read_csv(\"data/Exam_data.csv\", show_col_types = FALSE)\n\n\n\n\n\n\n\n\nCode\nggplot(data = exam,\n       aes(x = ENGLISH,\n           y = CLASS)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(data = exam, aes(x = ENGLISH, y = CLASS)) +\n  geom_boxplot(fill = \"#7097BB\", color = \"black\", alpha = 0.6, outlier.shape = NA) +  # Boxplot with color\n  geom_jitter(aes(color = CLASS), width = 0.2, alpha = 0.7, size = 2) +  # Add jittered points\n  scale_color_brewer(palette = \"Dark2\") +  # Use Brewer palette for distinct colors\n  labs(\n    title = \"Distribution of English Grades by Class\",\n    x = \"English Grades\",\n    y = \"Class\"\n  ) +\n  theme_minimal() +  # Modern theme\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),  # Centered title\n    legend.position = \"none\"  # Hide legend if not needed\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(data = exam, aes(x = ENGLISH, y = CLASS)) +\n  geom_density_ridges(\n    scale = 3,\n    rel_min_height = 0.01,\n    bandwidth = 3.4,\n    fill = lighten(\"#7097BB\", .3),\n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    name = \"English grades\",\n    expand = c(0, 0)\n  ) +\n  scale_y_discrete(\n    name = NULL, \n    expand = expansion(add = c(0.2, 2.6))\n  ) +\n  theme_ridges()"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05a.html",
    "href": "Hands-on_Ex/Hands-on_Ex05a.html",
    "title": "Hands-on Exercise 05a",
    "section": "",
    "text": "With the assistance of ChatGPT"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05a.html#creating-ternary-plot-with-r",
    "href": "Hands-on_Ex/Hands-on_Ex05a.html#creating-ternary-plot-with-r",
    "title": "Hands-on Exercise 05a",
    "section": "5. Creating Ternary Plot with R",
    "text": "5. Creating Ternary Plot with R\n\n5.1 Overview\nTernary plots visualize three-part compositional data, such as population age groups or soil composition. Displayed as a triangle with sides scaled from 0 to 1, each side represents a component. Points are plotted based on their proportions, with perpendicular lines indicating their values.\nIn this hands-on exercise, we will learn how to create a ternary plot in R to visualize and analyze Singapore’s population structure. The steps include:\n\nInstalling and loading tidyverse and ggtern packages.\nUsing mutate() from dplyr to derive three new measures.\nCreating a static ternary plot with ggtern().\nBuilding an interactive ternary plot using plot_ly() from Plotly.\n\n\n\n5.2 Installing and launching R packages\nFor this exercise, we will use two primary R packages:\n\nggtern – A ggplot extension designed specifically for creating ternary diagrams (static ternary plots).\nPlotly R – A package that enables interactive web-based graphs, using the ggplotly function to convert ggplot2 figures into interactive Plotly objects.\n\nAdditionally, we will install and load key tidyverse packages, including readr, dplyr, and tidyr, for data handling and manipulation.\nSince the current ggtern package is incompatible with the latest ggplot2 version, we will install ggplot2 version 3.2.1 instead.\n\n\nCode\npacman::p_load(plotly, ggtern, tidyverse)\n\n\n\n\n5.3 Data preparation\n\n5.3.1 The data\nFor this hands-on exercise, we will use the Singapore Residents by Planning Area/Subzone, Age Group, Sex, and Type of Dwelling (June 2000-2018) dataset. The dataset, named “respopagsex2000to2018_tidy.csv”, has already been downloaded and is stored in the data sub-folder within the exercise directory. It is in CSV format.\n\n\n5.3.2 Importing data\nTo import “respopagsex2000to2018_tidy.csv” into R, we will use the read_csv() function from the readr package.\n\n\nCode\n#Reading the data into R environment\npop_data &lt;- read_csv(\"data/respopagsex2000to2018_tidy.csv\") \n\n\n\n\n5.3.3 Preparing the data\nNext, use the mutate() function from the dplyr package to create three new measures: young, active, and old.\n\n\nCode\n#Deriving the young, economy active and old measures\nagpop_mutated &lt;- pop_data %&gt;%\n  mutate(`Year` = as.character(Year))%&gt;%\n  spread(AG, Population) %&gt;%\n  mutate(YOUNG = rowSums(.[4:8]))%&gt;%\n  mutate(ACTIVE = rowSums(.[9:16]))  %&gt;%\n  mutate(OLD = rowSums(.[17:21])) %&gt;%\n  mutate(TOTAL = rowSums(.[22:24])) %&gt;%\n  filter(Year == 2018)%&gt;%\n  filter(TOTAL &gt; 0)\n\n\n\n\n\n5.4 Data preparation\n\n5.4.1 Plotting a static ternary diagram\nUse the ggtern() function from the ggtern package to create a basic ternary plot.\n\n\nCode\n#Building the static ternary plot\nggtern(data=agpop_mutated,aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\nCode\n#Building the static ternary plot\nggtern(data=agpop_mutated, aes(x=YOUNG,y=ACTIVE, z=OLD)) +\n  geom_point() +\n  labs(title=\"Population structure, 2015\") +\n  theme_rgbw()\n\n\n\n\n\n\n\n\n\n\n\n5.4.2 Plotting an interactive ternary diagram\nThe code below uses the plot_ly() function from the Plotly R package to create an interactive ternary plot\n\n\nCode\n# reusable function for creating annotation object\nlabel &lt;- function(txt) {\n  list(\n    text = txt, \n    x = 0.1, y = 1,\n    ax = 0, ay = 0,\n    xref = \"paper\", yref = \"paper\", \n    align = \"center\",\n    font = list(family = \"serif\", size = 15, color = \"white\"),\n    bgcolor = \"#b3b3b3\", bordercolor = \"black\", borderwidth = 2\n  )\n}\n\n# reusable function for axis formatting\naxis &lt;- function(txt) {\n  list(\n    title = txt, tickformat = \".0%\", tickfont = list(size = 10)\n  )\n}\n\nternaryAxes &lt;- list(\n  aaxis = axis(\"Young\"), \n  baxis = axis(\"Active\"), \n  caxis = axis(\"Old\")\n)\n\n# Initiating a plotly visualization \nplot_ly(\n  agpop_mutated, \n  a = ~YOUNG, \n  b = ~ACTIVE, \n  c = ~OLD, \n  color = I(\"black\"), \n  type = \"scatterternary\"\n) %&gt;%\n  layout(\n    annotations = label(\"Ternary Markers\"), \n    ternary = ternaryAxes\n  )\n\n\n\n\n\n\n\n\n\n5.5 References\n\nKam, T.S(2024). Visual Statistical Analysis.\n\n\n\n5.6 Takeaway\n\n\n\n\n\n\nKey takeaways\n\n\n\n\nTernary plots are powerful for visualizing three-variable compositional data.\nggtern makes it easy to create and customize ternary plots in R.\nPlotly enhances interactivity, making ternary plots more intuitive for exploration.\n\n\n\n\n\n5.7 Further exploration\n\nTo explore ternary plot by planning area (PA)\n\nQuestion: Do different planning areas (PA) have distinct age distributions?\nObservations:\n\nAll regions have a similar age distribution, with no extreme differences.\nThe working-age (“Active”) population is the largest group across all regions.\n\n\nGraphCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Categorize Planning Areas into broader Regions\nregion_mapping &lt;- c(\n  # **North Region**\n  \"Woodlands\" = \"North\", \"Sembawang\" = \"North\", \"Yishun\" = \"North\", \n  \"Mandai\" = \"North\", \"Simpang\" = \"North\", \"Seletar\" = \"North\", \"Central Water Catchment\" = \"North\",\n\n  # **West Region**\n  \"Bukit Batok\" = \"West\", \"Bukit Panjang\" = \"West\", \"Choa Chu Kang\" = \"West\", \n  \"Clementi\" = \"West\", \"Jurong East\" = \"West\", \"Jurong West\" = \"West\", \n  \"Tengah\" = \"West\", \"Tuas\" = \"West\", \"Pioneer\" = \"West\", \n  \"Western Islands\" = \"West\", \"Western Water Catchment\" = \"West\",\n  \"Boon Lay\" = \"West\", \"Boon Lay/Pioneer\" = \"West\", \"Sungei Kadut\" = \"West\", \"Lim Chu Kang\" = \"West\",\n\n  # **East Region**\n  \"Bedok\" = \"East\", \"Changi\" = \"East\", \"Pasir Ris\" = \"East\", \"Tampines\" = \"East\", \n  \"Paya Lebar\" = \"East\", \"Changi Bay\" = \"East\",\n\n  # **South Region**\n  \"Bukit Merah\" = \"South\", \"Queenstown\" = \"South\", \"Sentosa\" = \"South\", \n  \"Marina East\" = \"South\", \"Marina South\" = \"South\", \"Southern Islands\" = \"South\",\n  \"Singapore River\" = \"South\", \"Straits View\" = \"South\", \"Outram\" = \"South\",\n\n  # **Central Region**\n  \"Ang Mo Kio\" = \"Central\", \"Bishan\" = \"Central\", \"Toa Payoh\" = \"Central\", \n  \"Kallang\" = \"Central\", \"Marine Parade\" = \"Central\", \"Downtown Core\" = \"Central\",\n  \"Geylang\" = \"Central\", \"Hougang\" = \"Central\", \"Novena\" = \"Central\", \"Newton\" = \"Central\",\n  \"Orchard\" = \"Central\", \"River Valley\" = \"Central\", \"Rochor\" = \"Central\", \n  \"Tanglin\" = \"Central\", \"Serangoon\" = \"Central\", \"Sengkang\" = \"Central\", \n  \"Punggol\" = \"Central\", \"Museum\" = \"Central\", \"North-Eastern Islands\" = \"Central\",\n  \"Bukit Timah\" = \"Central\",  # **Added Missing Bukit Timah**\n  \n  # **Other / Unclassified**\n  \"Not Stated\" = \"Other\"  # **Added Missing \"Not Stated\"**\n)\n\n# **Step 1: Assign Regions based on Planning Areas (PA)**\npop_data &lt;- pop_data %&gt;%\n  mutate(Region = recode(PA, !!!region_mapping, .default = \"Other\"))  # Assign regions\n\n# Ensure the Age_Group column is properly categorized\npop_data &lt;- pop_data %&gt;%\n  mutate(Age_Group = case_when(\n    AG %in% c(\"AGE0-4\", \"AGE5-9\", \"AGE10-14\", \"AGE15-19\", \"AGE20-24\") ~ \"Young\",\n    AG %in% c(\"AGE25-29\", \"AGE30-34\", \"AGE35-39\", \"AGE40-44\", \"AGE45-49\", \"AGE50-54\") ~ \"Active\",\n    AG %in% c(\"AGE55-59\", \"AGE60-64\", \"AGE65-69\", \"AGE70-74\", \"AGE75-79\", \"AGE80-84\", \"AGE85+\") ~ \"Old\",\n    TRUE ~ NA_character_  # Assign NA to any undefined age groups\n  )) %&gt;%\n  filter(!is.na(Age_Group))  # Remove any rows with missing Age_Group\n\n# Summarize population by Region and Age_Group\ndf_region &lt;- pop_data %&gt;%\n  group_by(Region, Age_Group) %&gt;%\n  summarise(Population = sum(Population, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  pivot_wider(names_from = Age_Group, values_from = Population, values_fill = list(Population = 0))  # Convert to wide format\n\n# Convert population counts to proportions\ndf_region &lt;- df_region %&gt;%\n  mutate(Total = Young + Active + Old,\n         Young = Young / Total,\n         Active = Active / Total,\n         Old = Old / Total) %&gt;%\n  select(Region, Young, Active, Old)\n\nggtern(data = df_region, aes(x = Active, y = Young, z = Old)) +  # Swapped x and y\n  geom_point(aes(color = Region), size = 4) +  \n  labs(title = \"Age Distribution by Region\",\n       x = \"Active\",   # Left axis\n       y = \"Young\",    # Top axis\n       z = \"Old\") +    # Right axis\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14),\n    axis.title = element_text(size = 11),\n    axis.text = element_text(size = 10, color = \"black\"),\n    axis.title.x = element_text(margin = margin(t = 10)),  # Space below \"Active\"\n    axis.title.y = element_text(margin = margin(r = 10)),  # Space right of \"Young\"\n    axis.title.z = element_text(margin = margin(l = 10))   # Space left of \"Old\"\n  )\n\n\n\n\n\n\nTo explore ternary plot by planning area (PA) - using interactive plot - plotly\n\n\nGraphCode\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(plotly)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Define custom colors matching the attached legend\ncustom_colors &lt;- c(\n  \"Central\" = \"#F8766D\",  \n  \"North\" = \"#00BA38\",     \n  \"South\" = \"#619CFF\",     \n  \"East\" = \"#B79F00\",      \n  \"Other\" = \"#00BFC4\",     \n  \"West\" = \"#F564E3\"       \n)\n\n# Create the interactive ternary plot with custom legend colors\nplot_ly(\n  df_region, \n  a = ~Young,   # Left side\n  b = ~Active,  # Top\n  c = ~Old,     # Right side\n  text = ~paste(\"Region:\", Region, \n                \"&lt;br&gt;Young:\", round(Young * 100, 2), \"%\", \n                \"&lt;br&gt;Active:\", round(Active * 100, 2), \"%\", \n                \"&lt;br&gt;Old:\", round(Old * 100, 2), \"%\"), \n  color = ~Region, \n  colors = custom_colors,  \n  type = 'scatterternary',\n  mode = 'markers',\n  marker = list(size = 10),\n  hoverinfo = \"text\"  \n) %&gt;%\n  layout(\n    title = list(text = \"Interactive Ternary Plot: Age Distribution by Region\", font = list(size = 16)),\n    margin = list(l = 100, r = 100, b = 100, t = 100),  # Adjust margins to prevent label clipping\n    ternary = list(\n      sum = 1,  # Ensure the ternary plot normalizes proportions\n      aaxis = list(title = \"Young\", min = 0, max = 1, tickformat = \".0%\", titlefont = list(size = 14), tickfont = list(size = 12), titleoffset = 40),\n      baxis = list(title = \"Active\", min = 0, max = 1, tickformat = \".0%\", titlefont = list(size = 14), tickfont = list(size = 12), titleoffset = 40),\n      caxis = list(title = \"Old\", min = 0, max = 1, tickformat = \".0%\", titlefont = list(size = 14), tickfont = list(size = 12), titleoffset = 40)\n    ),\n    legend = list(\n      orientation = \"h\",  # Horizontal legend\n      x = 0.5, y = -0.2,  # Centered at bottom\n      xanchor = \"center\", yanchor = \"top\"\n    ),\n    annotations = list(\n      list(\n        text = \"Hover over points for details\",\n        x = 0.5, y = 1.15,\n        xref = \"paper\", yref = \"paper\",\n        showarrow = FALSE,\n        font = list(size = 12, color = \"black\")\n      )\n    )\n  )"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#graph-2",
    "href": "Take-home_Ex/Take-home_Ex01.html#graph-2",
    "title": "Take-home Exercise 01",
    "section": "",
    "text": ":::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCh"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b.html",
    "href": "Hands-on_Ex/Hands-on_Ex05b.html",
    "title": "Hands-on Exercise 05b",
    "section": "",
    "text": "With the assistance of ChatGPT"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b.html#visual-correlation-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex05b.html#visual-correlation-analysis",
    "title": "Hands-on Exercise 05b",
    "section": "5. Visual Correlation Analysis",
    "text": "5. Visual Correlation Analysis\n\n5.1 Overview\nThe correlation coefficient is a widely used statistic for measuring the type and strength of the relationship between two variables. It ranges from -1.0 to 1.0, where 1.0 indicates a perfect positive linear relationship, -1.0 signifies a perfect inverse relationship, and 0.0 represents no linear correlation.\nWhen dealing with multivariate data, the correlation coefficients for all variable pairs are typically presented in a correlation matrix or scatterplot matrix.\nA correlation matrix is computed for three main reasons:\n\nUnderstanding Relationships – It helps reveal pairwise relationships between high-dimensional variables.\nInput for Further Analysis – It serves as an input for techniques like exploratory and confirmatory factor analysis, structural equation modeling, and linear regression (especially when handling missing values pairwise).\nDiagnostic Tool – It aids in assessing other analyses, such as detecting multicollinearity in linear regression, which can affect the reliability of estimates.\n\nFor large datasets with many observations and variables, a corrgram is often used to visually explore relationships and patterns. It is designed with two key principles:\n\nVisual Representation – Correlations are depicted based on their sign and magnitude.\nVariable Reordering – Similar variables are positioned adjacently in the correlation matrix to enhance pattern recognition.\n\nIn this hands-on exercise, we will learn data visualization for correlation matrices in R, covering three key sections:\n\nCreating a Correlation Matrix – Using the pairs() function from R Graphics.\nPlotting a Corrgram – Utilizing the corrplot package.\nBuilding an Interactive Correlation Matrix – Implementing plotly in R.\n\n\n\n5.2 Installing and launching R packages\nWe will use the following code chunk in RStudio to install and load the required packages: corrplot, ggpubr, plotly, and tidyverse for data visualization and analysis.\n\n\nCode\npacman::p_load(corrplot, ggstatsplot, tidyverse)\n\n\n\n\n5.3 Importing and preparing The Data Set\nIn this exercise, we will use the Wine Quality Dataset from the UCI Machine Learning Repository, which contains 13 variables and 6,497 observations. The dataset combines both red and white wine data into a single CSV file named wine_quality.\n\n5.3.1 Importing the data\n\n\nCode\nwine &lt;- read_csv(\"data/wine_quality.csv\")\n\n\nWe will check the dataset using below\n\nglimpse(): provides a transposed overview of a dataset, showing variables and their types in a concise format.\nhead(): displays the first few rows of a dataset (default is 6 rows) to give a quick preview of the data.\nsummary(): generates a statistical summary of each variable, including measures like mean, median, and range for numeric data.\nduplicated():returns a logical vector indicating which elements or rows in a vector or data frame are duplicates.\nSum(is.na()): counts the number of missing values (NA) in each column of the data frame.\nspec(): use spec() to quickly inspect the column\n\n\nglimpse()head()summary()duplicated()sum(is.na())spec()\n\n\n\n\nCode\nglimpse(wine)\n\n\nRows: 6,497\nColumns: 13\n$ `fixed acidity`        &lt;dbl&gt; 7.4, 7.8, 7.8, 11.2, 7.4, 7.4, 7.9, 7.3, 7.8, 7…\n$ `volatile acidity`     &lt;dbl&gt; 0.700, 0.880, 0.760, 0.280, 0.700, 0.660, 0.600…\n$ `citric acid`          &lt;dbl&gt; 0.00, 0.00, 0.04, 0.56, 0.00, 0.00, 0.06, 0.00,…\n$ `residual sugar`       &lt;dbl&gt; 1.9, 2.6, 2.3, 1.9, 1.9, 1.8, 1.6, 1.2, 2.0, 6.…\n$ chlorides              &lt;dbl&gt; 0.076, 0.098, 0.092, 0.075, 0.076, 0.075, 0.069…\n$ `free sulfur dioxide`  &lt;dbl&gt; 11, 25, 15, 17, 11, 13, 15, 15, 9, 17, 15, 17, …\n$ `total sulfur dioxide` &lt;dbl&gt; 34, 67, 54, 60, 34, 40, 59, 21, 18, 102, 65, 10…\n$ density                &lt;dbl&gt; 0.9978, 0.9968, 0.9970, 0.9980, 0.9978, 0.9978,…\n$ pH                     &lt;dbl&gt; 3.51, 3.20, 3.26, 3.16, 3.51, 3.51, 3.30, 3.39,…\n$ sulphates              &lt;dbl&gt; 0.56, 0.68, 0.65, 0.58, 0.56, 0.56, 0.46, 0.47,…\n$ alcohol                &lt;dbl&gt; 9.4, 9.8, 9.8, 9.8, 9.4, 9.4, 9.4, 10.0, 9.5, 1…\n$ quality                &lt;dbl&gt; 5, 5, 5, 6, 5, 5, 5, 7, 7, 5, 5, 5, 5, 5, 5, 5,…\n$ type                   &lt;chr&gt; \"red\", \"red\", \"red\", \"red\", \"red\", \"red\", \"red\"…\n\n\n\n\n\n\nCode\nhead(wine)\n\n\n# A tibble: 6 × 13\n  `fixed acidity` `volatile acidity` `citric acid` `residual sugar` chlorides\n            &lt;dbl&gt;              &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;     &lt;dbl&gt;\n1             7.4               0.7           0                 1.9     0.076\n2             7.8               0.88          0                 2.6     0.098\n3             7.8               0.76          0.04              2.3     0.092\n4            11.2               0.28          0.56              1.9     0.075\n5             7.4               0.7           0                 1.9     0.076\n6             7.4               0.66          0                 1.8     0.075\n# ℹ 8 more variables: `free sulfur dioxide` &lt;dbl&gt;,\n#   `total sulfur dioxide` &lt;dbl&gt;, density &lt;dbl&gt;, pH &lt;dbl&gt;, sulphates &lt;dbl&gt;,\n#   alcohol &lt;dbl&gt;, quality &lt;dbl&gt;, type &lt;chr&gt;\n\n\n\n\n\n\nCode\nsummary(wine)\n\n\n fixed acidity    volatile acidity  citric acid     residual sugar  \n Min.   : 3.800   Min.   :0.0800   Min.   :0.0000   Min.   : 0.600  \n 1st Qu.: 6.400   1st Qu.:0.2300   1st Qu.:0.2500   1st Qu.: 1.800  \n Median : 7.000   Median :0.2900   Median :0.3100   Median : 3.000  \n Mean   : 7.215   Mean   :0.3397   Mean   :0.3186   Mean   : 5.443  \n 3rd Qu.: 7.700   3rd Qu.:0.4000   3rd Qu.:0.3900   3rd Qu.: 8.100  \n Max.   :15.900   Max.   :1.5800   Max.   :1.6600   Max.   :65.800  \n   chlorides       free sulfur dioxide total sulfur dioxide    density      \n Min.   :0.00900   Min.   :  1.00      Min.   :  6.0        Min.   :0.9871  \n 1st Qu.:0.03800   1st Qu.: 17.00      1st Qu.: 77.0        1st Qu.:0.9923  \n Median :0.04700   Median : 29.00      Median :118.0        Median :0.9949  \n Mean   :0.05603   Mean   : 30.53      Mean   :115.7        Mean   :0.9947  \n 3rd Qu.:0.06500   3rd Qu.: 41.00      3rd Qu.:156.0        3rd Qu.:0.9970  \n Max.   :0.61100   Max.   :289.00      Max.   :440.0        Max.   :1.0390  \n       pH          sulphates         alcohol         quality     \n Min.   :2.720   Min.   :0.2200   Min.   : 8.00   Min.   :3.000  \n 1st Qu.:3.110   1st Qu.:0.4300   1st Qu.: 9.50   1st Qu.:5.000  \n Median :3.210   Median :0.5100   Median :10.30   Median :6.000  \n Mean   :3.219   Mean   :0.5313   Mean   :10.49   Mean   :5.818  \n 3rd Qu.:3.320   3rd Qu.:0.6000   3rd Qu.:11.30   3rd Qu.:6.000  \n Max.   :4.010   Max.   :2.0000   Max.   :14.90   Max.   :9.000  \n     type          \n Length:6497       \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n\n\n\n\n\nCode\nwine[duplicated(wine),]\n\n\n# A tibble: 1,177 × 13\n   `fixed acidity` `volatile acidity` `citric acid` `residual sugar` chlorides\n             &lt;dbl&gt;              &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;     &lt;dbl&gt;\n 1             7.4              0.7            0                1.9      0.076\n 2             7.5              0.5            0.36             6.1      0.071\n 3             7.9              0.43           0.21             1.6      0.106\n 4             7.3              0.45           0.36             5.9      0.074\n 5             7.2              0.725          0.05             4.65     0.086\n 6             8.8              0.41           0.64             2.2      0.093\n 7             8.6              0.49           0.28             1.9      0.11 \n 8             7.7              0.49           0.26             1.9      0.062\n 9             8.1              0.545          0.18             1.9      0.08 \n10             8.1              0.575          0.22             2.1      0.077\n# ℹ 1,167 more rows\n# ℹ 8 more variables: `free sulfur dioxide` &lt;dbl&gt;,\n#   `total sulfur dioxide` &lt;dbl&gt;, density &lt;dbl&gt;, pH &lt;dbl&gt;, sulphates &lt;dbl&gt;,\n#   alcohol &lt;dbl&gt;, quality &lt;dbl&gt;, type &lt;chr&gt;\n\n\n\n\n\n\nCode\nsum(is.na(wine))  \n\n\n[1] 0\n\n\n\n\n\n\nCode\nspec(wine)\n\n\ncols(\n  `fixed acidity` = col_double(),\n  `volatile acidity` = col_double(),\n  `citric acid` = col_double(),\n  `residual sugar` = col_double(),\n  chlorides = col_double(),\n  `free sulfur dioxide` = col_double(),\n  `total sulfur dioxide` = col_double(),\n  density = col_double(),\n  pH = col_double(),\n  sulphates = col_double(),\n  alcohol = col_double(),\n  quality = col_double(),\n  type = col_character()\n)\n\n\n\n\n\nThe wine tibble contains 13 attributes, as shown above:\n\nCategorical attributes: quality, type\nContinuous attributes: fixed acidity volatile acidity citric acid residual sugar chlorides free sulfur dioxide total sulfur dioxide density pH sulphates alcohol\n\n\n\n\n5.4 Building correlation matrix: pairs() method\nR offers multiple ways to create a scatterplot matrix. In this section, we will use the pairs() function from R Graphics to build a correlation matrix.\nBefore proceeding, we will review and read the syntax description of the pairs() function.\n\n5.4.1 Building a basic correlation matrix\nFigure below shows the scatter plot matrix of Wine Quality Data. It is a 11 by 11 matrix.\n\n\nCode\npairs(wine[,1:11])\n\n\n\n\n\n\n\n\n\nThe pairs() function in R requires a matrix or data frame as input.\nTo create a scatterplot matrix, a simple code chunk is used with the default pairs() function.\nIn the below code, columns 2 to 12 of the wine data frame are selected, including variables such as fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, and alcohol.\n\n\nCode\npairs(wine[,2:12])\n\n\n\n\n\n\n\n\n\n\n\n5.4.2 Drawing the lower corner\nThe pairs() function in R Graphics offers various customization options. Since a correlation matrix is symmetric, it is common to display only the upper or lower half.\nTo show the lower half, the upper.panel argument is used, as demonstrated in the following code chunk.\n\n\nCode\npairs(wine[,2:12], upper.panel = NULL)\n\n\n\n\n\n\n\n\n\nTo show the upper half, the lower.panel argument is used, as demonstrated in the following code chunk.\n\n\nCode\npairs(wine[,2:12], lower.panel = NULL)\n\n\n\n\n\n\n\n\n\n\n\n5.4.3 Including with correlation coefficients\nTo display correlation coefficients instead of scatter plots, the panel.cor function is used, with higher correlations shown in a larger font.\n\n\nCode\npanel.cor &lt;- function(x, y, digits=2, prefix=\"\", cex.cor, ...) {\nusr &lt;- par(\"usr\")\non.exit(par(usr))\npar(usr = c(0, 1, 0, 1))\nr &lt;- abs(cor(x, y, use=\"complete.obs\"))\ntxt &lt;- format(c(r, 0.123456789), digits=digits)[1]\ntxt &lt;- paste(prefix, txt, sep=\"\")\nif(missing(cex.cor)) cex.cor &lt;- 0.8/strwidth(txt)\ntext(0.5, 0.5, txt, cex = cex.cor * (1 + r) / 2)\n}\n\npairs(wine[,2:12], \n      upper.panel = panel.cor)\n\n\n\n\n\n\n\n\n\n\n\n\n5.5 Visualising Correlation Matrix: ggcormat()\nA key limitation of the correlation matrix is that scatterplots become cluttered when the dataset is large (over 500 observations). To address this, the Corrgram visualization technique, proposed by Murdoch & Chow (1996) and Friendly (2002), will be used.\nThere are at least three R packages - corrgram, - ellipse, and - corrplot\nwhich offer functions to plot corrgrams.\nAdditionally, packages like ggstatsplot include functions for building corrgrams.\nIn the below section, we will learn how to visualize a correlation matrix using the ggcorrmat() function from the ggstatsplot package.\n\n5.5.1 The basic plot\nA key advantage of using ggcorrmat() over other methods for visualizing a correlation matrix is its ability to generate a comprehensive and professional statistical report, enhancing clarity and interpretability.\n\n\nCode\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11)\n\n\n\n\n\n\n\n\n\n\n\nCode\nggstatsplot::ggcorrmat(\n  data = wine, \n  cor.vars = 1:11,\n  ggcorrplot.args = list(outline.color = \"black\", \n                         hc.order = TRUE,\n                         tl.cex = 10),\n  title    = \"Correlogram for wine dataset\",\n  subtitle = \"Four pairs are no significant at p &lt; 0.05\"\n)\n\n\n\n\n\n\n\n\n\nSome of the key takeaways from the code above: - cor.vars – Defines the variables for computing the correlation matrix in the corrgram. - ggcorrplot.args – Adds aesthetic customizations for ggcorrplot::ggcorrplot, excluding internally used arguments like corr, method, p.mat, sig.level, ggtheme, colors, lab, pch, legend.title, and digits.\nA sample sub-code chunk below can be used to customize specific plot components, such as font size for the x-axis, y-axis, and statistical report.\n\n\nCode\nggplot.component = list(\n    theme(text=element_text(size=5),\n      axis.text.x = element_text(size = 8),\n      axis.text.y = element_text(size = 8)))\n\n\n\n\n\n6.6 Building multiple plots\nSince ggstatsplot extends ggplot2, it supports faceting. However, faceting is not available in ggcorrmat() but is supported in grouped_ggcorrmat() from ggstatsplot.\n\n\nCode\ngrouped_ggcorrmat(\n  data = wine,\n  cor.vars = 1:11,\n  grouping.var = type,\n  type = \"robust\",\n  p.adjust.method = \"holm\",\n  plotgrid.args = list(ncol = 2),\n\n  # Optimizing appearance for clarity\n  ggcorrplot.args = list(\n    outline.color = \"black\",\n    hc.order = TRUE,\n    tl.cex = 5,   # Adjust text size to prevent overlap\n    tl.srt = 45,  # Rotate text for better alignment\n    lab_size = 1  # Adjust label size for better readability (without lab=TRUE)\n  ),\n\n  # Improving annotation clarity\n  annotation.args = list(\n    tag_levels = \"a\",\n    title = \"Correlogram for Wine Dataset\",\n    subtitle = \"Correlation between key chemical properties of red and white wines\",\n    caption = \"Dataset: UCI Machine Learning Repository\"\n  )\n)\n\n\n\n\n\n\n\n\n\nSome of the key takeaways from the code above: - grouping.var – The only required argument to create a facet plot in grouped_ggcorrmat(). - Patchwork Package – Used behind the scenes for multi-plot layouts. - plotgrid.args – Passes additional arguments to patchwork::wrap_plots, except for guides, which is set separately. - annotation.args – Specifies plot annotation arguments from the patchwork package.\n\n\n6.7 Visualising correlation matrix using corrplot package\nWe read on the review “An Introduction to corrplot Package” to gain a basic understanding of corrplot package.\n\n6.7.1 Getting started with corrplot\nBefore using corrplot() to create a corrgram, the correlation matrix of the wine data frame must be computed. This is done using the cor() function from R Stats, as shown in the code chunk below.\n\n\nCode\nwine.cor &lt;- cor(wine[, 1:11])\n\n\nThe corrplot() function is then used to plot the corrgram with its default settings, as shown in the code chunk below.\n\n\nCode\ncorrplot(wine.cor)\n\n\n\n\n\n\n\n\n\nBy default, corrplot() uses circles for visualization and a symmetric matrix layout. The color scheme is blue-red, where:\n\nBlue represents positive correlations, and red represents negative correlations.\nColor intensity (saturation) indicates correlation strength — darker colors show stronger relationships, while lighter colors indicate weaker correlations.\n\n\n\n6.7.2 Working with visual geometrics\nThe corrplot package offers seven visualization methods (method parameter) to represent correlation values: circle (default), square, ellipse, number, shade, color, and pie.\nThe default circle representation can be modified using the method argument, as demonstrated in the code chunk below.\n\nellipsesquarenumbershadecolorpie\n\n\n\n\nCode\ncorrplot(wine.cor, \n         method = \"ellipse\") \n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncorrplot(wine.cor, \n         method = \"square\") \n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncorrplot(wine.cor, \n         method = \"number\") \n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncorrplot(wine.cor, \n         method = \"shade\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncorrplot(wine.cor, \n         method = \"color\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncorrplot(wine.cor, \n         method = \"pie\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.7.3 Working with layout\ncorrplot() supports three layout types: “full” (default), “upper”, and “lower”.\nThe default full matrix display can be modified using the type argument.\n\nlowerupperfull\n\n\n\n\nCode\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncorrplot(wine.cor, \n         method = \"square\", \n         type=\"upper\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncorrplot(wine.cor, \n         method = \"pie\", \n         type=\"full\")\n\n\n\n\n\n\n\n\n\n\n\n\nThe corrgram layout can be further customized. For example:\n\ndiag = FALSE - removes diagonal cells.\ntl.col = \"black\" - changes axis text labels to black.\n\n\n\nCode\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\",\n         diag = FALSE,\n         tl.col = \"black\")\n\n\n\n\n\n\n\n\n\nFurther exploration:\n\ntl.postl.cextl.offsettl.srtcl.poscl.cexcl.offset\n\n\n\ntl.pos - Position of the text labels\nit must be one of lt, ld, td, d, l or n.\n\nlt(default if type=full) means left and top,\nld(default if type=lower) means left and diagonal\ntd(default if type=upper) means top and diagonal(near)\nd means diagonal, l means left,\nn means don’t add text-label.\n\n\n\n\nCode\ncorrplot(wine.cor, \n         method = \"number\", \n         type=\"lower\",\n         diag = FALSE,\n         tl.pos = \"lt\")\n\n\n\n\n\n\n\n\n\n\n\n\ntl.cex - size of text label (variable names).\n\n\n\nCode\ncorrplot(wine.cor, \n         method = \"square\", \n         type=\"lower\",\n         diag = FALSE,\n         tl.cex = 1)\n\n\n\n\n\n\n\n\n\n\n\nModifies text label spacing\n\n\nCode\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"lower\",\n         diag = FALSE,\n         tl.offset = 2)\n\n\n\n\n\n\n\n\n\n\n\n\ntl.cex - text label string rotation in degrees.\n\n\n\nCode\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         type=\"upper\",\n         diag = FALSE,\n         tl.srt = 45 )\n\n\n\n\n\n\n\n\n\n\n\n\ncl.pos - Moves the color legend, in this case, it is “b” - bottom\nOther options include\n\n“r” : right (default)\n“b” : bottom\n“t” : top\n“n” : none\n\n\n\n\nCode\ncorrplot(wine.cor, \n         method = \"pie\", \n         type=\"upper\",\n         diag = FALSE,\n         cl.pos = \"b\" )\n\n\n\n\n\n\n\n\n\n\n\n\ncl.cex- Adjusts color legend text size\n\n\n\nCode\ncorrplot(wine.cor, \n         method = \"color\", \n         type=\"upper\",\n         diag = FALSE,\n         cl.cex = 2 )\n\n\n\n\n\n\n\n\n\n\n\n\ncl.offset- Adjusts color legend positioning\n\n\n\nCode\ncorrplot(wine.cor, \n         method = \"shade\", \n         type=\"upper\",\n         diag = FALSE,\n         cl.offset = 0.5 )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.7.4 Working with mixed layout\nThe corrplot package allows creating a mixed corrgram, where one half displays a visual matrix and the other half shows numerical values. This is achieved using corrplot.mixed(), a specialized function for mixed visualization styles.\nFigure below shows a mixed layout corrgram plotted using wine quality data.\n\n\nCode\ncorrplot.mixed(wine.cor, \n               lower = \"shade\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"green\",\n               tl.srt = 45)\n\n\n\n\n\n\n\n\n\n\n\nCode\ncorrplot.mixed(wine.cor, \n               lower = \"circle\", \n               upper = \"square\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"red\")\n\n\n\n\n\n\n\n\n\nIn corrplot.mixed(), the arguments lower and upper define the visualization methods for each half of the corrgram (e.g., circles for the lower half and squares for the upper half).\n\ntl.pos : controls the placement of axis labels.\ndiag : specifies the glyph style on the diagonal of the corrgram.\n\n\n\n6.7.5 Combining corrgram with the significant test\nIn statistical analysis, it’s important to determine which variable pairs have statistically significant correlations.\nThe corrgram with significance testing reveals that not all correlations are significant. For example:\n\nThe correlation between total sulfur dioxide and free sulfur dioxide is significant at a 0.1 level.\nHowever, the correlation between total sulfur dioxide and citric acid is not statistically significant.\n\n\n\n\n\n\n\n\n\n\nWe can use the cor.mtest() to compute the p-values and confidence interval for each pair of variables.\n\n\nCode\nwine.sig = cor.mtest(wine.cor, conf.level= .95)\n\n\nWe can then use the p.mat argument of corrplot function as shown in the code below.\n\n\nCode\ncorrplot(wine.cor,\n         method = \"number\",\n         type = \"lower\",\n         diag = FALSE,\n         tl.col = \"black\",\n         tl.srt = 45,\n         p.mat = wine.sig$p,\n         sig.level = .05)\n\n\n\n\n\n\n\n\n\n\n\n6.7.6 Reorder a corrgram\nMatrix reordering is crucial for uncovering hidden structures and patterns in a corrgram. By default, attributes are sorted based on the original correlation matrix, but this can be changed using the order argument in corrplot().\nSupported Sorting Methods in corrplot():\n\n“AOE” – Angular order of eigenvectors (Michael Friendly, 2002).\n“FPC” – First principal component order.\n“hclust” – Hierarchical clustering order (customizable with hclust.method).\n\nSupported agglomeration methods: “ward”, “single”, “complete”, “average”, “mcquitty”, “median”, “centroid”.\n\n“alphabet” – Alphabetical order. Additional ordering algorithms are available in the [seriation](https://www.rdocumentation.org/packages/seriation/versions/1.4.1) package.\n\n\nAOEFPChclustalphabet\n\n\n\n\nCode\ncorrplot.mixed(wine.cor, \n               lower = \"shade\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               order=\"AOE\",\n               tl.col = \"black\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncorrplot.mixed(wine.cor, \n               lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               order=\"FPC\",\n               tl.col = \"blue\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         tl.pos = \"lt\",\n         tl.col = \"purple\",\n         order=\"hclust\",\n         hclust.method = \"centroid\",\n         addrect = 5)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncorrplot.mixed(wine.cor, \n               lower = \"circle\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               order=\"alphabet\",\n               tl.col = \"pink\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.7.7 Reordering a correlation matrix using hclust\nWhen using hclust in corrplot(), hierarchical clustering can be applied to group similar variables, and rectangles can be drawn around clusters in the corrgram to highlight these groupings.\nThere are seven supported methods:\n\n\n\n\n\n\n\n\n\nMethod\nDescription\nStrengths\nWeaknesses\n\n\n\n\nward.D / ward.D2\nMinimizes variance\nCompact, balanced clusters\nSensitive to outliers\n\n\nsingle\nMerges based on closest points\nCan handle noisy data\nCreates elongated chains\n\n\ncomplete\nMerges based on farthest points\nWell-separated clusters\nCan over-separate data\n\n\naverage\nUses average distances\nGood balance of compactness & separation\nCan struggle with noisy data\n\n\nmcquitty\nWeighted version of average\nLess computationally expensive\nLess balanced clusters\n\n\nmedian\nUses median distances\nHandles outliers well\nLess commonly used\n\n\ncentroid\nUses centroid of clusters\nComputationally efficient\nCan create inconsistent results\n\n\n\n\nward.Dsinglecompleteaveragemcquittymediancentroid\n\n\n\n\nCode\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         tl.pos = \"lt\",\n         tl.col = \"black\",\n         order=\"hclust\",\n         hclust.method = \"ward.D\",\n         addrect = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncorrplot(wine.cor, \n         method = \"number\", \n         tl.pos = \"lt\",\n         tl.col = \"black\",\n         order = \"hclust\",\n         hclust.method = \"single\",\n         addrect = 4) \n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncorrplot(wine.cor, \n         method = \"ellipse\", \n         tl.pos = \"t\",\n         tl.col = \"blue\",\n         order = \"hclust\",\n         hclust.method = \"complete\",\n         addrect = 6)  \n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncorrplot(wine.cor, \n         method = \"shade\", \n         tl.pos = \"lt\",\n         tl.col = \"black\",\n         order=\"hclust\",\n         hclust.method = \"average\",\n         addrect = 8)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncorrplot(wine.cor, \n         method = \"shade\", \n         tl.pos = \"lt\",\n         tl.col = \"black\",\n         order=\"hclust\",\n         hclust.method = \"mcquitty\",\n         addrect = 6)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncorrplot(wine.cor, \n         method = \"square\", \n         tl.pos = \"n\",  \n         order = \"hclust\",\n         hclust.method = \"median\",\n         addrect = 5, \n         cl.pos = \"b\")  \n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncorrplot(wine.cor, \n         method = \"color\", \n         tl.pos = \"d\", \n         tl.col = \"red\",\n         order = \"hclust\",\n         hclust.method = \"centroid\",\n         addrect = 7)  # Draw 7 clusters\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.8 References\n\nKam, T.S(2024). Visual Statistical Analysis.\n\n\n\n6.9 Takeaway\n\n\n\n\n\n\nKey takeaways\n\n\n\n\nLearnt to use ggcorrmat() for clean, report-friendly visualizations.\nLearnt to use corrplot() for more customization and clustering insights.\nReordering methods help detect hidden structures in correlation matrices.\nSignificance testing is crucial to avoid misleading interpretations.\n\n\nR functions for correlation analysis\n\n\n\nFunctions\nPurpose\n\n\n\n\npairs()\nScatterplot matrix\n\n\nggcorrmat()\nStatistical correlogram\n\n\ngrouped_ggcorrmat()\nFaceted correlogram\n\n\ncorrplot()\nHighly customizable correlation matrix\n\n\ncorrplot.mixed()\nMixed visual/numerical correlation matrix\n\n\ncor.mtest()\nComputes p-values for significance testing\n\n\norder in corrplot()\nReorders matrix based on structure\n\n\n\n\n\n\n\n\n7.0 Further exploration"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05b.html#r-functions-for-correlation-analysis",
    "href": "Hands-on_Ex/Hands-on_Ex05b.html#r-functions-for-correlation-analysis",
    "title": "Hands-on Exercise 05b",
    "section": "R functions for correlation analysis",
    "text": "R functions for correlation analysis\n\n\n\nFunctions\nPurpose\n\n\n\n\npairs()\nScatterplot matrix\n\n\nggcorrmat()\nStatistical correlogram\n\n\ngrouped_ggcorrmat()\nFaceted correlogram\n\n\ncorrplot()\nHighly customizable correlation matrix\n\n\ncorrplot.mixed()\nMixed visual/numerical correlation matrix\n\n\ncor.mtest()\nComputes p-values for significance testing\n\n\norder in corrplot()\nReorders matrix based on structure"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05c.html",
    "href": "Hands-on_Ex/Hands-on_Ex05c.html",
    "title": "Hands-on Exercise 05c",
    "section": "",
    "text": "With the assistance of ChatGPT"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05c.html#heatmap-for-visualising-and-analysing-multivariate-data",
    "href": "Hands-on_Ex/Hands-on_Ex05c.html#heatmap-for-visualising-and-analysing-multivariate-data",
    "title": "Hands-on Exercise 05c",
    "section": "5. Heatmap for Visualising and Analysing Multivariate Data",
    "text": "5. Heatmap for Visualising and Analysing Multivariate Data\n\n5.1 Overview\nHeatmaps use color variations to visualize data patterns in a tabular format. They are useful for examining multivariate data, where columns represent variables and rows represent observations.\nKey Benefits of Heatmaps:\n\nShow variance across multiple variables.\nReveal patterns and relationships between variables.\nIdentify similar variables and potential correlations\n\nIn this hands-on, we will learn how to create both static and interactive heatmaps using R for data visualization and analysis.\n\n\n5.2 Installing and Launching R Packages\nWe will install and load the following packages in R:\n\nseriation – For data ordering and clustering\nheatmaply – For creating interactive heatmaps\ndendextend – For enhancing dendrograms\ntidyverse – For data manipulation and visualization\n\n\n\nCode\npacman::p_load(seriation, dendextend, heatmaply, tidyverse, gplots)\n\n\n\n\n5.3 Importing and preparing the data set\nThis exercise uses the World Happiness Report 2018 dataset, extracted from Excel and saved as WHData-2018.csv, for heatmap visualization and analysis in R.\n\n5.3.1 Importing the dataset\nThe read_csv() function from readr is used to import WHData-2018.csv into R, converting it into a tibble data frame for easier analysis.\n\n\nCode\nwh &lt;- read_csv(\"data/WHData-2018.csv\")\n\n\nWe will check the dataset using below\n\nglimpse(): provides a transposed overview of a dataset, showing variables and their types in a concise format.\nhead(): displays the first few rows of a dataset (default is 6 rows) to give a quick preview of the data.\nsummary(): generates a statistical summary of each variable, including measures like mean, median, and range for numeric data.\nduplicated():returns a logical vector indicating which elements or rows in a vector or data frame are duplicates.\nSum(is.na()): counts the number of missing values (NA) in each column of the data frame.\nspec(): use spec() to quickly inspect the column\n\n\nglimpse()head()summary()duplicated()sum(is.na())spec()\n\n\n\n\nCode\nglimpse(wh)\n\n\nRows: 156\nColumns: 12\n$ Country                        &lt;chr&gt; \"Albania\", \"Bosnia and Herzegovina\", \"B…\n$ Region                         &lt;chr&gt; \"Central and Eastern Europe\", \"Central …\n$ `Happiness score`              &lt;dbl&gt; 4.586, 5.129, 4.933, 5.321, 6.711, 5.73…\n$ `Whisker-high`                 &lt;dbl&gt; 4.695, 5.224, 5.022, 5.398, 6.783, 5.81…\n$ `Whisker-low`                  &lt;dbl&gt; 4.477, 5.035, 4.844, 5.244, 6.639, 5.66…\n$ Dystopia                       &lt;dbl&gt; 1.462, 1.883, 1.219, 1.769, 2.494, 1.45…\n$ `GDP per capita`               &lt;dbl&gt; 0.916, 0.915, 1.054, 1.115, 1.233, 1.20…\n$ `Social support`               &lt;dbl&gt; 0.817, 1.078, 1.515, 1.161, 1.489, 1.53…\n$ `Healthy life expectancy`      &lt;dbl&gt; 0.790, 0.758, 0.712, 0.737, 0.854, 0.73…\n$ `Freedom to make life choices` &lt;dbl&gt; 0.419, 0.280, 0.359, 0.380, 0.543, 0.55…\n$ Generosity                     &lt;dbl&gt; 0.149, 0.216, 0.064, 0.120, 0.064, 0.08…\n$ `Perceptions of corruption`    &lt;dbl&gt; 0.032, 0.000, 0.009, 0.039, 0.034, 0.17…\n\n\n\n\n\n\nCode\nhead(wh)\n\n\n# A tibble: 6 × 12\n  Country         Region `Happiness score` `Whisker-high` `Whisker-low` Dystopia\n  &lt;chr&gt;           &lt;chr&gt;              &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n1 Albania         Centr…              4.59           4.70          4.48     1.46\n2 Bosnia and Her… Centr…              5.13           5.22          5.04     1.88\n3 Bulgaria        Centr…              4.93           5.02          4.84     1.22\n4 Croatia         Centr…              5.32           5.40          5.24     1.77\n5 Czech Republic  Centr…              6.71           6.78          6.64     2.49\n6 Estonia         Centr…              5.74           5.82          5.66     1.46\n# ℹ 6 more variables: `GDP per capita` &lt;dbl&gt;, `Social support` &lt;dbl&gt;,\n#   `Healthy life expectancy` &lt;dbl&gt;, `Freedom to make life choices` &lt;dbl&gt;,\n#   Generosity &lt;dbl&gt;, `Perceptions of corruption` &lt;dbl&gt;\n\n\n\n\n\n\nCode\nsummary(wh)\n\n\n   Country             Region          Happiness score  Whisker-high  \n Length:156         Length:156         Min.   :2.905   Min.   :3.074  \n Class :character   Class :character   1st Qu.:4.454   1st Qu.:4.590  \n Mode  :character   Mode  :character   Median :5.378   Median :5.478  \n                                       Mean   :5.376   Mean   :5.479  \n                                       3rd Qu.:6.168   3rd Qu.:6.260  \n                                       Max.   :7.632   Max.   :7.695  \n  Whisker-low       Dystopia     GDP per capita   Social support \n Min.   :2.735   Min.   :0.292   Min.   :0.0000   Min.   :0.000  \n 1st Qu.:4.345   1st Qu.:1.654   1st Qu.:0.6162   1st Qu.:1.077  \n Median :5.285   Median :1.909   Median :0.9495   Median :1.262  \n Mean   :5.273   Mean   :1.923   Mean   :0.8874   Mean   :1.217  \n 3rd Qu.:6.051   3rd Qu.:2.270   3rd Qu.:1.1978   3rd Qu.:1.463  \n Max.   :7.569   Max.   :2.961   Max.   :1.6490   Max.   :1.644  \n Healthy life expectancy Freedom to make life choices   Generosity    \n Min.   :0.0000          Min.   :0.0000               Min.   :0.0000  \n 1st Qu.:0.4223          1st Qu.:0.3583               1st Qu.:0.1095  \n Median :0.6440          Median :0.4940               Median :0.1740  \n Mean   :0.5980          Mean   :0.4570               Mean   :0.1816  \n 3rd Qu.:0.7772          3rd Qu.:0.5800               3rd Qu.:0.2422  \n Max.   :1.0300          Max.   :0.7240               Max.   :0.5980  \n Perceptions of corruption\n Min.   :0.0000           \n 1st Qu.:0.0510           \n Median :0.0820           \n Mean   :0.1125           \n 3rd Qu.:0.1390           \n Max.   :0.4570           \n\n\n\n\n\n\nCode\nwh[duplicated(wh),]\n\n\n# A tibble: 0 × 12\n# ℹ 12 variables: Country &lt;chr&gt;, Region &lt;chr&gt;, Happiness score &lt;dbl&gt;,\n#   Whisker-high &lt;dbl&gt;, Whisker-low &lt;dbl&gt;, Dystopia &lt;dbl&gt;,\n#   GDP per capita &lt;dbl&gt;, Social support &lt;dbl&gt;, Healthy life expectancy &lt;dbl&gt;,\n#   Freedom to make life choices &lt;dbl&gt;, Generosity &lt;dbl&gt;,\n#   Perceptions of corruption &lt;dbl&gt;\n\n\n\n\n\n\nCode\nsum(is.na(wh))  \n\n\n[1] 0\n\n\n\n\n\n\nCode\nspec(wh)\n\n\ncols(\n  Country = col_character(),\n  Region = col_character(),\n  `Happiness score` = col_double(),\n  `Whisker-high` = col_double(),\n  `Whisker-low` = col_double(),\n  Dystopia = col_double(),\n  `GDP per capita` = col_double(),\n  `Social support` = col_double(),\n  `Healthy life expectancy` = col_double(),\n  `Freedom to make life choices` = col_double(),\n  Generosity = col_double(),\n  `Perceptions of corruption` = col_double()\n)\n\n\n\n\n\nThe wh tibble contains 12 attributes, as shown above:\n\nCategorical attributes: Country, Region\nContinuous attributes: Happiness score, Whisker-high, Whisker-low, Dystopia, GDP per capita, Social support, Healthy life expectancy, Freedom to make life choices, Generosity, Perceptions of corruption\n\n\n\n5.3.2 Preparing the data\nNext, the rows are renamed using country names instead of row numbers,\n\n\nCode\nrow.names(wh) &lt;- wh$Country\n\n\nThe row number has been replaced into the country name.\n\n\n5.3.3 Transforming the data frame into a matrix\nSince the data was loaded as a data frame, it needs to be converted into a data matrix for heatmap visualization.\nThe code below transforms the wh data frame into a matrix format suitable for plotting.\n\n\nCode\nwh1 &lt;- dplyr::select(wh, c(3, 7:12))\nwh_matrix &lt;- data.matrix(wh)\n\n\nwh_matrix is in R matrix format.\n\n\n\n5.4 Static heatmap\nThere are several R packages provide functions for creating static heatmaps:\n\nheatmap() (R Stats) – Basic heatmap function.\nheatmap.2() (gplots) – Enhanced version with more features.\npheatmap() (pheatmap) – “Pretty Heatmap” with customizable aesthetics.\nComplexHeatmap (Bioconductor) – Advanced heatmaps, useful for genomic data. The package’s reference guide is available here\nsuperheat – Customizable heatmaps for exploring complex datasets. The package’s reference guide is available here.\n\nThis section focuses on plotting static heatmaps using heatmap()\n\n5.4.1 Heatmap() of R stats\nThis section shows how to create a heatmap using heatmap() from Base R Stats, with the provided code chunk.\n\n\nCode\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      Rowv=NA, Colv=NA)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nBy default, heatmap() creates a clustered heatmap.\nSetting Rowv = NA and Colv = NA disables row and column dendrograms.\n\n\n\nTo plot a cluster heatmap, we just have to use the default as shown in the code below.\n\n\nCode\nwh_heatmap &lt;- heatmap(wh_matrix)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe row and column order in the heatmap differs from the original wh_matrix due to clustering.\nheatmap() reorders data by calculating distances between rows and columns, grouping similar values together.\nDendrograms are displayed alongside the heatmap to visualize hierarchical clustering.\n\n\n\nIn this heatmap, red cells represent smaller values, while larger values are also red, making the visualization less informative. The Happiness Score variable has relatively high values, causing other variables with smaller values to appear similar. To improve clarity, the matrix needs to be normalized using the scale argument, which can be applied to either rows or columns based on the analysis needs.\nThe code below normalises the matrix column-wise.\n\n\nCode\nwh_heatmap &lt;- heatmap(wh_matrix,\n                      scale=\"column\",\n                      cexRow = 0.6, \n                      cexCol = 0.8,\n                      margins = c(10, 5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe values are now scaled for better visualization.\nmargins ensures that x-axis labels are fully displayed.\ncexRow adjusts the font size of y-axis labels.\ncexCol adjusts the font size of x-axis labels.\n\n\n\n\n\n\n5.5 Creating interactive heatamp\nheatmaply is an R package for creating interactive cluster heatmaps, which can be shared as stand-alone HTML files. Developed and maintained by Tal Galili, it offers powerful visualization capabilities.\nIt is recommended to review the Introduction to Heatmaply for an overview of its features and functions, and the user manual will also be available for reference.\nWe will use heatmaply to design an interactive cluster heatmap. We will still use the wh_matrix as the input data.\n\n5.5.1 Working with heatmaply\n\n\nCode\nheatmaply(mtcars)\n\n\n\n\n\n\nThe below code create an interactive heatmap by using heatmaply package.\n\n\nCode\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)])\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nUnlike heatmap(), heatmaply() places the horizontal dendrogram on the left side of the heatmap, while row labels appear on the right\nIf the x-axis labels are too long, they are automatically rotated 135 degrees for better readability.\n\n\n\n\n\n5.5.2 Data transformation methods in heatmaply()\nWhen analyzing multivariate datasets, variables often have different measurement scales, making direct comparisons difficult. To address this, data transformation is commonly applied before clustering.\nheatmaply() supports three main transformation methods:\n\nScaling (scale)\n\nSuitable for variables from a normal distribution.\nStandardizes data by subtracting the mean and dividing by the standard deviation.\nValues reflect distance from the mean in standard deviation units.\nSupports column-wise or row-wise scaling.\n\nNormalizing (normalize)\n\nUsed when variables come from different or non-normal distributions.\nRescales values to a 0 to 1 range by subtracting the minimum and dividing by the maximum.\nPreserves the shape of the original distribution while making values comparable.\n\nPercentizing (percentize)\n\nSimilar to ranking, but converts values into percentiles.\nUses the empirical cumulative distribution function (ecdf) to transform values.\nProvides an intuitive interpretation: each value represents the percentage of observations at or below it.\n\n\nThese transformation methods ensure that variables with different scales can be effectively compared and clustered in heatmaps\n\n5.5.2.1 Scaling method\nCode below is used to scale variable values columewise.\n\n\nCode\nheatmaply(wh_matrix[, -c(1, 2, 4, 5)],\n          scale = \"column\")\n\n\n\n\n\n\n\n\n5.5.2.2 Normalising method\nDifferent from Scaling, the normalise method is performed on the input data set i.e. wh_matrix as shown in the code below.\n\n\nCode\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\n5.5.2.3 Percentising method\nSimilar to Normalize method, the Percentize method is also performed on the input data set i.e. wh_matrix as shown in the code chunk below.\n\n\nCode\nheatmaply(percentize(wh_matrix[, -c(1, 2, 4, 5)]))\n\n\n\n\n\n\n\n\n\n5.5.3 Clustering algorithm\nheatmaply() supports various hierarchical clustering algorithms, allowing customization through key parameters:\n\ndistfun – Defines the function for computing distance (dissimilarity) between rows and columns.\n\nDefault: dist (Euclidean distance).\nOptions: “pearson”, “spearman”, “kendall” (correlation-based clustering).\n\nhclustfun – Specifies the function for hierarchical clustering when dendrograms are not provided.\n\nDefault: hclust (standard hierarchical clustering).\n\ndist_method – Controls the distance metric used for clustering.\n\nDefault: “euclidean”.\nOptions: “maximum”, “manhattan”, “canberra”, “binary”, “minkowski”.\n\nhclust_method – Determines the clustering linkage method.\n\nDefault: “complete”.\nOptions: “ward.D”, “ward.D2”, “single”, “complete”, “average” (UPGMA), “mcquitty” (WPGMA), “median” (WPGMC), “centroid” (UPGMC).\n\n\nClustering models can be fine-tuned manually or calibrated statistically for optimal results.\n\n\n5.5.4 Manual approach\nIn the code chunk below, the heatmap is plotted by using hierachical clustering algorithm with “Euclidean distance” and “ward.D” method.\n\n\nCode\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\")\n\n\n\n\n\n\n\n\n5.5.5 Statistical approach\nTo determine the best clustering method and optimal number of clusters, the dendextend package provides two key functions:\n\ndend_expend() – Identifies the recommended clustering method based on data structure.\nfind_k() – Helps determine the optimal number of clusters.\n\nFirst, dend_expend() is used to analyze the dataset and suggest the most suitable clustering method for hierarchical clustering\n\n\nCode\nwh_d &lt;- dist(normalize(wh_matrix[, -c(1, 2, 4, 5)]), method = \"euclidean\")\ndend_expend(wh_d)[[3]]\n\n\n  dist_methods hclust_methods     optim\n1      unknown         ward.D 0.6137851\n2      unknown        ward.D2 0.6289186\n3      unknown         single 0.4774362\n4      unknown       complete 0.6434009\n5      unknown        average 0.6701688\n6      unknown       mcquitty 0.5020102\n7      unknown         median 0.5901833\n8      unknown       centroid 0.6338734\n\n\nThe output table shows that “average” method should be used because it gave the high optimum value.\nNext, find_k() is used to determine the optimal number of cluster.\n\n\nCode\nwh_clust &lt;- hclust(wh_d, method = \"average\")\nnum_k &lt;- find_k(wh_clust)\nplot(num_k)\n\n\n\n\n\n\n\n\n\nFigure above shows that k=3 would be good.\nWith reference to the statistical analysis results, we can prepare the code chunk as shown below.\n\n\nCode\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          dist_method = \"euclidean\",\n          hclust_method = \"average\",\n          k_row = 3)\n\n\n\n\n\n\n\n\n5.5.6 Seriation\nOne limitation of hierarchical clustering is that it does not impose a strict row order, only a constraint on possible arrangements. For example, given items A, B, and C, a tree structure like ((A+B)+C) ensures that C won’t be placed between A and B, but it does not specify whether the order should be ABC or BAC for better visualization.\nTo address this, heatmaply uses the seriation package to optimize row and column ordering. It applies the Optimal Leaf Ordering (OLO) algorithm, which:\n\nStarts with agglomerative clustering results.\nRotates dendrogram branches to minimize dissimilarity between adjacent leaves.\nOptimizes the Hamiltonian path length, a restricted form of the Traveling Salesman Problem (TSP).\n\nApplying OLO results in a clearer heatmap by minimizing the sum of distances between adjacent elements while preserving the hierarchical structure.\n\n\nCode\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"OLO\")\n\n\n\n\n\n\nThe default ordering method in heatmaply is “OLO” (Optimal Leaf Ordering), which optimizes row and column arrangements but has a computational complexity of O(n⁴).\nThe other alternatives are:\n\nOLOGWmeannone\n\n\nOLO optimizes row and column arrangements but has a computational complexity of O(n⁴)\n\n\nCode\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"OLO\")\n\n\n\n\n\n\n\n\nGW aims for the same goal as OLO but uses a potentially faster heuristic.\n\n\nCode\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"GW\")\n\n\n\n\n\n\n\n\nThe option “mean” gives the output we would get by default from heatmap functions in other packages such as gplots::heatmap.2.\n\n\nCode\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"mean\")\n\n\n\n\n\n\n\n\nThe option “none” gives us the dendrograms without any rotation that is based on the data matrix.\n\n\nCode\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n5.5.7 Working with color palettes\nThe default colour palette uses by heatmaply is viridis. heatmaply users, however, can use other colour palettes in order to improve the aestheticness and visual friendliness of the heatmap.\n\nviridisBluesRed-Yellow-BlueGreen-Yellow-Red\n\n\n\n\nCode\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\",\n          colors = viridis)\n\n\n\n\n\n\n\n\n\n\nCode\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          seriate = \"none\",\n          colors = Blues)\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(RColorBrewer)\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]), \n          seriate = \"none\", \n          colors = brewer.pal(9, \"RdYlBu\")) \n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(RColorBrewer)\n\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]), \n          seriate = \"none\", \n          colors = colorRampPalette(c(\"green\", \"yellow\", \"red\"))(200))\n\n\n\n\n\n\n\n\n\n\n\n5.5.8 The finishing touch\nheatmaply() offers various features to enhance both statistical analysis and visual quality of heatmaps.\nIn the provided code:\n\nk_row = 5 → Creates five row clusters.\nmargins = c(60, 200, 0, 0) → Adjusts top (60) and row (200) margins for better label visibility.\nfontsize_row = 4, fontsize_col = 4 → Sets row and column label font size.\nmain → Defines the plot title.\nxlab / ylab → Labels the x-axis and y-axis.\n\n\n\nCode\nheatmaply(normalize(wh_matrix[, -c(1, 2, 4, 5)]),\n          Colv=NA,\n          seriate = \"none\",\n          colors = Blues,\n          k_row = 5,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"World Happiness Score and Variables by Country, 2018 \\nDataTransformation using Normalise Method\",\n          xlab = \"World Happiness Indicators\",\n          ylab = \"World Countries\"\n          )\n\n\n\n\n\n\n\n\n\n5.6 References\n\nKam, T.S(2024). Visual Statistical Analysis.\n\n\n\n6.9 Takeaway\n\n\n\n\n\n\nKey takeaways\n\n\n\n\nLearnt about static Heatmaps: heatmap() (Base R) and heatmap.2() (gplots)\nLearnt about interactive Heatmaps: heatmaply() allows exploration with scaling, normalizing, and percentizing options.\nLearnt about clustering: Uses hierarchical clustering (dist_method, hclust_method) to group similar data.\nLearnt about seriation: Optimizes row/column ordering for better clarity (OLO, GW, mean).\n\n\n\n\n\n5.7 Further exploration\n\nTo explore heatmaply_cor function, which is a wrapper around heatmaply with arguments optimised for use with correlation matrices.\n\n\nbasic heatmaply_cor()Thresholding()add stat significance()add stat significance()\n\n\n\n\nCode\nheatmaply_cor(\n  cor(mtcars),\n  xlab = \"Features\",\n  ylab = \"Features\",\n  k_col = 2,\n  k_row = 2\n)\n\n\n\n\n\n\n\n\n\nHighlight Strong and Weak Correlations\nIn this case, correlations below 0.7 (absolute value) appear as white - filtering out weak correlations\n\n\n\nCode\n# Compute correlation matrix\ncor_matrix &lt;- cor(mtcars, use = \"complete.obs\")\n\n# Define threshold for highlighting strong correlations\nthreshold &lt;- 0.7  \n\n# Create a masked color matrix where weak correlations are replaced with NA\nmasked_colors &lt;- ifelse(abs(cor_matrix) &gt;= threshold, cor_matrix, NA)\n\nmasked_colors[is.na(masked_colors)] &lt;- 0  # Replace NA with 0\n\nheatmaply_cor(\n  masked_colors,\n  xlab = \"Features\",\n  ylab = \"Features\",\n  k_col = 2,\n  k_row = 2,\n  main = \"Heatmap with Highlighted Strong Correlations\",\n  cellnote = round(cor_matrix, 2),  # Show all values\n  colors = colorRampPalette(c(\"blue\", \"white\", \"red\"))(200),\n  limits = c(-1, 1),\n  na.value = \"grey90\"\n)\n\n\n\n\n\n\n\n\n\nInclude p-values helps identify statistically significant correlations.\nAdds “*” for significant correlations, in this case, p value &lt; 0.05\n\n\n\nCode\nlibrary(heatmaply)\n\n# Function to compute correlation matrix with p-values\ncor_pval_matrix &lt;- function(df) {\n  cor_test &lt;- function(x, y) cor.test(x, y)$p.value\n  p_mat &lt;- outer(colnames(df), colnames(df), Vectorize(function(i, j) cor_test(df[[i]], df[[j]])))\n  rownames(p_mat) &lt;- colnames(p_mat) &lt;- colnames(df)\n  return(p_mat)\n}\n\n# Compute correlation and p-values\ncor_matrix &lt;- cor(mtcars)\np_matrix &lt;- cor_pval_matrix(mtcars)\n\n# Plot heatmap with p-value annotations\nheatmaply_cor(\n  cor_matrix,\n  xlab = \"Features\",\n  ylab = \"Features\",\n  k_col = 2,\n  k_row = 2,\n  main = \"Correlation Heatmap with P-values\",\n  cellnote = ifelse(p_matrix &lt; 0.05, \"*\", \"\"),  # Adds \"*\" for significant correlations\n  colors = viridis::viridis(100)\n)\n\n\n\n\n\n\n\n\n\np-value from the correlation test is mapped to point size.\n\n\n\nCode\nr &lt;- cor(mtcars)\n## We use this function to calculate a matrix of p-values from correlation tests\n## https://stackoverflow.com/a/13112337/4747043\ncor.test.p &lt;- function(x){\n    FUN &lt;- function(x, y) cor.test(x, y)[[\"p.value\"]]\n    z &lt;- outer(\n      colnames(x), \n      colnames(x), \n      Vectorize(function(i,j) FUN(x[,i], x[,j]))\n    )\n    dimnames(z) &lt;- list(colnames(x), colnames(x))\n    z\n}\np &lt;- cor.test.p(mtcars)\n\nheatmaply_cor(\n  r,\n  node_type = \"scatter\",\n  point_size_mat = -log10(p), \n  point_size_name = \"-log10(p-value)\",\n  label_names = c(\"x\", \"y\", \"Correlation\")\n)\n\n\n\n\n\n\n\n\n\n\nTo explore text annotations\n\nheatmaply supports the cellnote argument, which allows overlaying text values on the heatmap. By default, the text color is automatically adjusted for readability—black text on light cells and white text on dark cells.\n\n\nCode\nheatmaply(\n  mtcars,\n  cellnote = mtcars\n)\n\n\n\n\n\n\n\nTo explore the different ways of visualizing - Correlation Matrix Visualization\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(reshape2)\n\n# Prepare data for heatmap\nmelted_data &lt;- melt(cor(mtcars))\n\n# Plot heatmap with improved clarity\nggplot(melted_data, aes(Var1, Var2, fill = value)) +\n  geom_tile(color = \"black\") +  # Add borders to each cell\n  geom_text(aes(label = round(value, 2)), color = \"white\", size = 3) +\n  scale_fill_gradient2(low = \"blue\", mid = \"white\", high = \"red\") + \n  theme_minimal() + \n  labs(title = \"Correlation Heatmap\")"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05d.html",
    "href": "Hands-on_Ex/Hands-on_Ex05d.html",
    "title": "Hands-on Exercise 05d",
    "section": "",
    "text": "With the assistance of ChatGPT"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05d.html#visual-multivariate-analysis-with-parallel-coordinates-plot",
    "href": "Hands-on_Ex/Hands-on_Ex05d.html#visual-multivariate-analysis-with-parallel-coordinates-plot",
    "title": "Hands-on Exercise 05d",
    "section": "5. Visual Multivariate Analysis with Parallel Coordinates Plot",
    "text": "5. Visual Multivariate Analysis with Parallel Coordinates Plot\n\n5.1 Overview\nParallel coordinates plots, invented by Alfred Inselberg in the 1970s, are a visualization technique designed for analyzing multivariate numerical data. They help in comparing multiple variables and identifying relationships, commonly used in academic and scientific fields rather than business settings. As noted by Stephen Few (2006), their strength lies in interactive analysis rather than public presentation. A key application includes characterizing clusters in customer segmentation.\nBy the end of this hands-on exercise, we will be able to:\n\nPlot static parallel coordinates plots using ggparcoord() from the GGally package.\nCreate interactive parallel coordinates plots with the parcoords package.\nGenerate interactive parallel coordinates plots using the parallelPlot package.\n\n\n\n5.2 Installing and Launching R Packages\nFor this exercise, the below R packages will be used: - GGally: An extension of ggplot2 that simplifies multivariate data visualization, including parallel coordinates plots. - parcoords:A package for creating interactive parallel coordinates plots using htmlwidgets in R. - parallelPlot:A tool for generating interactive parallel coordinates plots with enhanced visualization capabilities. - tidyverse:A collection of R packages designed for data manipulation, visualization, and analysis following a consistent syntax.\n\n\nCode\npacman::p_load(GGally, parallelPlot, tidyverse)\n\n\n\n\n5.3 Data Preparation\n\n\nCode\nwh &lt;- read_csv(\"data/WHData-2018.csv\")\n\nspec(wh)\n\n\ncols(\n  Country = col_character(),\n  Region = col_character(),\n  `Happiness score` = col_double(),\n  `Whisker-high` = col_double(),\n  `Whisker-low` = col_double(),\n  Dystopia = col_double(),\n  `GDP per capita` = col_double(),\n  `Social support` = col_double(),\n  `Healthy life expectancy` = col_double(),\n  `Freedom to make life choices` = col_double(),\n  Generosity = col_double(),\n  `Perceptions of corruption` = col_double()\n)\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThis section data is the same dataset used in Hands-on_Ex05c\n\n\n\n\n\n5.4 Plotting Static Parallel Coordinates Plot\nThis section covers plotting static parallel coordinates plots using ggparcoord() from the GGally package.\n\n5.4.1 Plotting a simple parallel coordinates\nCode below shows a typical syntax used to plot a basic static parallel coordinates plot by using ggparcoord().\n\n\nCode\nggparcoord(data = wh, \n           columns = c(7:12))\n\n\n\n\n\n\n\n\n\n\nOnly two arguments, data and columns, are used in ggparcoord().\nThe data argument maps the dataset (e.g., wh), while columns selects the variables for the parallel coordinates plot.\n\n\n\n5.4.2 Plotting a parallel coordinates with boxplot\nTo improve the interpretability of the World Happiness parallel coordinates plot, ggparcoord() provides customization options like - groupColumn for grouping, - scale for variable scaling, - alphaLines to reduce the intensity of the line colour, - showPoints for displaying data points, - splineFactor for smoothing, - title for parallel coordinates plot a title, - alphaLines for transparency control, and - boxplot for overlaying distributions.\n\n\nCode\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Parallel Coordinates Plot of World Happines Variables\")\n\n\n\n\n\n\n\n\n\n\n\n5.4.3 Parallel coordinates with facet\nSince ggparcoord() extends ggplot2, it allows integration with ggplot2 functions.\nThe example below uses facet_wrap() to create multiple small parallel coordinates plots, each representing a different geographical region, such as East Asia.\n\n\nCode\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region)\n\n\n\n\n\n\n\n\n\nOne of the aesthetic defect of the current design is that some of the variable names overlap on x-axis.\n\n\n5.4.4 Rotating x-axis text label\nTo improve readability, the x-axis labels are rotated by 30 degrees using the theme() function in ggplot2. This enhances clarity when displaying parallel coordinates plots.\n\n\nCode\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\n\n\n\n\n\nThing to learn from the code chunk above:\n\nTo rotate x-axis text labels, use axis.text.x in the theme() function and set element_text(angle = 30) to rotate the labels by 30 degrees.\n\n\n\n5.4.5 Adjusting the rotated x-axis text label\nTo prevent label overlap after rotating x-axis text by 30 degrees, adjust text positioning using the hjust argument in element_text() within the theme() function, applied to axis.text.x.\n\n\nCode\nggparcoord(data = wh, \n           columns = c(7:12), \n           groupColumn = 2,\n           scale = \"uniminmax\",\n           alphaLines = 0.2,\n           boxplot = TRUE, \n           title = \"Multiple Parallel Coordinates Plots of World Happines Variables by Region\") +\n  facet_wrap(~ Region) + \n  theme(axis.text.x = element_text(angle = 30, hjust=1))\n\n\n\n\n\n\n\n\n\n\n\n\n5.5 Plotting Interactive Parallel Coordinates Plot: parallelPlot methods\n\n5.5.1 The basic plot\nThe parallelPlot package in R, built on htmlwidgets and d3.js, enables interactive parallel coordinates plots. This section covers its key functions for creating dynamic visualizations.\n\n\nCode\nwh &lt;- wh %&gt;%\n  select(\"Happiness score\", c(7:12))\nparallelPlot(wh,\n             width = 320,\n             height = 250)\n\n\n\n\n\n\nNotice that some of the axis labels are too long.\n\n\n5.5.2 Rotate axis label\nrotateTitle argument is used to avoid overlapping axis labels.\n\n\nCode\nparallelPlot(wh,\n             rotateTitle = TRUE)\n\n\n\n\n\n\nA key interactive feature of parallelPlot allows users to click on a variable, such as Happiness Score, changing the default blue color to varying intensities for better visualization.\n\n\n5.5.3 Changing the colour scheme\nWe can change the default blue colour scheme by using continousCS argument as shown in the code below:\n\n\nCode\nparallelPlot(wh,\n             continuousCS = \"YlOrRd\",\n             rotateTitle = TRUE)\n\n\n\n\n\n\n\n\n5.5.4 Parallel coordinates plot with histogram\nhistoVisibility argument is used to plot histogram along the axis of each variables.\n\n\nCode\nhistoVisibility &lt;- rep(TRUE, ncol(wh))\nparallelPlot(wh,\n             rotateTitle = TRUE,\n             histoVisibility = histoVisibility)\n\n\n\n\n\n\n\n\n\n5.6 References\n\nKam, T.S(2024). Visual Statistical Analysis.\nggparcoord() of GGally package\nparcoords user guide\nparallelPlot\n\n\n\n5.7 Takeaway\n\n\n\n\n\n\nKey takeaways\n\n\n\n\nParallel Coordinates Plots: Useful for visualizing multivariate numerical data and identifying relationships.\nStatic Plots: Created using ggparcoord() from GGally, allowing customization with grouping, scaling, and faceting.\nImproving Readability: Rotate x-axis labels (theme()), adjust label positioning (hjust).\nInteractive Plots: parallelPlot() enables dynamic visualization with adjustable colors, rotated labels, and histograms.\n\n\n\n\n\n5.8 Further exploration\n\nTo explore key variables that affect happiness score\n\n\nTo determine the optimal number of clusters for segmenting the dataset, I use the Elbow Method, which evaluates the within-cluster sum of squares (WSS) for different values of k (from 1 to 10).\nThe WSS measures the compactness of clusters, with lower values indicating better-defined clusters. By plotting WSS against the number of clusters, I identify the “elbow point”—the value of k where additional clusters no longer significantly improve the clustering quality.\n\n\n\nCode\nhead(wh)\n\n\n# A tibble: 6 × 7\n  `Happiness score` `GDP per capita` `Social support` `Healthy life expectancy`\n              &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;                     &lt;dbl&gt;\n1              4.59            0.916            0.817                     0.79 \n2              5.13            0.915            1.08                      0.758\n3              4.93            1.05             1.52                      0.712\n4              5.32            1.12             1.16                      0.737\n5              6.71            1.23             1.49                      0.854\n6              5.74            1.2              1.53                      0.737\n# ℹ 3 more variables: `Freedom to make life choices` &lt;dbl&gt;, Generosity &lt;dbl&gt;,\n#   `Perceptions of corruption` &lt;dbl&gt;\n\n\n\n\nCode\nlibrary(ggplot2)\n\n# Run k-means for different values of k (1 to 10)\nwss &lt;- sapply(1:10, function(k) {\n  kmeans(wh %&gt;% select(1, 2, 4, 5, 6, 7), centers = k, nstart = 10)$tot.withinss\n})\n\n# Plot the Elbow Method graph\nqplot(1:10, wss, geom = \"line\") +\n  geom_point() +\n  labs(title = \"Elbow Method for Optimal k\", x = \"Number of Clusters\", y = \"WSS\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nBased on the Elbow Method, I determine that the optimal number of clusters is k = 3. I then apply k-means clustering to the dataset, selecting key numerical variables such as Happiness Score, GDP per Capita, Social Support, Healthy Life Expectancy, Freedom, and Perceptions of Corruption.\nThe k-means algorithm assigns each country to one of the three clusters, grouping similar countries based on these features.\n\n\n\nCode\noptimal_k &lt;- 3  \n\nwh_cluster &lt;- wh %&gt;%\n  select(1, 2, 4, 5, 6, 7) %&gt;%\n  kmeans(centers = optimal_k, nstart = 10)\n\nwh$Cluster &lt;- as.factor(wh_cluster$cluster)\n\n\n\nTo better understand the clustering results, I use a Parallel Coordinates Plot, a multivariate visualization technique that represents each country as a line spanning multiple axes, corresponding to the selected variables.\nThis visualization provides insights into how different groups of countries compare in terms of happiness-related indicators, highlighting key differences between the clusters.\n\n\n\nCode\nggparcoord(wh, \n           columns = c(1, 2, 4, 5, 6, 7),  \n           groupColumn = \"Cluster\",  \n           scale = \"uniminmax\",  \n           alphaLines = 0.3,  \n           title = paste(\"Parallel Coordinates Plot with k =\", optimal_k, \"Clusters\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  \n\n\n\n\n\n\n\n\n\nKey observations:\nCluster Differentiation:\n\nCluster 3 (Blue): Represents Middle-Income Countries\n\nThis cluster falls between the two extremes, with moderate happiness scores and GDP per capita.\n\n\n\nMore variation in freedom to make life choices and generosity, suggesting diverse political and social environments.\nLikely includes emerging economies or nations in transition.\nExample: Egypt, Cambodia, Laos\n\nCluster 2 (Green): Cluster 2 (Green) Represents Wealthier and Happier Countries\n\nThis cluster consistently scores high on GDP per capita, happiness score, life expectancy, and freedom to make life choices.\nThese countries also have lower perceptions of corruption, indicating a strong governance system.\nLikely includes developed nations with strong economies and social support systems.\nExample: Denmark, Finland, and Norway\n\nCluster 1 (Red): Represents Lower-Income Countries\n\nCountries in this cluster have lower GDP per capita, happiness scores, and life expectancy.\n\n\n\nHigher perceptions of corruption suggest governance and institutional challenges.\nThese may be developing nations with weaker economies.\nExample: Tanzania, South Sudan, Central African Republic"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05e.html",
    "href": "Hands-on_Ex/Hands-on_Ex05e.html",
    "title": "Hands-on Exercise 05e",
    "section": "",
    "text": "With the assistance of ChatGPT"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex05e.html#treemap-visualisation-with-r",
    "href": "Hands-on_Ex/Hands-on_Ex05e.html#treemap-visualisation-with-r",
    "title": "Hands-on Exercise 05e",
    "section": "5. Treemap Visualisation with R",
    "text": "5. Treemap Visualisation with R\n\n5.1 Overview\nIn this hands-on exercise, we will learn how to design treemaps using R. We will first manipulate transaction data into a treemap structure with dplyr, then create a static treemap using treemap package, and finally design an interactive version with d3treeR.\n\n\n5.2 Installing and Launching R Packages\nFor this exercise, the below R packages will be used: - - treemap: Creates hierarchical treemaps to visualize data distributions using nested rectangles. - treemapify:Provides ggplot2 support for creating treemaps, allowing seamless integration with other visualizations. - tidyverse: A collection of R packages designed for data science, including ggplot2, dplyr, and tidyr for data manipulation and visualization.\n\n\nCode\npacman::p_load(treemap, treemapify, tidyverse) \n\n\n\n\n5.3 Data Wrangling\nIn this exercise, we will use the REALIS2018.csv dataset, which contains private property transaction records from 2018. The data is sourced from the REALIS portal of Singapore’s Urban Redevelopment Authority (URA) and provides insights into real estate transactions.\n\n5.3.1 Importing the data set\n\n\nCode\nrealis2018 &lt;- read_csv(\"data/realis2018.csv\")\n\n\nWe will check the dataset using below\n\nglimpse(): provides a transposed overview of a dataset, showing variables and their types in a concise format.\nhead(): displays the first few rows of a dataset (default is 6 rows) to give a quick preview of the data.\nsummary(): generates a statistical summary of each variable, including measures like mean, median, and range for numeric data.\nduplicated():returns a logical vector indicating which elements or rows in a vector or data frame are duplicates.\nSum(is.na()): counts the number of missing values (NA) in each column of the data frame.\nspec(): use spec() to quickly inspect the column\n\n\nglimpse()head()summary()duplicated()sum(is.na())spec()\n\n\n\n\nCode\nglimpse(realis2018)\n\n\nRows: 23,205\nColumns: 20\n$ `Project Name`                &lt;chr&gt; \"ADANA @ THOMSON\", \"ALANA\", \"ALANA\", \"AL…\n$ Address                       &lt;chr&gt; \"8 Old Upper Thomson Road  #05-03\", \"156…\n$ `No. of Units`                &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ `Area (sqm)`                  &lt;dbl&gt; 52, 284, 256, 256, 277, 285, 234, 155, 1…\n$ `Type of Area`                &lt;chr&gt; \"Strata\", \"Strata\", \"Strata\", \"Strata\", …\n$ `Transacted Price ($)`        &lt;dbl&gt; 888888, 2530000, 2390863, 2450000, 19800…\n$ `Nett Price($)`               &lt;chr&gt; \"-\", \"-\", \"2382517\", \"2441654\", \"-\", \"-\"…\n$ `Unit Price ($ psm)`          &lt;dbl&gt; 17094, 8908, 9307, 9538, 7148, 6947, 147…\n$ `Unit Price ($ psf)`          &lt;dbl&gt; 1588, 828, 865, 886, 664, 645, 1371, 149…\n$ `Sale Date`                   &lt;chr&gt; \"4-Jul-18\", \"5-Oct-18\", \"9-Jun-18\", \"14-…\n$ `Property Type`               &lt;chr&gt; \"Apartment\", \"Terrace House\", \"Terrace H…\n$ Tenure                        &lt;chr&gt; \"Freehold\", \"103 Yrs From 12/08/2013\", \"…\n$ `Completion Date`             &lt;chr&gt; \"2018\", \"2018\", \"2018\", \"2018\", \"2008\", …\n$ `Type of Sale`                &lt;chr&gt; \"New Sale\", \"Sub Sale\", \"New Sale\", \"New…\n$ `Purchaser Address Indicator` &lt;chr&gt; \"Private\", \"Private\", \"HDB\", \"N.A\", \"Pri…\n$ `Postal District`             &lt;dbl&gt; 20, 28, 28, 28, 26, 26, 26, 26, 26, 26, …\n$ `Postal Sector`               &lt;dbl&gt; 57, 80, 80, 80, 78, 78, 78, 78, 78, 78, …\n$ `Postal Code`                 &lt;dbl&gt; 573868, 804555, 804529, 804540, 786300, …\n$ `Planning Region`             &lt;chr&gt; \"North East Region\", \"North East Region\"…\n$ `Planning Area`               &lt;chr&gt; \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\"…\n\n\n\n\n\n\nCode\nhead(realis2018)\n\n\n# A tibble: 6 × 20\n  `Project Name`  Address             `No. of Units` `Area (sqm)` `Type of Area`\n  &lt;chr&gt;           &lt;chr&gt;                        &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;         \n1 ADANA @ THOMSON 8 Old Upper Thomso…              1           52 Strata        \n2 ALANA           156 Sunrise Terrace              1          284 Strata        \n3 ALANA           104 Sunrise Terrace              1          256 Strata        \n4 ALANA           126 Sunrise Terrace              1          256 Strata        \n5 ATELIER VILLAS  43 Yio Chu Kang Dr…              1          277 Strata        \n6 ATELIER VILLAS  11 Yio Chu Kang Dr…              1          285 Strata        \n# ℹ 15 more variables: `Transacted Price ($)` &lt;dbl&gt;, `Nett Price($)` &lt;chr&gt;,\n#   `Unit Price ($ psm)` &lt;dbl&gt;, `Unit Price ($ psf)` &lt;dbl&gt;, `Sale Date` &lt;chr&gt;,\n#   `Property Type` &lt;chr&gt;, Tenure &lt;chr&gt;, `Completion Date` &lt;chr&gt;,\n#   `Type of Sale` &lt;chr&gt;, `Purchaser Address Indicator` &lt;chr&gt;,\n#   `Postal District` &lt;dbl&gt;, `Postal Sector` &lt;dbl&gt;, `Postal Code` &lt;dbl&gt;,\n#   `Planning Region` &lt;chr&gt;, `Planning Area` &lt;chr&gt;\n\n\n\n\n\n\nCode\nsummary(realis2018)\n\n\n Project Name         Address           No. of Units   Area (sqm)    \n Length:23205       Length:23205       Min.   :1     Min.   :  24.0  \n Class :character   Class :character   1st Qu.:1     1st Qu.:  67.0  \n Mode  :character   Mode  :character   Median :1     Median :  98.0  \n                                       Mean   :1     Mean   : 118.2  \n                                       3rd Qu.:1     3rd Qu.: 127.0  \n                                       Max.   :1     Max.   :4836.0  \n Type of Area       Transacted Price ($) Nett Price($)      Unit Price ($ psm)\n Length:23205       Min.   :    40000    Length:23205       Min.   :  355     \n Class :character   1st Qu.:   950000    Class :character   1st Qu.:11231     \n Mode  :character   Median :  1280000    Mode  :character   Median :14621     \n                    Mean   :  1734099                       Mean   :15246     \n                    3rd Qu.:  1858000                       3rd Qu.:18075     \n                    Max.   :100000000                       Max.   :54363     \n Unit Price ($ psf)  Sale Date         Property Type         Tenure         \n Min.   :  33       Length:23205       Length:23205       Length:23205      \n 1st Qu.:1043       Class :character   Class :character   Class :character  \n Median :1358       Mode  :character   Mode  :character   Mode  :character  \n Mean   :1416                                                               \n 3rd Qu.:1679                                                               \n Max.   :5050                                                               \n Completion Date    Type of Sale       Purchaser Address Indicator\n Length:23205       Length:23205       Length:23205               \n Class :character   Class :character   Class :character           \n Mode  :character   Mode  :character   Mode  :character           \n                                                                  \n                                                                  \n                                                                  \n Postal District Postal Sector    Postal Code     Planning Region   \n Min.   : 1.00   Min.   : 1.00   Min.   : 18965   Length:23205      \n 1st Qu.:10.00   1st Qu.:26.00   1st Qu.:267952   Class :character  \n Median :15.00   Median :45.00   Median :456068   Mode  :character  \n Mean   :14.96   Mean   :42.66   Mean   :434269                     \n 3rd Qu.:19.00   3rd Qu.:54.00   3rd Qu.:548461                     \n Max.   :28.00   Max.   :82.00   Max.   :829750                     \n Planning Area     \n Length:23205      \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n\n\n\n\n\nCode\nrealis2018[duplicated(realis2018),]\n\n\n# A tibble: 0 × 20\n# ℹ 20 variables: Project Name &lt;chr&gt;, Address &lt;chr&gt;, No. of Units &lt;dbl&gt;,\n#   Area (sqm) &lt;dbl&gt;, Type of Area &lt;chr&gt;, Transacted Price ($) &lt;dbl&gt;,\n#   Nett Price($) &lt;chr&gt;, Unit Price ($ psm) &lt;dbl&gt;, Unit Price ($ psf) &lt;dbl&gt;,\n#   Sale Date &lt;chr&gt;, Property Type &lt;chr&gt;, Tenure &lt;chr&gt;, Completion Date &lt;chr&gt;,\n#   Type of Sale &lt;chr&gt;, Purchaser Address Indicator &lt;chr&gt;,\n#   Postal District &lt;dbl&gt;, Postal Sector &lt;dbl&gt;, Postal Code &lt;dbl&gt;,\n#   Planning Region &lt;chr&gt;, Planning Area &lt;chr&gt;\n\n\n\n\n\n\nCode\nsum(is.na(realis2018))  \n\n\n[1] 0\n\n\n\n\n\n\nCode\nspec(realis2018)\n\n\ncols(\n  `Project Name` = col_character(),\n  Address = col_character(),\n  `No. of Units` = col_double(),\n  `Area (sqm)` = col_double(),\n  `Type of Area` = col_character(),\n  `Transacted Price ($)` = col_double(),\n  `Nett Price($)` = col_character(),\n  `Unit Price ($ psm)` = col_double(),\n  `Unit Price ($ psf)` = col_double(),\n  `Sale Date` = col_character(),\n  `Property Type` = col_character(),\n  Tenure = col_character(),\n  `Completion Date` = col_character(),\n  `Type of Sale` = col_character(),\n  `Purchaser Address Indicator` = col_character(),\n  `Postal District` = col_double(),\n  `Postal Sector` = col_double(),\n  `Postal Code` = col_double(),\n  `Planning Region` = col_character(),\n  `Planning Area` = col_character()\n)\n\n\n\n\n\nThe realis2018 tibble contains 12 attributes, as shown above:\n\nCategorical attributes: Project Name Address Type of Area Sale Date Property Type Tenure Type of Sale Purchaser Address Indicator Planning Region Planning Area\nContinuous attributes: No. of Units Area (sqm) Transacted Price (\\()    Nett Price(\\)) Unit Price ($ psm) Unit Price ($ psf) Completion Date Postal District Postal Sector Postal Code\n\n\n\n5.3.2 Data Wrangling and Manipulation\nThe REALIS2018 dataset contains highly disaggregated transaction records, making it unsuitable for direct treemap visualization.\nTo prepare the data, we will use dplyr functions to group transactions by key attributes such as Project Name, Planning Region, Planning Area, Property Type, and Type of Sale.\nWe will then apply summary statistics using summarise() to compute Total Units Sold, Total Area, Median Unit Price, and Median Transacted Price.\nThe group_by() function will structure the data for aggregation, ensuring that summarise() calculates relevant statistics for each group, creating a summarized dataset suitable for treemap visualization.\n\n\n5.3.3 Grouped summaries without the Pipe\nThe code below shows a typical two lines code approach to perform the steps.\n\n\nCode\nrealis2018_grouped &lt;- group_by(realis2018, `Project Name`,\n                               `Planning Region`, `Planning Area`, \n                               `Property Type`, `Type of Sale`)\nrealis2018_summarised &lt;- summarise(realis2018_grouped, \n                          `Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE),\n                          `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n                          `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE), \n                          `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nAggregation functions likesum() and median() in R follow the standard rule for missing values: if any input contains NA, the result will also be NA.\nTo handle this, the argument na.rm = TRUE can be used to remove missing values before computation, ensuring accurate summary statistics.\nThis is particularly useful when calculating total units sold, total area, median unit price, and median transacted price in the REALIS2018 dataset, preventing missing values from affecting the results.\n\n\n\nThe code chunk above is not very efficient because we have to give each intermediate data.frame a name, even though we don’t have to care about it.\n\n\n5.3.4 Grouped summaries with the Pipe\nThe code chunk below shows a more efficient way to tackle the same processes by using the pipe, %&gt;%:\n\n\nCode\nrealis2018_summarised &lt;- realis2018 %&gt;% \n  group_by(`Project Name`,`Planning Region`, \n           `Planning Area`, `Property Type`, \n           `Type of Sale`) %&gt;%\n  summarise(`Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE), \n            `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n            `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE),\n            `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE))\n\n\n\n\n\n5.4 Designing Treemap with treemap Package\nThe treemap package in R provides a highly flexible treemap() function with over 43 arguments for customization. This section focuses on key parameters to create elegant and accurate treemaps, allowing effective visualization of hierarchical data while maintaining clarity and truthfulness.\n\n5.4.1 Designing a static treemap\nIn this section, treemap() from the treemap package is used to visualize the distribution of median unit prices and total units sold for resale condominiums by geographic hierarchy in 2017.\nFirst, records for resale condominiums are filtered from the realis2018_selected dataset to ensure relevant data for visualization.\n\n\nCode\nrealis2018_selected &lt;- realis2018_summarised %&gt;%\n  filter(`Property Type` == \"Condominium\", `Type of Sale` == \"Resale\")\n\n\n\n\n5.4.2 Using the basic arguments\nThe code below creates a treemap using three key arguments of treemap():\n\nindex: Defines hierarchical grouping.\nvSize: Specifies the variable determining the rectangle size.\nvColor: Assigns colors based on a chosen variable.\n\n\n\nCode\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\nKey takeaways from the three core treemap() arguments:\n\nindex: Requires at least two column names for hierarchical treemaps. The first column represents the highest level of aggregation, followed by lower levels.\nvSize: Maps rectangle sizes based on a numeric variable, which must contain only non-negative values.\nvColor: Works with the type argument to determine rectangle colors. If type is not defined, treemap() defaults to using index, which may result in incorrect coloring. Properly setting type ensures colors reflect median unit prices in this case.\n\n\n\n5.4.3 Working with vColor and type arguments\nIn the code below, type argument is define as value.\n\n\nCode\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type = \"value\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\nKey insights from the code:\n\nThe rectangles are shaded in varying intensities of green, representing differences in median unit prices.\nThe legend indicates that values are binned into ten equal intervals (e.g., 0–5000, 5000–10000, etc.) with a fixed interval of 5000, ensuring clear data interpretation.\n\n\n\n5.4.4 Colours in treemap package\nTwo arguments control color mapping in treemap():\n\nmapping: Defines how values are mapped to colors.\npalette: Specifies the color scheme.\n\nDifferences:\n\n“value” mapping: Uses a diverging color palette (e.g., “RdYlBu”), centering 0 at the middle color and mapping extreme values symmetrically.\n“manual” mapping: Maps min(values) to the left-end color, max(values) to the right-end color, and the midpoint to the middle color.\n\n\n\n5.4.5 The “value” type treemap\nThe code below shows a value type treemap.\n\n\nCode\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\nKey takeaways from the code:\n\nDespite using the “RdYlBu” color palette, no red rectangles appear because all median unit prices are positive.\nThe legend displays values from 5000 to 45000 due to the default range = c(min(values), max(values)), which applies automatic rounding for better readability.\n\n\n\n5.4.6 The “manual” type treemap\nThe “manual” type maps the value range linearly to the color palette, unlike the “value” type, which directly interprets values.\n\n\nCode\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"RdYlBu\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\nKey takeaways from the code:\n\nConfusing Color Scheme: The chosen color mapping makes interpretation difficult.\nMapping Issue: The mapping formula (min(values), mean(range(values)), max(values)) may not suit all datasets.\nDiverging Palette Misuse: Avoid diverging palettes (e.g., RdYlBu) when all values are strictly positive or negative.\n\nTo overcome this problem, a single colour palette such as Blues should be used.\n\n\nCode\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\n\n5.4.7 Treemap Layout\n\nTreemap Layouts: Supports “squarified” and “pivotSize” (default: “pivotSize”).\nSquarified (Bruls et al., 2000): Optimizes aspect ratios but ignores sorting order (sortID).\nPivot-by-Size (Bederson et al., 2002): Maintains sorting order while keeping acceptable aspect ratios.\n\n\n\n5.4.8 Working with alogorithm argument\nThe code below plots a squarified treemap by changing the algorithm argument.\n\n\nCode\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"squarified\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\n\n5.4.9 Using sortID\nWith the “pivotSize” algorithm, the sortID argument controls the placement order of rectangles from top left to bottom right.\n\n\nCode\ntreemap(realis2018_selected,\n        index=c(\"Planning Region\", \"Planning Area\", \"Project Name\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"manual\",\n        palette=\"Blues\", \n        algorithm = \"pivotSize\",\n        sortID = \"Median Transacted Price\",\n        title=\"Resale Condominium by Planning Region and Area, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\n\n\n5.5 Designing Treemap using treemapify Package\nThe treemapify package in R is designed for creating treemaps in ggplot2.\nReferences: (1) Introduction to treemapify, & (2) user guide\n\n5.5.1 Designing a basic treemap\n\n\nCode\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`),\n       layout = \"scol\",\n       start = \"bottomleft\") + \n  geom_treemap() +\n  scale_fill_gradient(low = \"light blue\", high = \"pink\")\n\n\n\n\n\n\n\n\n\n\n\n5.5.2 Defining hierarchy\n\nGroup by Planning Region\n\n\n\nCode\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`),\n       start = \"topleft\") + \n  geom_treemap()\n\n\n\n\n\n\n\n\n\n\nGroup by Planning Area\n\n\n\nCode\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap()\n\n\n\n\n\n\n\n\n\n\nAdding boundary line\n\n\n\nCode\nggplot(data=realis2018_selected, \n       aes(area = `Total Unit Sold`,\n           fill = `Median Unit Price ($ psm)`,\n           subgroup = `Planning Region`,\n           subgroup2 = `Planning Area`)) + \n  geom_treemap() +\n  geom_treemap_subgroup2_border(colour = \"lightgreen\",\n                                size = 2) +\n  geom_treemap_subgroup_border(colour = \"green\")\n\n\n\n\n\n\n\n\n\n\n\n\n5.6 Designing Interactive Treemap using d3treeR\n\n5.6.1 Installing d3treeR package\n\n\nCode\n# install.packages(\"devtools\")\n\n\n\n\nCode\nlibrary(devtools)\n# install_github(\"timelyportfolio/d3treeR\")\n\n\n\n\nCode\nlibrary(d3treeR)\n\n\n\n\n5.6.2 Designing An Interactive Treemap\nThe code below perform two steps:\n\nUses treemap() to create a treemap from selected variables in the condominium data frame and saves it as an object named tm.\n\n\n\nCode\ntm &lt;- treemap(realis2018_summarised,\n        index=c(\"Planning Region\", \"Planning Area\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        title=\"Private Residential Property Sold, 2017\",\n        title.legend = \"Median Unit Price (S$ per sq. m)\"\n        )\n\n\n\n\n\n\n\n\n\n\nd3tree() is then used to build an interactive treemap.\n\n\n\nCode\nd3tree(tm,rootname = \"Singapore\" )\n\n\n\n\n\n\n\n\n\n5.7 References\n\nKam, T.S(2024). Visual Statistical Analysis.\n\n\n\n5.8 Takeaway\n\n\n\n\n\n\nKey takeaways\n\n\n\n\nTreemaps in R: Use treemap for static visualizations, treemapify for ggplot2 integration, and d3treeR for interactivity.\nTreemap Arguments:\n\nindex: Defines hierarchy.\nvSize: Sets rectangle size.\nvColor: Determines color mapping.\n\nColor Mapping:\n\n“value”: Uses diverging palettes, best for datasets with mixed values.\n“manual”: Maps values linearly but can be misleading.\n\nTreemap Layouts:\n\n“squarified”: Better aspect ratios but ignores sorting.\n“pivotSize”: Preserves sorting (sortID argument) while maintaining acceptable aspect ratios.\n\nInteractive Treemaps: Convert static treemaps (treemap()) into interactive ones using d3tree().\n\n\n\n\n\n5.9 Further exploration\n\nHelps identify which property type dominates each region and highlights the price variations across Singapore’s real estate market.\n\n\nThis treemap visualizes the distribution of property types across different planning regions in Singapore.\nEach rectangle’s size represents the total number of units sold\nThe color intensity corresponds to the median unit price per square meter (psm).\n\nKey insights:\n\nLarger rectangles indicate dominant property types in each region.\nCentral Region has the largest share of Apartments and Condominiums, reflecting its high-density residential areas.\nNorth-East, East, and North Regions show a mix of Condominiums, Apartments, and landed properties like Terrace Houses.\nDarker purple shades signify areas with higher property prices, particularly in central locations.\n\n\n\nCode\n# Load required libraries\n# Load required libraries\nlibrary(tidyverse)\nlibrary(treemap)\nlibrary(d3treeR)\n\n# Summarize data: Total units sold and median unit price per sqm\nrealis2018_summarised &lt;- realis2018 %&gt;%\n  group_by(`Planning Region`, `Property Type`) %&gt;%\n  summarise(\n    `Total Unit Sold` = sum(`No. of Units`, na.rm = TRUE),\n    `Total Area` = sum(`Area (sqm)`, na.rm = TRUE),\n    `Median Unit Price ($ psm)` = median(`Unit Price ($ psm)`, na.rm = TRUE),\n    `Median Transacted Price` = median(`Transacted Price ($)`, na.rm = TRUE)\n  )\n\n# Create a static treemap\ntm &lt;- treemap(realis2018_summarised,\n        index=c(\"Planning Region\", \"Property Type\"),\n        vSize=\"Total Unit Sold\",\n        vColor=\"Median Unit Price ($ psm)\",\n        type=\"value\",\n        palette = \"Purples\",\n        title=\"Property Type Distribution by Planning Region\",\n        title.legend = \"Median Unit Price (S$ per sqm)\"\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Convert to interactive treemap\nd3tree(tm, rootname = \"Singapore\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#exploratory-data-analysis",
    "href": "Take-home_Ex/Take-home_Ex01.html#exploratory-data-analysis",
    "title": "Take-home Exercise 01",
    "section": "",
    "text": "Important\n\n\n\nObservations\n\nUneven representation of age groups\n\nNoticeably lower number of Youth (≤25 years) and Elderly (≥71 years) compared to the middle age groups\nYoung adults (26–40 years), middle-aged adults (41–55 years), and older adults (56–70 years) make up the majority of the dataset\n\nPotential bias in data collection\n\nUnderrepresentation of elderly individuals might inflate their heart attack rate in percentage-based analyses\nLow count of youth suggests they may be under-sampled\n\n\nWhat can we do:\n\nIn the future, ensure that the data collected are proportionally across all age groups\nCheck for potential sampling bias\nNormalize the heart attack rates based on the total population for each age group\n\n\n\n\nAbsolute age count()Visualization: Absolute age count()Count by Age_Category()Visualization: Age_Category()\n\n\n\n\nCode\n# Count the number of individuals by exact age\nage_distribution &lt;- heart_attack_2 %&gt;%\n  group_by(Age) %&gt;%\n  summarise(Count = n()) %&gt;%\n  arrange(Age)\n\n# Print age count\nprint(age_distribution)\n\n\n# A tibble: 62 × 2\n     Age Count\n   &lt;dbl&gt; &lt;int&gt;\n 1    18   478\n 2    19   480\n 3    20   483\n 4    21   483\n 5    22   468\n 6    23   462\n 7    24   466\n 8    25   493\n 9    26   480\n10    27   493\n# ℹ 52 more rows\n\n\n\n\n\n\nCode\nggplot(age_distribution, aes(x = Age, y = Count)) +\n  geom_bar(stat = \"identity\", fill = \"#78B3EA\", color = \"black\", alpha = 0.7) +\n  labs(title = \"Age Distribution of Individuals in Dataset\",\n       x = \"Age\",\n       y = \"Number of Individuals\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Define age category levels in the required order\nage_levels &lt;- c(\n  \"Youth (≤25 years)\",\n  \"Young adults (26–40 years)\",\n  \"Older adults (56–70 years)\",\n  \"Middle-aged adults (41–55 years)\",\n  \"Elderly (≥71 years)\"\n)\n\n# Count individuals by Age Category with ordered factor levels\nage_category_distribution &lt;- heart_attack_2 %&gt;%\n  mutate(Age_Category = case_when(\n    Age &lt;= 25 ~ \"Youth (≤25 years)\",\n    Age &gt;= 26 & Age &lt;= 40 ~ \"Young adults (26–40 years)\",\n    Age &gt;= 41 & Age &lt;= 55 ~ \"Middle-aged adults (41–55 years)\",\n    Age &gt;= 56 & Age &lt;= 70 ~ \"Older adults (56–70 years)\",\n    Age &gt;= 71 ~ \"Elderly (≥71 years)\",\n    TRUE ~ \"Unknown\"\n  )) %&gt;%\n  group_by(Age_Category) %&gt;%\n  summarise(Count = n()) %&gt;%\n  mutate(Age_Category = factor(Age_Category, levels = age_levels)) %&gt;%  # Apply custom sorting\n  arrange(Age_Category)\n\n# Print age category count\nprint(age_category_distribution)\n\n\n# A tibble: 5 × 2\n  Age_Category                     Count\n  &lt;fct&gt;                            &lt;int&gt;\n1 Youth (≤25 years)                 3813\n2 Young adults (26–40 years)        7432\n3 Older adults (56–70 years)        7201\n4 Middle-aged adults (41–55 years)  7153\n5 Elderly (≥71 years)               4401\n\n\n\n\n\n\nCode\n# Visualization: Age Category Distribution\nggplot(age_category_distribution, aes(x = Age_Category, y = Count, fill = Age_Category)) +\n  geom_bar(stat = \"identity\", color = \"black\", alpha = 0.7) +\n  \n  # Add text labels above bars\n  geom_text(aes(label = Count), vjust = -0.5, color = \"black\", size = 2.5) +\n  \n  labs(title = \"Age Category Distribution in Dataset\",\n       x = \"Age Category\",\n       y = \"Number of Individuals\") +\n  \n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set2\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations:\n\nHeart attacks occur across all ages:\n\nHistogram shows a wide distribution of heart attack patients across various age groups\n\nAge group variation in heart attack rates:\n\nHeart attack rate appears to be relatively stable across most age bins, ranging between 8.6% to 11.3%.\nHighest normalized heart attack rate is in the [78,83] age bin (~11.3%).\n\nGradual increase in heart attack rates:\n\nGradual increase in heart attack rates between younger groups [18,23] to middle-aged adults [38,43]\n\n\n\nGraph()Code()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe cut()function was used to divide a continuous variable (like Age) into discrete intervals such as bins.\ncut() function is part of Base R, which does not require any additional packages to use.\n\n\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Step 1: Remove any NA values in Age before binning\nheart_attack_2 &lt;- heart_attack_2 %&gt;%\n  filter(!is.na(Age)) %&gt;%  # Remove missing Age values\n  mutate(Age_Bin = cut(Age, \n                       breaks = seq(floor(min(Age, na.rm = TRUE)), \n                                    ceiling(max(Age, na.rm = TRUE)) + 5,  \n                                    by = 5), \n                       right = FALSE))  \n\n# Step 2: Count total individuals per age bin\ntotal_population &lt;- heart_attack_2 %&gt;%\n  group_by(Age_Bin) %&gt;%\n  summarise(Total_Count = n(), .groups = \"drop\")\n\n# Step 3: Count heart attack occurrences per age bin\nheart_attack_counts &lt;- heart_attack_2 %&gt;%\n  filter(Heart_Attack_Occurrence == \"Yes\") %&gt;%\n  group_by(Age_Bin) %&gt;%\n  summarise(Heart_Attack_Count = n(), .groups = \"drop\")\n\n# Step 4: Merge the two tables and replace NA values with 0\nnormalized_data &lt;- left_join(total_population, heart_attack_counts, by = \"Age_Bin\") %&gt;%\n  mutate(Heart_Attack_Count = replace_na(Heart_Attack_Count, 0),  # Replace NA counts with 0\n         Normalized_Heart_Attack_Rate = (Heart_Attack_Count / Total_Count) * 100) %&gt;%\n  filter(!is.na(Age_Bin))  # Ensure Age_Bin does not contain NA values\n\n# Step 5: Create the normalized histogram\nggplot(normalized_data, aes(x = Age_Bin, y = Normalized_Heart_Attack_Rate, fill = Age_Bin)) +\n  geom_bar(stat = \"identity\", color = \"black\", alpha = 0.7) +\n  \n  # Add text labels showing the percentage per bin\n  geom_text(aes(label = sprintf(\"%.1f%%\", Normalized_Heart_Attack_Rate)), vjust = -0.5, color = \"black\") +\n  \n  labs(title = \"Normalized Age Distribution of Heart Attack Patients\",\n       x = \"Age Groups (Binned by 5 Years)\",\n       y = \"Heart Attack Rate (Normalized, % of total in group)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  \n\n\n\n\n\n\n\n\nAnother point of view, categorizing ages into groups and calculating the heart attack rate for each group.\nObservations:\n\nAlmost similar rates across age groups :\n\nHeart attack rates range between 8.9% and 10.3%, showing no extreme variation.\nYoung Adults (26–40) & Elderly (≥71) have the highest rate - at 10.3%\nYouth (≤25) has the lowest rate - at 8.9%, only slightly lower than other groups.\n\nSuggests that age alone may not be the strongest predictor of heart attack risk, and other demographic, lifestyle or health factors might play a key role.\n\n\nGraph()Code()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Define Age Category Labels with Age Ranges\nage_labels &lt;- c(\n  \"Youth (≤25 years)\", \n  \"Young adults (26–40 years)\", \n  \"Middle-aged adults (41–55 years)\", \n  \"Older adults (56–70 years)\", \n  \"Elderly (≥71 years)\"\n)\n\n# Count total individuals per age category\ntotal_population &lt;- heart_attack_2 %&gt;%\n  group_by(Age_Category) %&gt;%\n  summarise(Total_Count = n())\n\n# Count heart attack occurrences per age category\nheart_attack_counts &lt;- heart_attack_2 %&gt;%\n  filter(Heart_Attack_Occurrence == \"Yes\") %&gt;%\n  group_by(Age_Category) %&gt;%\n  summarise(Heart_Attack_Count = n())\n\n# Merge both datasets and calculate the normalized heart attack rate\nnormalized_data &lt;- left_join(heart_attack_counts, total_population, by = \"Age_Category\") %&gt;%\n  mutate(Normalized_Heart_Attack_Rate = (Heart_Attack_Count / Total_Count) * 100) %&gt;%\n  \n  # Convert Age_Category into a factor with labels\n  mutate(Age_Category = factor(Age_Category, \n                               levels = c(\"Youth\", \"Young Adult\", \"Middle-Aged Adult\", \"Older Adult\", \"Elderly\"), \n                               labels = age_labels))\n\n# Create bar plot with normalized rates\nggplot(normalized_data, aes(x = Age_Category, y = Normalized_Heart_Attack_Rate, fill = Age_Category)) +\n  geom_bar(stat = \"identity\", color = \"black\", alpha = 0.7) +\n  \n  # Add data labels to show percentages\n  geom_text(aes(label = sprintf(\"%.1f%%\", Normalized_Heart_Attack_Rate)), vjust = -0.5, color = \"black\",size = 2.5) +\n  \n  labs(title = \"Normalized Heart Attack Rate by Age Group\",\n       x = \"Age Group\",\n       y = \"Heart Attack Rate (% of total in group)\",\n       fill = \"Age Categories (with Ranges)\") +  # Updated legend title\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set2\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        axis.text.y = element_text(size = 8))  \n\n\n\n\n\n\n\n\nObservations:\n\nHeart attacks occur across all ages\n\nThe density distribution shows that heart attacks (Yes) occur at all age ranges, from young to elderly individuals. Suggests that age alone is not a definitive factor.\n\nSimilar age distribution for both groups\n\nThe Heart Attack Occurrence = “No” group is more evenly distributed across age ranges\nOverall shape of the density curves for individuals who had heart attacks and those who did not appear similar.\n\nNeed for deeper analysis to identify other contributing factors that play a more crucial role.\n\n\nGraph()Code()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Ensure Heart Attack Occurrence is a factor\nheart_attack_2 &lt;- heart_attack_2 %&gt;%\n  mutate(Heart_Attack_Occurrence = factor(Heart_Attack_Occurrence, levels = c(\"No\", \"Yes\")))\n\n# Create the Ridgeline Density Plot \nggplot(heart_attack_2, aes(x = Age, y = Heart_Attack_Occurrence, fill = after_stat(x))) +\n  geom_density_ridges_gradient(scale = 2, rel_min_height = 0.01, show.legend = TRUE) +\n  scale_fill_viridis_c(option = \"magma\") +  \n\n  labs(title = \"Ridgeline Density Plot: Age Distribution by Heart Attack Occurrence\",\n       x = \"Age\",\n       y = \"Heart Attack Occurrence\",\n       fill = \"Age\") +  \n\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\n\n\n\n\n\n\n\nThe analysis is based on five age categories, with heart attack occurrence represented as a proportion.\n\n\n\nCategories\nAge\n\n\n\n\nYouth\n≤25 years\n\n\nYoung adults\n26–40 years\n\n\nMiddle-aged adults\n41–55 years\n\n\nOlder adults\n56–70 years\n\n\nElderly\n≥71 years\n\n\n\nObservations:\n\nNo overdispersion was detected, meaning the data does not exhibit excessive variance beyond what is expected.\nThe funnel plot confirms that all data points fall within expected control limits, indicating no statistically significant outliers.\nAlthough, the heart attack occurrence rate varies among age categories but remains within the expected range.\n\nFurther analysis might be needed to examine specific risk factors that contribute to heart attack rates in different age groups.\n\nGraph()Code()\n\n\n\n\n\n\n\n\n\n\n\nA funnel plot object with 5 points of which 0 are outliers. \nPlot is not adjusted for overdispersion. \n\n\n\n\n\n\nCode\n# Step 1: Convert \"Yes\"/\"No\" to 1/0\nheart_attack_2 &lt;- heart_attack_2 %&gt;%\n  mutate(Heart_Attack_Occurrence = ifelse(Heart_Attack_Occurrence == \"Yes\", 1, 0))\n\n# Step 2: Summarize data for all 5 age categories\nage_summary &lt;- heart_attack_2 %&gt;%\n  group_by(Age_Category) %&gt;%\n  summarise(\n    Heart_Attack_Count = sum(Heart_Attack_Occurrence, na.rm = TRUE),  # Numerator\n    Total_Population = n()  # Denominator\n  )\n\n# Step 3: Funnel Plot for all age categories\nfunnel_plot(\n  .data = age_summary,\n  numerator = \"Heart_Attack_Count\",\n  denominator = \"Total_Population\",\n  group = \"Age_Category\",\n  data_type = \"PR\",  # Proportion Rate\n  x_range = c(0, max(age_summary$Total_Population) + 500),  \n  y_range = c(0, max(age_summary$Heart_Attack_Count / age_summary$Total_Population) + 0.02) \n)\n\n\n\n\n\n\n\n\n\nFrom the above analysis, it suggests that age alone may not be the strongest predictor of heart attack risk, other factors might play a key role.\nIn the following analysis, we will examine the strongest predictors of heart attack incidents based on two key factor types: Demographic and Lifestyle/ Health.\n\nDemographic variables are inherent characteristics that define a person but are not directly influence by behavior or lifestyle\nLifestyle/ health variables are a result from both genetics and lifestyle factors\n\n\n\n\n\n\n\n\nFactor types\nVariables to consider\n\n\n\n\nDemographic\nGender, Region, Family_History\n\n\nLifestyle/ Health\nSmoking_History, Alcohol_Consumption, Physical_Activity, Diet_Quality, Stress_Levels_Category, Diabetes_History, Hypertension_History, Cholesterol_Level, BMI, Heart_Rate, Systolic_BP, Diastolic_BP\n\n\n\n\n\nObservations:\n\nNo statistically significant relationship was found between Gender, Region, or Family History and heart attack occurrence (p-values &gt; 0.05).\nSince all p-values are above 0.05, demographic factors alone do not play a significant role in determining heart attack risk\n\n\nChi-test()Code()\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nGender (p = 0.0758) is the closest to significance, suggesting a potential but weak association\nRegion (p = 0.9890) and Family History (p = 0.8531) show almost no correlation with heart attack occurrence, indicating that location and family history may not be strong predictors.\n\n\n\n\n\n                      Factor   Chi_Square    p_value\nX-squared...1         Gender 3.1522718581 0.07582133\nX-squared...2         Region 0.0001911346 0.98896948\nX-squared...3 Family_History 0.0342903979 0.85309047\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\n\n# Select demographic factors\ndemographic_factors &lt;- heart_attack_2 %&gt;%\n  select(Gender, Region, Family_History, Heart_Attack_Occurrence) %&gt;% \n  mutate(across(everything(), as.factor))  # Ensure all categorical variables are factors\n\n# Function to perform chi-square test for each demographic factor\nperform_chi_test &lt;- function(var) {\n  contingency_table &lt;- table(demographic_factors[[var]], demographic_factors$Heart_Attack_Occurrence)\n  test_result &lt;- chisq.test(contingency_table)\n  return(data.frame(\n    Factor = var,\n    Chi_Square = test_result$statistic,\n    p_value = test_result$p.value\n  ))\n}\n\n# Apply the chi-square test to all demographic variables\nchi_test_results &lt;- map_df(c(\"Gender\", \"Region\", \"Family_History\"), perform_chi_test)\n\n# Display chi-square test results\nprint(chi_test_results)\n\n\n\n\n\n\n\nDrawing inference from the half-eye plot with boxplot which visualizes the distribution of age across demographic factors for heart attack cases.\nObservations:\n\nDistributions across demographics look similar\n\nAge distributions for different demographic categories overlap significantly. There is no obvious skew or deviations which suggests a particular demographic factor influence heart attack risk\n\nBoxplots show similar medians & spreads\n\nMedians for each group are relatively aligned, the spread (IQR) across categories does not indicate a distinct outlier effect.\n\nThe statistical evidence (Chi-Square Test) aligns with the visual analysis (Half-Eye Plot).\n\n\nGraphCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(ggdist)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Prepare Data: Convert demographic factors to long format & Filter for \"Yes\"\nheart_attack_long &lt;- heart_attack_2 %&gt;%\n  select(Age, Gender, Region, Family_History, Heart_Attack_Occurrence) %&gt;%\n  filter(Heart_Attack_Occurrence == \"1\") %&gt;%  # 🔹 Keep only heart attack cases\n  pivot_longer(cols = c(Gender, Region, Family_History), \n               names_to = \"Demographic_Factor\", \n               values_to = \"Category\") %&gt;%\n  mutate(Age = as.numeric(Age),\n         Heart_Attack_Occurrence = as.factor(Heart_Attack_Occurrence))\n\n# Half-Eye Plot with Boxplot Overlay (Filtered)\nggplot(heart_attack_long, aes(x = Age, \n                              y = interaction(Demographic_Factor, Category), \n                              fill = Category)) +  # 🔹 Change fill to Category for demographic focus\n  stat_halfeye(adjust = 0.5, justification = -0.2,  \n               .width = c(0.5, 0.8, 0.95),  \n               slab_alpha = 0.8, \n               point_colour = NA) +  # Remove points from the half-eye plot\n  geom_boxplot(width = 0.20, \n               outlier.shape = NA, \n               alpha = 0.6) +  # Add boxplot without outliers\n  scale_fill_brewer(palette = \"Set2\", name = \"Demographic Category\") +  # 🔹 Different colors for categories\n  theme_minimal() +\n  labs(title = \"Half-Eye Plot with Boxplot: Age Distribution for Heart Attack Cases\",\n       x = \"Age\", y = \"Demographic Categories\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe below are the steps taken to prepare the dataset for logistic regression. The key lifestyle/ health factors were selected, where categorical variables were encoded, and ordinal scales variables were converted to numeric values.\n\nTableCode\n\n\n\n\n\n\n\n\n\n\nVariable Names\nType (Ordinal/ Binary)\nValues\n\n\n\n\nGender\nBinary\nMale, Female\n\n\nRegion\nBinary\nRural, Urban\n\n\nSmoking_History\nBinary\nYes = 1, No = 0\n\n\nDiabetes_History\nBinary\nYes = 1, No = 0\n\n\nHypertension_History\nBinary\nYes = 1, No = 0\n\n\nPhysical_Activity\nOrdinal\nLow = 0, Moderate = 1, High = 2\n\n\nDiet_Quality\nOrdinal\nPoor = 0, Average = 1, Good = 2\n\n\nAlcohol_Consumption\nOrdinal\nNone = 0, Low = 1, Moderate = 2, High = 3\n\n\nFamily_History\nBinary\nYes = 1, No = 0\n\n\nCholesterol_Level_Category\nOrdinal\nLow = 0, Moderate = 1, High = 2\n\n\nStress_Levels_Category\nOrdinal\nMinimal Stress = 0, Low Stress = 1, Moderate Stress = 2, High Stress = 3\n\n\nBMI_Category\nOrdinal\nUnderweight = 0, Normal Weight = 1, Overweight = 2, Obese = 3\n\n\nHeart_Rate_Category\nOrdinal\nLow = 0, Normal = 1, High = 2\n\n\nSystolic_BP_Category\nOrdinal\nNormal = 0, Elevated = 1, Hypertension Stage 1 = 2, Hypertension Stage 2 = 3\n\n\nDiastolic_BP_Category\nOrdinal\nNormal = 0, Elevated = 1, Hypertension Stage 1 = 2, Hypertension Stage 2 = 3\n\n\nHeart_Attack_Occurrence\nBinary\nYes = 1, No = 0\n\n\n\n\n\n\n\nCode\n# Select relevant health-related columns\nrelevant_factors &lt;- heart_attack_2 %&gt;% \n  select(\"Gender\", \"Region\", \"Smoking_History\", \"Diabetes_History\", \"Hypertension_History\", \"Physical_Activity\",\n         \"Diet_Quality\", \"Alcohol_Consumption\", \"Family_History\", \"Cholesterol_Level_Category\", \"Heart_Attack_Occurrence\", \n         \"Stress_Levels_Category\", \"BMI_Category\", \"Heart_Rate_Category\", \"Systolic_BP_Category\", \"Diastolic_BP_Category\")\n\n# Convert categorical variables\n# Gender encoding\nrelevant_factors$Gender &lt;- factor(relevant_factors$Gender, levels = c(\"Male\", \"Female\"))\n\n# Region encoding\nrelevant_factors$Region &lt;- factor(relevant_factors$Region, levels = c(\"Rural\", \"Urban\"))\n\n# Smoking_History encoding\nrelevant_factors$Smoking_History &lt;- factor(relevant_factors$Smoking_History, levels = c(\"Yes\", \"No\"))\n\n# Diabetes_History encoding\nrelevant_factors$Diabetes_History &lt;- factor(relevant_factors$Diabetes_History, levels = c(\"Yes\", \"No\"))\n\n# Hypertension_History encoding\nrelevant_factors$Hypertension_History &lt;- factor(relevant_factors$Hypertension_History, levels = c(\"Yes\", \"No\"))\n\n# Physical_Activity is now numeric (Low = 0, Moderate = 1, High = 2)\nrelevant_factors$Physical_Activity &lt;- recode(relevant_factors$Physical_Activity, \"Low\" = 0, \"Moderate\" = 1, \"High\" = 2) %&gt;% as.numeric()\n\n# Diet_Quality is now numeric (Poor = 0, Average = 1, Good = 2)\nrelevant_factors$Diet_Quality &lt;- recode(relevant_factors$Diet_Quality, \"Poor\" = 0, \"Average\" = 1, \"Good\" = 2) %&gt;% as.numeric()\n\n# Alcohol_Consumption is now numeric (None = 0, Low = 1, Moderate = 2, High = 3)\nrelevant_factors$Alcohol_Consumption &lt;- recode(relevant_factors$Alcohol_Consumption, \"None\" = 0, \"Low\" = 1, \"Moderate\" = 2, \"High\" = 3) %&gt;% as.numeric()\n\n# Family_History encoding\nrelevant_factors$Family_History &lt;- factor(relevant_factors$Family_History, levels = c(\"Yes\", \"No\"))\n\n# Cholesterol_Level_Category is now numeric (Low = 0, Moderate = 1, High = 2)\nrelevant_factors$Cholesterol_Level_Category &lt;- recode(relevant_factors$Cholesterol_Level_Category, \"Low\" = 0, \"Moderate\" = 1, \"High\" = 2) %&gt;% as.numeric()\n\n# Stress_Levels_Category is now numeric (Minimal Stress = 0, Low Stress = 1, Moderate Stress = 2, High Stress = 3)\nrelevant_factors$Stress_Levels_Category &lt;- recode(relevant_factors$Stress_Levels_Category,\n  \"Miniminal_Stress\" = 0,\n  \"Low_Stress\" = 1,\n  \"Moderate_Stress\" = 2,\n  \"High_Stress\" = 3\n) %&gt;% as.numeric()\n\n# BMI_Category is now numeric (Underweight = 0, Normal_Weight = 1, Overweight = 2, Obese = 3)\nrelevant_factors$BMI_Category &lt;- recode(relevant_factors$BMI_Category, \"Underweight\" = 0, \"Normal_Weight\" = 1, \"Overweight\" = 2, \"Obese\" = 3) %&gt;% as.numeric()\n\n# Heart_Rate_Category is now numeric (Low = 0, Normal = 1, High = 2)\nrelevant_factors$Heart_Rate_Category &lt;- recode(relevant_factors$Heart_Rate_Category, \"Low\" = 0, \"Normal\" = 1, \"High\" = 2) %&gt;% as.numeric()\n\n# Systolic_BP_Category is now numeric (Normal = 0, Elevated = 1, Hypertension_Stage_1 = 2, Hypertension_Stage_2 = 3)\nrelevant_factors$Systolic_BP_Category &lt;- recode(relevant_factors$Systolic_BP_Category, \"Normal\" = 0, \"Elevated\" = 1, \"Hypertension_Stage_1\" = 2, \"Hypertension_Stage_2\" = 3) %&gt;% as.numeric()\n\n# Diastolic_BP_Category is now numeric (Normal = 0, Elevated = 1, Hypertension_Stage_1 = 2, Hypertension_Stage_2 = 3)\nrelevant_factors$Diastolic_BP_Category &lt;- recode(relevant_factors$Diastolic_BP_Category, \"Normal\" = 0, \"Elevated\" = 1, \"Hypertension_Stage_1\" = 2, \"Hypertension_Stage_2\" = 3) %&gt;% as.numeric()\n\n# Convert Heart_Attack_Occurrence to binary and ensure it remains a factor\nrelevant_factors$Heart_Attack_Occurrence &lt;- factor(ifelse(relevant_factors$Heart_Attack_Occurrence == \"Yes\", 1, 0), levels = c(0, 1))\n\n\n\n\n\n\n\n\nThe code below is used to calibrate a multiple linear regression model by using glm() of Base Stats of R.\n\nResultCode\n\n\n\n\n\nCall:  glm(formula = Heart_Attack_Occurrence ~ ., family = binomial(), \n    data = relevant_factors)\n\nCoefficients:\n               (Intercept)                GenderFemale  \n                -2.657e+01                  -1.461e-13  \n               RegionUrban           Smoking_HistoryNo  \n                 1.057e-13                  -2.443e-13  \n        Diabetes_HistoryNo      Hypertension_HistoryNo  \n                 9.216e-14                   9.946e-14  \n         Physical_Activity                Diet_Quality  \n                 1.453e-15                  -1.592e-13  \n       Alcohol_Consumption            Family_HistoryNo  \n                -6.374e-14                   1.125e-13  \nCholesterol_Level_Category      Stress_Levels_Category  \n                -1.047e-13                   2.965e-14  \n              BMI_Category         Heart_Rate_Category  \n                 1.425e-13                   8.528e-14  \n      Systolic_BP_Category       Diastolic_BP_Category  \n                 1.136e-14                   3.630e-14  \n\nDegrees of Freedom: 29999 Total (i.e. Null);  29984 Residual\nNull Deviance:      0 \nResidual Deviance: 1.74e-07     AIC: 32\n\n\n\n\n\n\nCode\n# Run Logistic Regression Model\nlogistic_model &lt;- glm(Heart_Attack_Occurrence ~ ., data = relevant_factors, family = binomial())\n\n# Print Model Summary\nlogistic_model\n\n\n\n\n\n\n\n\nIn the code below, we use check_collinearity() of performance package to check for multicolinearity.\nObservations:\n\nLow Multicollinearity – VIF values are all ~1.00, indicating minimal correlation among independent variables.\nNo Immediate Need for Variable Removal – Since there is no strong multicollinearity, all predictors can remain in the model.\n\n\nResultCode\n\n\n\n\n# Check for Multicollinearity\n\nLow Correlation\n\n                       Term  VIF       VIF 95% CI Increased SE Tolerance\n                     Gender 1.00 [1.00, 14468.50]         1.00      1.00\n                     Region 1.00 [1.00, 8.76e+11]         1.00      1.00\n            Smoking_History 1.00 [1.00,      Inf]         1.00      1.00\n           Diabetes_History 1.00 [1.00, 95895.49]         1.00      1.00\n       Hypertension_History 1.00 [1.00, 1.72e+06]         1.00      1.00\n          Physical_Activity 1.00 [1.00, 2.28e+09]         1.00      1.00\n               Diet_Quality 1.00 [1.00, 1.15e+09]         1.00      1.00\n        Alcohol_Consumption 1.00 [1.00,      Inf]         1.00      1.00\n             Family_History 1.00 [1.00, 8.30e+06]         1.00      1.00\n Cholesterol_Level_Category 1.00 [1.00,      Inf]         1.00      1.00\n     Stress_Levels_Category 1.00 [1.00, 6.32e+12]         1.00      1.00\n               BMI_Category 1.00 [1.00, 3.32e+08]         1.00      1.00\n        Heart_Rate_Category 1.00 [1.00, 1.16e+11]         1.00      1.00\n       Systolic_BP_Category 1.00 [1.00, 9.06e+06]         1.00      1.00\n      Diastolic_BP_Category 1.00 [1.00, 1.13e+15]         1.00      1.00\n Tolerance 95% CI\n     [0.00, 1.00]\n     [0.00, 1.00]\n     [0.00, 1.00]\n     [0.00, 1.00]\n     [0.00, 1.00]\n     [0.00, 1.00]\n     [0.00, 1.00]\n     [0.00, 1.00]\n     [0.00, 1.00]\n     [0.00, 1.00]\n     [0.00, 1.00]\n     [0.00, 1.00]\n     [0.00, 1.00]\n     [0.00, 1.00]\n     [0.00, 1.00]\n\n\n\n\n\n\nCode\ncheck_collinearity(logistic_model)\n\n\n\n\n\n\n\n\nObservations:\n\nLow Correlations Between Variables\n\nCorrelation values range between -0.01 and 0.01, indicating very weak relationships.\nNo strong correlations suggest that multicollinearity is not a major issue\nNo predictor strongly influences another, meaning variables are mostly independent in the model, to include all variables in regression model\n\n\n\nGraphCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Generate correlation matrix plot \nggstatsplot::ggcorrmat(\n  data = relevant_factors, \n  cor.vars = 1:ncol(relevant_factors), \n  ggcorrplot.args = list(\n    outline.color = \"blue\",  # Add black outline for clarity\n    hc.order = TRUE,          # Hierarchical clustering to group correlated variables\n    tl.cex = 10               # Adjust text size for readability\n  ),\n  title    = \"Correlogram for Heart Attack Risk Factors\",\n  subtitle = \"Significance level: p &lt; 0.05\"\n)\n\n\n\n\n\n\n\n\n\nStress Levels Category is the only statistically significant variable (p = 0.0273).\nGender (Female) is the next most significant (p = 0.0666), although it is not below the traditional 0.05 threshold, it is marginally significant (p &lt; 0.1).\nAll other variables (Hypertension, Smoking, Diabetes, etc.) have p-values &gt; 0.1, making them far less significant.\n\n\nGraphCode\n\n\n\n\n\n\n\nCode\nsummary(logistic_model)\n\n\n\n\n\n\n\n\n\nGraphCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggcoefstats(logistic_model, \n            output = \"plot\")\n\n\n\n\n\n\n\n\n\nObservations:\n\nStepwise selection has removed many non-significant variables, leaving Gender, Cholesterol Levels, Stress Levels, and BMI Category as predictors.\nThis model has a lower AIC (19337), suggesting better fit compared to previous models.\n\n\nResultCode - Iteration of stepwise selection\n\n\n\n\n\n\n\nCode\n# Fit logistic regression model with all predictors + Stress Levels interactions\nfull_model &lt;- glm(Heart_Attack_Occurrence ~ . + Stress_Levels_Category * Hypertension_History, \n                  family = binomial(), data = relevant_factors)\n\n# Check significance of all predictors\nsummary(full_model)\n\nlibrary(MASS)\noptimized_model &lt;- stepAIC(full_model, direction = \"both\")\nsummary(optimized_model)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nStatistically significant variables table from each model:\n\n\n\n\n\n\n\nModel\nStatistically significant\n\n\n\n\nLogistic regression\nStress Levels Category, and Gender\n\n\nStepwise selection\nGender, Cholesterol Levels, Stress Levels Category, BMI Category\n\n\n\n\n\n\n\n\n\n\n\n\nBased on the statistically significant variables identified through logistic regression and stepwise selection — Gender, Cholesterol Levels, Stress Levels Category, and BMI Category — the following analysis focuses on individuals aged 26 to 40 years.\nThis analysis focuses on young adults (26–40 years), as they have the highest normalized heart attack rate (10.3%). We further filter for urban residents with hypertension history, as urban lifestyles and hypertension are key risk factors of heart attack occurence. By comparing males and females, we aim to assess whether stress and cholesterol levels contribute differently to heart attack risk across genders.\nObservations:\n\nWeak correlation between stress and cholesterol levels for both genders; regression lines are nearly flat.\nGender Differences: Males: Slight positive trend (higher stress → slightly higher cholesterol), greater cholesterol variability. Females: Slight negative trend (higher stress → slightly lower cholesterol).\nHeart Attack Patterns: Cases are evenly spread for males but slightly more frequent at mid-range cholesterol levels for females.\nBoxplots: Males show wider cholesterol variation; stress levels are similar across genders.\nOverall: No strong evidence of a direct link\n\n\nGraph - FemaleCode - FemaleGraph - MaleCode - Male\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(ggExtra)\n\n# Filter dataset: Females only, Age 26-40, Urban region, Underlying hypertension history\nheart_attack_filtered &lt;- heart_attack_3 %&gt;%\n  filter(Gender == \"Female\" & Age &gt;= 26 & Age &lt;= 40 & Region == \"Urban\" & Hypertension_History == \"Yes\") %&gt;%\n  mutate(Heart_Attack_Occurrence = factor(Heart_Attack_Occurrence, levels = c(\"No\", \"Yes\"))) %&gt;%  \n  filter(!is.na(Heart_Attack_Occurrence))  # Remove NA values to fix legend issue\n\n# Create scatter plot (Original Stress Levels vs. Cholesterol Level)\np &lt;- ggplot(heart_attack_filtered, aes(x = Stress_Levels, y = Cholesterol_Level, color = Heart_Attack_Occurrence)) +\n  geom_point(alpha = 0.8, size = 2) +  # Improve visibility\n  geom_smooth(method = \"lm\", se = TRUE, aes(color = Heart_Attack_Occurrence)) +  \n  scale_color_manual(values = c(\"No\" = \"lightpink\", \"Yes\" = \"deeppink\")) +  \n  labs(\n    title = \"Scatter Plot of Cholesterol Level vs Stress Levels\",\n    subtitle = \"Filtered: Females, Age 26-40, Urban, with Hypertension History\",\n    x = \"Stress Levels\",\n    y = \"Cholesterol Level\",\n    color = \"Heart Attack\"\n  ) +\n  theme_minimal()\n\n# Add marginal boxplots\nggMarginal(p, type = \"boxplot\", groupColour = TRUE, groupFill = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(ggExtra)\n\n# Filter dataset: Males only, Age 26-40, Urban region, Underlying hypertension history\nheart_attack_filtered &lt;- heart_attack_3 %&gt;%\n  filter(Gender == \"Male\" & Age &gt;= 26 & Age &lt;= 40 & Region == \"Urban\" & Hypertension_History == \"Yes\") %&gt;%\n  mutate(Heart_Attack_Occurrence = factor(Heart_Attack_Occurrence, levels = c(\"No\", \"Yes\"))) %&gt;%  \n  filter(!is.na(Heart_Attack_Occurrence))  \n\n# Create scatter plot (Original Stress Levels vs. Cholesterol Level)\np &lt;- ggplot(heart_attack_filtered, aes(x = Stress_Levels, y = Cholesterol_Level, color = Heart_Attack_Occurrence)) +\n  geom_point(alpha = 0.8, size = 2) +  \n  geom_smooth(method = \"lm\", se = TRUE, aes(color = Heart_Attack_Occurrence)) +  \n  scale_color_manual(values = c(\"No\" = \"skyblue\", \"Yes\" = \"navy\")) +  \n  labs(\n    title = \"Scatter Plot of Cholesterol Level vs Stress Levels\",\n    subtitle = \"Filtered: Males, Age 26-40, Urban, with Hypertension History\",\n    x = \"Stress Levels\",\n    y = \"Cholesterol Level\",\n    color = \"Heart Attack\"\n  ) +\n  theme_minimal()\n\nggMarginal(p, type = \"boxplot\", groupColour = TRUE, groupFill = TRUE)\n\n\n\n\n\n\n\n\n\nCholesterol levels above 200 are prevalent in heart attack cases.\nNo strong correlation between stress levels and cholesterol, but stress around 5+ is common.\nMajority of heart attack cases cluster around high cholesterol levels (above the 200 threshold).\nStress alone may not be a primary driver, but high cholesterol appears to be a stronger risk factor.\n\n\nGraphCode\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(ggExtra)\nlibrary(ggiraph)  \n\n# Filter dataset: Age 26-40, Urban region, Hypertension History, and Heart Attack = \"Yes\"\nheart_attack_filtered &lt;- heart_attack_3 %&gt;%\n  filter(Age &gt;= 26 & Age &lt;= 40 & Region == \"Urban\" & \n         Hypertension_History == \"Yes\" & Heart_Attack_Occurrence == \"Yes\") %&gt;%  \n  filter(!is.na(Cholesterol_Level))  \n\np &lt;- ggplot(heart_attack_filtered, aes(\n    x = Stress_Levels, \n    y = Cholesterol_Level, \n    color = \"Heart Attack (Yes)\",  # Dummy legend entry\n    tooltip = paste(\"Stress:\", Stress_Levels, \"&lt;br&gt;Cholesterol:\", Cholesterol_Level, \"&lt;br&gt;Heart Attack: Yes\")  \n  )) +\n  geom_point_interactive(alpha = 0.6) +  \n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +  # Regression trendline\n  geom_hline(yintercept = 200, linetype = \"dashed\", color = \"blue\") +  # Horizontal line at Cholesterol = 200\n  geom_vline(xintercept = 5, linetype = \"dotted\", color = \"green\") +   # Vertical line at Stress = 5\n  scale_color_manual(values = c(\"Heart Attack (Yes)\" = \"red\")) +  # Add legend entry\n  labs(\n    title = \"Interactive Scatter Plot of Stress Levels vs. Cholesterol Level\",\n    subtitle = \"Filtered: Age 26-40, Urban, with Hypertension History & Heart Attack Occurrence = Yes\",\n    x = \"Stress Levels\",\n    y = \"Cholesterol Level\",\n    color = \"Heart Attack Status\"  \n  ) +\n  theme_minimal()\n\n# Convert to interactive plot\ninteractive_plot &lt;- girafe(ggobj = p)\n\n# Display the interactive plot\ninteractive_plot"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05.html",
    "href": "In-class_Ex/In-class_Ex05.html",
    "title": "In-class Exercise 05",
    "section": "",
    "text": "Code\npacman::p_load(tidyverse, readxl, SmartEDA, easystats, gtsummary, ggstatsplot)\n\n\n\nreadxl(): Function from the readxl package used to read .xls (Excel 97-2003) files into R.\nSmartEDA(): Automates exploratory data analysis (EDA) by generating summaries, visualizations, and reports for both numerical and categorical variables.\neasystats(): Provides a collection of packages for easy, intuitive, and efficient statistical analysis, including model evaluation, data visualization, and reporting.\n\n\n\n\n\n\nCode\ncar_resale &lt;- read_xls(\"data/ToyotaCorolla.xls\", \n                       \"data\")\ncar_resale\n\n\n# A tibble: 1,436 × 38\n      Id Model    Price Age_08_04 Mfg_Month Mfg_Year     KM Quarterly_Tax Weight\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n 1    81 TOYOTA … 18950        25         8     2002  20019           100   1180\n 2     1 TOYOTA … 13500        23        10     2002  46986           210   1165\n 3     2 TOYOTA … 13750        23        10     2002  72937           210   1165\n 4     3  TOYOTA… 13950        24         9     2002  41711           210   1165\n 5     4 TOYOTA … 14950        26         7     2002  48000           210   1165\n 6     5 TOYOTA … 13750        30         3     2002  38500           210   1170\n 7     6 TOYOTA … 12950        32         1     2002  61000           210   1170\n 8     7  TOYOTA… 16900        27         6     2002  94612           210   1245\n 9     8 TOYOTA … 18600        30         3     2002  75889           210   1245\n10    44 TOYOTA … 16950        27         6     2002 110404           234   1255\n# ℹ 1,426 more rows\n# ℹ 29 more variables: Guarantee_Period &lt;dbl&gt;, HP_Bin &lt;chr&gt;, CC_bin &lt;chr&gt;,\n#   Doors &lt;dbl&gt;, Gears &lt;dbl&gt;, Cylinders &lt;dbl&gt;, Fuel_Type &lt;chr&gt;, Color &lt;chr&gt;,\n#   Met_Color &lt;dbl&gt;, Automatic &lt;dbl&gt;, Mfr_Guarantee &lt;dbl&gt;,\n#   BOVAG_Guarantee &lt;dbl&gt;, ABS &lt;dbl&gt;, Airbag_1 &lt;dbl&gt;, Airbag_2 &lt;dbl&gt;,\n#   Airco &lt;dbl&gt;, Automatic_airco &lt;dbl&gt;, Boardcomputer &lt;dbl&gt;, CD_Player &lt;dbl&gt;,\n#   Central_Lock &lt;dbl&gt;, Powered_Windows &lt;dbl&gt;, Power_Steering &lt;dbl&gt;, …\n\n\n\n\n\n\n\n\n\nCode\nsummary(car_resale)\n\n\n       Id            Model               Price         Age_08_04    \n Min.   :   1.0   Length:1436        Min.   : 4350   Min.   : 1.00  \n 1st Qu.: 361.8   Class :character   1st Qu.: 8450   1st Qu.:44.00  \n Median : 721.5   Mode  :character   Median : 9900   Median :61.00  \n Mean   : 721.6                      Mean   :10731   Mean   :55.95  \n 3rd Qu.:1081.2                      3rd Qu.:11950   3rd Qu.:70.00  \n Max.   :1442.0                      Max.   :32500   Max.   :80.00  \n   Mfg_Month         Mfg_Year          KM         Quarterly_Tax   \n Min.   : 1.000   Min.   :1998   Min.   :     1   Min.   : 19.00  \n 1st Qu.: 3.000   1st Qu.:1998   1st Qu.: 43000   1st Qu.: 69.00  \n Median : 5.000   Median :1999   Median : 63390   Median : 85.00  \n Mean   : 5.549   Mean   :2000   Mean   : 68533   Mean   : 87.12  \n 3rd Qu.: 8.000   3rd Qu.:2001   3rd Qu.: 87021   3rd Qu.: 85.00  \n Max.   :12.000   Max.   :2004   Max.   :243000   Max.   :283.00  \n     Weight     Guarantee_Period    HP_Bin             CC_bin         \n Min.   :1000   Min.   : 3.000   Length:1436        Length:1436       \n 1st Qu.:1040   1st Qu.: 3.000   Class :character   Class :character  \n Median :1070   Median : 3.000   Mode  :character   Mode  :character  \n Mean   :1072   Mean   : 3.815                                        \n 3rd Qu.:1085   3rd Qu.: 3.000                                        \n Max.   :1615   Max.   :36.000                                        \n     Doors           Gears         Cylinders  Fuel_Type        \n Min.   :2.000   Min.   :3.000   Min.   :4   Length:1436       \n 1st Qu.:3.000   1st Qu.:5.000   1st Qu.:4   Class :character  \n Median :4.000   Median :5.000   Median :4   Mode  :character  \n Mean   :4.033   Mean   :5.026   Mean   :4                     \n 3rd Qu.:5.000   3rd Qu.:5.000   3rd Qu.:4                     \n Max.   :5.000   Max.   :6.000   Max.   :4                     \n    Color             Met_Color        Automatic       Mfr_Guarantee   \n Length:1436        Min.   :0.0000   Min.   :0.00000   Min.   :0.0000  \n Class :character   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000  \n Mode  :character   Median :1.0000   Median :0.00000   Median :0.0000  \n                    Mean   :0.6748   Mean   :0.05571   Mean   :0.4095  \n                    3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:1.0000  \n                    Max.   :1.0000   Max.   :1.00000   Max.   :1.0000  \n BOVAG_Guarantee       ABS            Airbag_1         Airbag_2     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.8955   Mean   :0.8134   Mean   :0.9708   Mean   :0.7228  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n     Airco        Automatic_airco   Boardcomputer      CD_Player     \n Min.   :0.0000   Min.   :0.00000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median :0.00000   Median :0.0000   Median :0.0000  \n Mean   :0.5084   Mean   :0.05641   Mean   :0.2946   Mean   :0.2187  \n 3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.00000   Max.   :1.0000   Max.   :1.0000  \n  Central_Lock    Powered_Windows Power_Steering       Radio       \n Min.   :0.0000   Min.   :0.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.000   Median :1.0000   Median :0.0000  \n Mean   :0.5801   Mean   :0.562   Mean   :0.9777   Mean   :0.1462  \n 3rd Qu.:1.0000   3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.000   Max.   :1.0000   Max.   :1.0000  \n   Mistlamps      Sport_Model     Backseat_Divider  Metallic_Rim   \n Min.   :0.000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :0.000   Median :0.0000   Median :1.0000   Median :0.0000  \n Mean   :0.257   Mean   :0.3001   Mean   :0.7702   Mean   :0.2047  \n 3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n Radio_cassette      Tow_Bar      \n Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000  \n Mean   :0.1455   Mean   :0.2779  \n 3rd Qu.:0.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000  \n\n\n\n\n\n\n\nCode\nlist(car_resale)\n\n\n[[1]]\n# A tibble: 1,436 × 38\n      Id Model    Price Age_08_04 Mfg_Month Mfg_Year     KM Quarterly_Tax Weight\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n 1    81 TOYOTA … 18950        25         8     2002  20019           100   1180\n 2     1 TOYOTA … 13500        23        10     2002  46986           210   1165\n 3     2 TOYOTA … 13750        23        10     2002  72937           210   1165\n 4     3  TOYOTA… 13950        24         9     2002  41711           210   1165\n 5     4 TOYOTA … 14950        26         7     2002  48000           210   1165\n 6     5 TOYOTA … 13750        30         3     2002  38500           210   1170\n 7     6 TOYOTA … 12950        32         1     2002  61000           210   1170\n 8     7  TOYOTA… 16900        27         6     2002  94612           210   1245\n 9     8 TOYOTA … 18600        30         3     2002  75889           210   1245\n10    44 TOYOTA … 16950        27         6     2002 110404           234   1255\n# ℹ 1,426 more rows\n# ℹ 29 more variables: Guarantee_Period &lt;dbl&gt;, HP_Bin &lt;chr&gt;, CC_bin &lt;chr&gt;,\n#   Doors &lt;dbl&gt;, Gears &lt;dbl&gt;, Cylinders &lt;dbl&gt;, Fuel_Type &lt;chr&gt;, Color &lt;chr&gt;,\n#   Met_Color &lt;dbl&gt;, Automatic &lt;dbl&gt;, Mfr_Guarantee &lt;dbl&gt;,\n#   BOVAG_Guarantee &lt;dbl&gt;, ABS &lt;dbl&gt;, Airbag_1 &lt;dbl&gt;, Airbag_2 &lt;dbl&gt;,\n#   Airco &lt;dbl&gt;, Automatic_airco &lt;dbl&gt;, Boardcomputer &lt;dbl&gt;, CD_Player &lt;dbl&gt;,\n#   Central_Lock &lt;dbl&gt;, Powered_Windows &lt;dbl&gt;, Power_Steering &lt;dbl&gt;, …\n\n\n\n\n\n\n\nCode\nglimpse(car_resale)\n\n\nRows: 1,436\nColumns: 38\n$ Id               &lt;dbl&gt; 81, 1, 2, 3, 4, 5, 6, 7, 8, 44, 45, 46, 47, 49, 51, 6…\n$ Model            &lt;chr&gt; \"TOYOTA Corolla 1.6 5drs 1 4/5-Doors\", \"TOYOTA Coroll…\n$ Price            &lt;dbl&gt; 18950, 13500, 13750, 13950, 14950, 13750, 12950, 1690…\n$ Age_08_04        &lt;dbl&gt; 25, 23, 23, 24, 26, 30, 32, 27, 30, 27, 22, 23, 27, 2…\n$ Mfg_Month        &lt;dbl&gt; 8, 10, 10, 9, 7, 3, 1, 6, 3, 6, 11, 10, 6, 11, 11, 11…\n$ Mfg_Year         &lt;dbl&gt; 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002,…\n$ KM               &lt;dbl&gt; 20019, 46986, 72937, 41711, 48000, 38500, 61000, 9461…\n$ Quarterly_Tax    &lt;dbl&gt; 100, 210, 210, 210, 210, 210, 210, 210, 210, 234, 234…\n$ Weight           &lt;dbl&gt; 1180, 1165, 1165, 1165, 1165, 1170, 1170, 1245, 1245,…\n$ Guarantee_Period &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,…\n$ HP_Bin           &lt;chr&gt; \"100-120\", \"&lt; 100\", \"&lt; 100\", \"&lt; 100\", \"&lt; 100\", \"&lt; 100…\n$ CC_bin           &lt;chr&gt; \"1600\", \"&gt;1600\", \"&gt;1600\", \"&gt;1600\", \"&gt;1600\", \"&gt;1600\", …\n$ Doors            &lt;dbl&gt; 5, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 3, 3,…\n$ Gears            &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,…\n$ Cylinders        &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,…\n$ Fuel_Type        &lt;chr&gt; \"Petrol\", \"Diesel\", \"Diesel\", \"Diesel\", \"Diesel\", \"Di…\n$ Color            &lt;chr&gt; \"Blue\", \"Blue\", \"Silver\", \"Blue\", \"Black\", \"Black\", \"…\n$ Met_Color        &lt;dbl&gt; 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,…\n$ Automatic        &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Mfr_Guarantee    &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,…\n$ BOVAG_Guarantee  &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ ABS              &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airbag_1         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airbag_2         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airco            &lt;dbl&gt; 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Automatic_airco  &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,…\n$ Boardcomputer    &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ CD_Player        &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,…\n$ Central_Lock     &lt;dbl&gt; 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Powered_Windows  &lt;dbl&gt; 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Power_Steering   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Radio            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Mistlamps        &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Sport_Model      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,…\n$ Backseat_Divider &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Metallic_Rim     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Radio_cassette   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Tow_Bar          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n\n\n\n\n\n\nWhat does SmartEDApackage type means?\n\nType = 1: generates descriptive statistics for numerical and categorical variables, including summaries like mean, median, missing values, and unique counts.\nType = 2: generates data distribution summaries, including frequency tables for categorical variables and histogram/distribution plots for numerical variables.\n\n\n\n\n\nCode\ncar_resale %&gt;%\n  ExpData(type = 1)\n\n\n                                          Descriptions     Value\n1                                   Sample size (nrow)      1436\n2                              No. of variables (ncol)        38\n3                    No. of numeric/interger variables        33\n4                              No. of factor variables         0\n5                                No. of text variables         5\n6                             No. of logical variables         0\n7                          No. of identifier variables         1\n8                                No. of date variables         0\n9             No. of zero variance variables (uniform)         1\n10               %. of variables having complete cases 100% (38)\n11   %. of variables having &gt;0% and &lt;50% missing cases    0% (0)\n12 %. of variables having &gt;=50% and &lt;90% missing cases    0% (0)\n13          %. of variables having &gt;=90% missing cases    0% (0)\n\n\n\n\n\n\n\nCode\ncar_resale %&gt;%\n  ExpData(type = 2)\n\n\n   Index    Variable_Name Variable_Type Sample_n Missing_Count Per_of_Missing\n1      1               Id       numeric     1436             0              0\n2      2            Model     character     1436             0              0\n3      3            Price       numeric     1436             0              0\n4      4        Age_08_04       numeric     1436             0              0\n5      5        Mfg_Month       numeric     1436             0              0\n6      6         Mfg_Year       numeric     1436             0              0\n7      7               KM       numeric     1436             0              0\n8      8    Quarterly_Tax       numeric     1436             0              0\n9      9           Weight       numeric     1436             0              0\n10    10 Guarantee_Period       numeric     1436             0              0\n11    11           HP_Bin     character     1436             0              0\n12    12           CC_bin     character     1436             0              0\n13    13            Doors       numeric     1436             0              0\n14    14            Gears       numeric     1436             0              0\n15    15        Cylinders       numeric     1436             0              0\n16    16        Fuel_Type     character     1436             0              0\n17    17            Color     character     1436             0              0\n18    18        Met_Color       numeric     1436             0              0\n19    19        Automatic       numeric     1436             0              0\n20    20    Mfr_Guarantee       numeric     1436             0              0\n21    21  BOVAG_Guarantee       numeric     1436             0              0\n22    22              ABS       numeric     1436             0              0\n23    23         Airbag_1       numeric     1436             0              0\n24    24         Airbag_2       numeric     1436             0              0\n25    25            Airco       numeric     1436             0              0\n26    26  Automatic_airco       numeric     1436             0              0\n27    27    Boardcomputer       numeric     1436             0              0\n28    28        CD_Player       numeric     1436             0              0\n29    29     Central_Lock       numeric     1436             0              0\n30    30  Powered_Windows       numeric     1436             0              0\n31    31   Power_Steering       numeric     1436             0              0\n32    32            Radio       numeric     1436             0              0\n33    33        Mistlamps       numeric     1436             0              0\n34    34      Sport_Model       numeric     1436             0              0\n35    35 Backseat_Divider       numeric     1436             0              0\n36    36     Metallic_Rim       numeric     1436             0              0\n37    37   Radio_cassette       numeric     1436             0              0\n38    38          Tow_Bar       numeric     1436             0              0\n   No_of_distinct_values\n1                   1436\n2                    372\n3                    236\n4                     77\n5                     12\n6                      7\n7                   1263\n8                     13\n9                     59\n10                     9\n11                     3\n12                     3\n13                     4\n14                     4\n15                     1\n16                     3\n17                    10\n18                     2\n19                     2\n20                     2\n21                     2\n22                     2\n23                     2\n24                     2\n25                     2\n26                     2\n27                     2\n28                     2\n29                     2\n30                     2\n31                     2\n32                     2\n33                     2\n34                     2\n35                     2\n36                     2\n37                     2\n38                     2\n\n\n\n\n\n\nThe below code reads an Excel file, converts the Id column to a character, and transforms all columns in cols into factors.\n\n\nCode\ncols &lt;- c(\"Mfg_Month\", \"HP_Bin\", \"CC_bin\", \"Doors\", \"Gears\", \"Cylinders\", \n          \"Fuel_Type\", \"Color\", \"Met_Color\", \"Automatic\", \"Mfr_Guarantee\", \n          \"BOVAG_Guarantee\", \"ABS\", \"Airbag_1\", \"Airbag_2\", \"Airco\", \n          \"Automatic_airco\", \"Boardcomputer\", \"CD_Player\", \"Central_Lock\", \n          \"Powered_Windows\", \"Power_Steering\", \"Radio\", \"Mistlamps\", \n          \"Sport_Model\", \"Backseat_Divider\", \"Metallic_Rim\", \"Radio_cassette\", \n          \"Tow_Bar\")\n\ncar_resale &lt;- read_xls(\"data/ToyotaCorolla.xls\", \n                       sheet = \"data\") %&gt;%\n  mutate(Id = as.character(Id)) %&gt;%\n  mutate_each_(funs(factor(.)),cols)\n\n\n\n\n\n\n\n\ntarget =  NULL: No target variable is specified, so the function will analyze and visualize all numerical variables independently\n\n\n\nCode\ncar_resale %&gt;%\n  ExpNumViz(target = NULL, \n            nlim = 10, \n            Page = c(2,2))\n\n\n$`0`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntarget =  Price: Function will analyze numerical variables in relation to the “Price” column.\n\n\n\nCode\ncar_resale %&gt;%\n  ExpNumViz((target=\"Price\"),\n            nlim = 10,\n            Page=c(2,2))\n\n\n$`0`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncar_resale %&gt;%\n  ExpCatViz(target=NULL,\n            col = \"light green\",\n            clim=10,\n            margin=2,\n            Page = c(4,4),\n            sample=16)\n\n\n$`0`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nmodel &lt;- lm(Price ~ Age_08_04 + Mfg_Year + KM + \n              Weight + Guarantee_Period, data = car_resale)\nmodel\n\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01  \n\n\n\n\n\n\n\nCode\ncheck_collinearity(model)\n\n\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n               KM 1.46 [ 1.37,  1.57]         1.21      0.68     [0.64, 0.73]\n           Weight 1.41 [ 1.32,  1.51]         1.19      0.71     [0.66, 0.76]\n Guarantee_Period 1.04 [ 1.01,  1.17]         1.02      0.97     [0.86, 0.99]\n\nHigh Correlation\n\n      Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n Age_08_04 31.07 [28.08, 34.38]         5.57      0.03     [0.03, 0.04]\n  Mfg_Year 31.16 [28.16, 34.48]         5.58      0.03     [0.03, 0.04]\n\n\n\n\n\nWhen 1 - 5 : Low collinearity\nWhen 5 - 10: Middle collinearity\nAbove 10: Have high multi collinearity\n\n\n\nCode\ncheck_c &lt;- check_collinearity(model)\nplot(check_c)\n\n\n\n\n\n\n\n\n\n\nRemove the Mfg_Year as it has multi-collinearity\n\n\n\nCode\nmodel1 &lt;- lm(Price ~ Age_08_04 + KM + \n              Weight + Guarantee_Period, data = car_resale)\n\n\n\n\n\n\n\n\nCode\ncheck_n &lt;- check_normality(model1)\nplot(check_n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo check for homogeneity of variances\nBetter to have 2 graphs\n\n\n\nCode\ncheck_h &lt;- check_heteroscedasticity(model1)\n\nplot(check_h)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncheck_model(model1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ntbl_regression(model1, \n               intercept = TRUE) %&gt;%\n  add_glance_source_note(\n    label = list(sigma = \"\\U03c3\"),\n    include = c(\"r.squared\", \"adj.r.squared\",\n                \"AIC\", \"statistic\",\n                \"p.value\", \"sigma\")  \n  )\n\n\n\n\n\n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n-2,186\n-4,093, -278\n0.025\n    Age_08_04\n-119\n-125, -114\n&lt;0.001\n    KM\n-0.02\n-0.03, -0.02\n&lt;0.001\n    Weight\n20\n18, 21\n&lt;0.001\n    Guarantee_Period\n27\n2.1, 52\n0.034\n  \n  \n    \n      R² = 0.849; Adjusted R² = 0.848; AIC = 24,915; Statistic = 2,005; p-value = &lt;0.001; σ = 1,413\n    \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\n\n\n\n\n\nCode\nlibrary(parameters)\nmodel_parameters(model1)\n\n\nParameter        | Coefficient |       SE |              95% CI | t(1431) |      p\n----------------------------------------------------------------------------------\n(Intercept)      |    -2185.52 |   972.19 | [-4092.59, -278.45] |   -2.25 | 0.025 \nAge 08 04        |     -119.49 |     2.76 | [ -124.91, -114.08] |  -43.29 | &lt; .001\nKM               |       -0.02 | 1.20e-03 | [   -0.03,   -0.02] |  -20.04 | &lt; .001\nWeight           |       19.72 |     0.84 | [   18.08,   21.36] |   23.53 | &lt; .001\nGuarantee Period |       26.82 |    12.61 | [    2.08,   51.56] |    2.13 | 0.034 \n\n\n\n\n\n\nCode\np_model1 &lt;- parameters(model1)\n\n\n\n\n\n\n\nCode\nggcoefstats(model1, \n            output = \"plot\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex05.html#getting-started",
    "href": "In-class_Ex/In-class_Ex05.html#getting-started",
    "title": "In-class Exercise 05",
    "section": "",
    "text": "Code\npacman::p_load(tidyverse, readxl, SmartEDA, easystats, gtsummary, ggstatsplot)\n\n\n\nreadxl(): Function from the readxl package used to read .xls (Excel 97-2003) files into R.\nSmartEDA(): Automates exploratory data analysis (EDA) by generating summaries, visualizations, and reports for both numerical and categorical variables.\neasystats(): Provides a collection of packages for easy, intuitive, and efficient statistical analysis, including model evaluation, data visualization, and reporting.\n\n\n\n\n\n\nCode\ncar_resale &lt;- read_xls(\"data/ToyotaCorolla.xls\", \n                       \"data\")\ncar_resale\n\n\n# A tibble: 1,436 × 38\n      Id Model    Price Age_08_04 Mfg_Month Mfg_Year     KM Quarterly_Tax Weight\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n 1    81 TOYOTA … 18950        25         8     2002  20019           100   1180\n 2     1 TOYOTA … 13500        23        10     2002  46986           210   1165\n 3     2 TOYOTA … 13750        23        10     2002  72937           210   1165\n 4     3  TOYOTA… 13950        24         9     2002  41711           210   1165\n 5     4 TOYOTA … 14950        26         7     2002  48000           210   1165\n 6     5 TOYOTA … 13750        30         3     2002  38500           210   1170\n 7     6 TOYOTA … 12950        32         1     2002  61000           210   1170\n 8     7  TOYOTA… 16900        27         6     2002  94612           210   1245\n 9     8 TOYOTA … 18600        30         3     2002  75889           210   1245\n10    44 TOYOTA … 16950        27         6     2002 110404           234   1255\n# ℹ 1,426 more rows\n# ℹ 29 more variables: Guarantee_Period &lt;dbl&gt;, HP_Bin &lt;chr&gt;, CC_bin &lt;chr&gt;,\n#   Doors &lt;dbl&gt;, Gears &lt;dbl&gt;, Cylinders &lt;dbl&gt;, Fuel_Type &lt;chr&gt;, Color &lt;chr&gt;,\n#   Met_Color &lt;dbl&gt;, Automatic &lt;dbl&gt;, Mfr_Guarantee &lt;dbl&gt;,\n#   BOVAG_Guarantee &lt;dbl&gt;, ABS &lt;dbl&gt;, Airbag_1 &lt;dbl&gt;, Airbag_2 &lt;dbl&gt;,\n#   Airco &lt;dbl&gt;, Automatic_airco &lt;dbl&gt;, Boardcomputer &lt;dbl&gt;, CD_Player &lt;dbl&gt;,\n#   Central_Lock &lt;dbl&gt;, Powered_Windows &lt;dbl&gt;, Power_Steering &lt;dbl&gt;, …\n\n\n\n\n\n\n\n\n\nCode\nsummary(car_resale)\n\n\n       Id            Model               Price         Age_08_04    \n Min.   :   1.0   Length:1436        Min.   : 4350   Min.   : 1.00  \n 1st Qu.: 361.8   Class :character   1st Qu.: 8450   1st Qu.:44.00  \n Median : 721.5   Mode  :character   Median : 9900   Median :61.00  \n Mean   : 721.6                      Mean   :10731   Mean   :55.95  \n 3rd Qu.:1081.2                      3rd Qu.:11950   3rd Qu.:70.00  \n Max.   :1442.0                      Max.   :32500   Max.   :80.00  \n   Mfg_Month         Mfg_Year          KM         Quarterly_Tax   \n Min.   : 1.000   Min.   :1998   Min.   :     1   Min.   : 19.00  \n 1st Qu.: 3.000   1st Qu.:1998   1st Qu.: 43000   1st Qu.: 69.00  \n Median : 5.000   Median :1999   Median : 63390   Median : 85.00  \n Mean   : 5.549   Mean   :2000   Mean   : 68533   Mean   : 87.12  \n 3rd Qu.: 8.000   3rd Qu.:2001   3rd Qu.: 87021   3rd Qu.: 85.00  \n Max.   :12.000   Max.   :2004   Max.   :243000   Max.   :283.00  \n     Weight     Guarantee_Period    HP_Bin             CC_bin         \n Min.   :1000   Min.   : 3.000   Length:1436        Length:1436       \n 1st Qu.:1040   1st Qu.: 3.000   Class :character   Class :character  \n Median :1070   Median : 3.000   Mode  :character   Mode  :character  \n Mean   :1072   Mean   : 3.815                                        \n 3rd Qu.:1085   3rd Qu.: 3.000                                        \n Max.   :1615   Max.   :36.000                                        \n     Doors           Gears         Cylinders  Fuel_Type        \n Min.   :2.000   Min.   :3.000   Min.   :4   Length:1436       \n 1st Qu.:3.000   1st Qu.:5.000   1st Qu.:4   Class :character  \n Median :4.000   Median :5.000   Median :4   Mode  :character  \n Mean   :4.033   Mean   :5.026   Mean   :4                     \n 3rd Qu.:5.000   3rd Qu.:5.000   3rd Qu.:4                     \n Max.   :5.000   Max.   :6.000   Max.   :4                     \n    Color             Met_Color        Automatic       Mfr_Guarantee   \n Length:1436        Min.   :0.0000   Min.   :0.00000   Min.   :0.0000  \n Class :character   1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000  \n Mode  :character   Median :1.0000   Median :0.00000   Median :0.0000  \n                    Mean   :0.6748   Mean   :0.05571   Mean   :0.4095  \n                    3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:1.0000  \n                    Max.   :1.0000   Max.   :1.00000   Max.   :1.0000  \n BOVAG_Guarantee       ABS            Airbag_1         Airbag_2     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.0000   Median :1.0000   Median :1.0000  \n Mean   :0.8955   Mean   :0.8134   Mean   :0.9708   Mean   :0.7228  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n     Airco        Automatic_airco   Boardcomputer      CD_Player     \n Min.   :0.0000   Min.   :0.00000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :1.0000   Median :0.00000   Median :0.0000   Median :0.0000  \n Mean   :0.5084   Mean   :0.05641   Mean   :0.2946   Mean   :0.2187  \n 3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.00000   Max.   :1.0000   Max.   :1.0000  \n  Central_Lock    Powered_Windows Power_Steering       Radio       \n Min.   :0.0000   Min.   :0.000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :1.0000   Median :1.000   Median :1.0000   Median :0.0000  \n Mean   :0.5801   Mean   :0.562   Mean   :0.9777   Mean   :0.1462  \n 3rd Qu.:1.0000   3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.000   Max.   :1.0000   Max.   :1.0000  \n   Mistlamps      Sport_Model     Backseat_Divider  Metallic_Rim   \n Min.   :0.000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:1.0000   1st Qu.:0.0000  \n Median :0.000   Median :0.0000   Median :1.0000   Median :0.0000  \n Mean   :0.257   Mean   :0.3001   Mean   :0.7702   Mean   :0.2047  \n 3rd Qu.:1.000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:0.0000  \n Max.   :1.000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n Radio_cassette      Tow_Bar      \n Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000  \n Mean   :0.1455   Mean   :0.2779  \n 3rd Qu.:0.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000  \n\n\n\n\n\n\n\nCode\nlist(car_resale)\n\n\n[[1]]\n# A tibble: 1,436 × 38\n      Id Model    Price Age_08_04 Mfg_Month Mfg_Year     KM Quarterly_Tax Weight\n   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n 1    81 TOYOTA … 18950        25         8     2002  20019           100   1180\n 2     1 TOYOTA … 13500        23        10     2002  46986           210   1165\n 3     2 TOYOTA … 13750        23        10     2002  72937           210   1165\n 4     3  TOYOTA… 13950        24         9     2002  41711           210   1165\n 5     4 TOYOTA … 14950        26         7     2002  48000           210   1165\n 6     5 TOYOTA … 13750        30         3     2002  38500           210   1170\n 7     6 TOYOTA … 12950        32         1     2002  61000           210   1170\n 8     7  TOYOTA… 16900        27         6     2002  94612           210   1245\n 9     8 TOYOTA … 18600        30         3     2002  75889           210   1245\n10    44 TOYOTA … 16950        27         6     2002 110404           234   1255\n# ℹ 1,426 more rows\n# ℹ 29 more variables: Guarantee_Period &lt;dbl&gt;, HP_Bin &lt;chr&gt;, CC_bin &lt;chr&gt;,\n#   Doors &lt;dbl&gt;, Gears &lt;dbl&gt;, Cylinders &lt;dbl&gt;, Fuel_Type &lt;chr&gt;, Color &lt;chr&gt;,\n#   Met_Color &lt;dbl&gt;, Automatic &lt;dbl&gt;, Mfr_Guarantee &lt;dbl&gt;,\n#   BOVAG_Guarantee &lt;dbl&gt;, ABS &lt;dbl&gt;, Airbag_1 &lt;dbl&gt;, Airbag_2 &lt;dbl&gt;,\n#   Airco &lt;dbl&gt;, Automatic_airco &lt;dbl&gt;, Boardcomputer &lt;dbl&gt;, CD_Player &lt;dbl&gt;,\n#   Central_Lock &lt;dbl&gt;, Powered_Windows &lt;dbl&gt;, Power_Steering &lt;dbl&gt;, …\n\n\n\n\n\n\n\nCode\nglimpse(car_resale)\n\n\nRows: 1,436\nColumns: 38\n$ Id               &lt;dbl&gt; 81, 1, 2, 3, 4, 5, 6, 7, 8, 44, 45, 46, 47, 49, 51, 6…\n$ Model            &lt;chr&gt; \"TOYOTA Corolla 1.6 5drs 1 4/5-Doors\", \"TOYOTA Coroll…\n$ Price            &lt;dbl&gt; 18950, 13500, 13750, 13950, 14950, 13750, 12950, 1690…\n$ Age_08_04        &lt;dbl&gt; 25, 23, 23, 24, 26, 30, 32, 27, 30, 27, 22, 23, 27, 2…\n$ Mfg_Month        &lt;dbl&gt; 8, 10, 10, 9, 7, 3, 1, 6, 3, 6, 11, 10, 6, 11, 11, 11…\n$ Mfg_Year         &lt;dbl&gt; 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002,…\n$ KM               &lt;dbl&gt; 20019, 46986, 72937, 41711, 48000, 38500, 61000, 9461…\n$ Quarterly_Tax    &lt;dbl&gt; 100, 210, 210, 210, 210, 210, 210, 210, 210, 234, 234…\n$ Weight           &lt;dbl&gt; 1180, 1165, 1165, 1165, 1165, 1170, 1170, 1245, 1245,…\n$ Guarantee_Period &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,…\n$ HP_Bin           &lt;chr&gt; \"100-120\", \"&lt; 100\", \"&lt; 100\", \"&lt; 100\", \"&lt; 100\", \"&lt; 100…\n$ CC_bin           &lt;chr&gt; \"1600\", \"&gt;1600\", \"&gt;1600\", \"&gt;1600\", \"&gt;1600\", \"&gt;1600\", …\n$ Doors            &lt;dbl&gt; 5, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 3, 3,…\n$ Gears            &lt;dbl&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,…\n$ Cylinders        &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,…\n$ Fuel_Type        &lt;chr&gt; \"Petrol\", \"Diesel\", \"Diesel\", \"Diesel\", \"Diesel\", \"Di…\n$ Color            &lt;chr&gt; \"Blue\", \"Blue\", \"Silver\", \"Blue\", \"Black\", \"Black\", \"…\n$ Met_Color        &lt;dbl&gt; 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,…\n$ Automatic        &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Mfr_Guarantee    &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,…\n$ BOVAG_Guarantee  &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ ABS              &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airbag_1         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airbag_2         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Airco            &lt;dbl&gt; 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Automatic_airco  &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,…\n$ Boardcomputer    &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ CD_Player        &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,…\n$ Central_Lock     &lt;dbl&gt; 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Powered_Windows  &lt;dbl&gt; 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Power_Steering   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Radio            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Mistlamps        &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Sport_Model      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,…\n$ Backseat_Divider &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Metallic_Rim     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Radio_cassette   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Tow_Bar          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n\n\n\n\n\n\nWhat does SmartEDApackage type means?\n\nType = 1: generates descriptive statistics for numerical and categorical variables, including summaries like mean, median, missing values, and unique counts.\nType = 2: generates data distribution summaries, including frequency tables for categorical variables and histogram/distribution plots for numerical variables.\n\n\n\n\n\nCode\ncar_resale %&gt;%\n  ExpData(type = 1)\n\n\n                                          Descriptions     Value\n1                                   Sample size (nrow)      1436\n2                              No. of variables (ncol)        38\n3                    No. of numeric/interger variables        33\n4                              No. of factor variables         0\n5                                No. of text variables         5\n6                             No. of logical variables         0\n7                          No. of identifier variables         1\n8                                No. of date variables         0\n9             No. of zero variance variables (uniform)         1\n10               %. of variables having complete cases 100% (38)\n11   %. of variables having &gt;0% and &lt;50% missing cases    0% (0)\n12 %. of variables having &gt;=50% and &lt;90% missing cases    0% (0)\n13          %. of variables having &gt;=90% missing cases    0% (0)\n\n\n\n\n\n\n\nCode\ncar_resale %&gt;%\n  ExpData(type = 2)\n\n\n   Index    Variable_Name Variable_Type Sample_n Missing_Count Per_of_Missing\n1      1               Id       numeric     1436             0              0\n2      2            Model     character     1436             0              0\n3      3            Price       numeric     1436             0              0\n4      4        Age_08_04       numeric     1436             0              0\n5      5        Mfg_Month       numeric     1436             0              0\n6      6         Mfg_Year       numeric     1436             0              0\n7      7               KM       numeric     1436             0              0\n8      8    Quarterly_Tax       numeric     1436             0              0\n9      9           Weight       numeric     1436             0              0\n10    10 Guarantee_Period       numeric     1436             0              0\n11    11           HP_Bin     character     1436             0              0\n12    12           CC_bin     character     1436             0              0\n13    13            Doors       numeric     1436             0              0\n14    14            Gears       numeric     1436             0              0\n15    15        Cylinders       numeric     1436             0              0\n16    16        Fuel_Type     character     1436             0              0\n17    17            Color     character     1436             0              0\n18    18        Met_Color       numeric     1436             0              0\n19    19        Automatic       numeric     1436             0              0\n20    20    Mfr_Guarantee       numeric     1436             0              0\n21    21  BOVAG_Guarantee       numeric     1436             0              0\n22    22              ABS       numeric     1436             0              0\n23    23         Airbag_1       numeric     1436             0              0\n24    24         Airbag_2       numeric     1436             0              0\n25    25            Airco       numeric     1436             0              0\n26    26  Automatic_airco       numeric     1436             0              0\n27    27    Boardcomputer       numeric     1436             0              0\n28    28        CD_Player       numeric     1436             0              0\n29    29     Central_Lock       numeric     1436             0              0\n30    30  Powered_Windows       numeric     1436             0              0\n31    31   Power_Steering       numeric     1436             0              0\n32    32            Radio       numeric     1436             0              0\n33    33        Mistlamps       numeric     1436             0              0\n34    34      Sport_Model       numeric     1436             0              0\n35    35 Backseat_Divider       numeric     1436             0              0\n36    36     Metallic_Rim       numeric     1436             0              0\n37    37   Radio_cassette       numeric     1436             0              0\n38    38          Tow_Bar       numeric     1436             0              0\n   No_of_distinct_values\n1                   1436\n2                    372\n3                    236\n4                     77\n5                     12\n6                      7\n7                   1263\n8                     13\n9                     59\n10                     9\n11                     3\n12                     3\n13                     4\n14                     4\n15                     1\n16                     3\n17                    10\n18                     2\n19                     2\n20                     2\n21                     2\n22                     2\n23                     2\n24                     2\n25                     2\n26                     2\n27                     2\n28                     2\n29                     2\n30                     2\n31                     2\n32                     2\n33                     2\n34                     2\n35                     2\n36                     2\n37                     2\n38                     2\n\n\n\n\n\n\nThe below code reads an Excel file, converts the Id column to a character, and transforms all columns in cols into factors.\n\n\nCode\ncols &lt;- c(\"Mfg_Month\", \"HP_Bin\", \"CC_bin\", \"Doors\", \"Gears\", \"Cylinders\", \n          \"Fuel_Type\", \"Color\", \"Met_Color\", \"Automatic\", \"Mfr_Guarantee\", \n          \"BOVAG_Guarantee\", \"ABS\", \"Airbag_1\", \"Airbag_2\", \"Airco\", \n          \"Automatic_airco\", \"Boardcomputer\", \"CD_Player\", \"Central_Lock\", \n          \"Powered_Windows\", \"Power_Steering\", \"Radio\", \"Mistlamps\", \n          \"Sport_Model\", \"Backseat_Divider\", \"Metallic_Rim\", \"Radio_cassette\", \n          \"Tow_Bar\")\n\ncar_resale &lt;- read_xls(\"data/ToyotaCorolla.xls\", \n                       sheet = \"data\") %&gt;%\n  mutate(Id = as.character(Id)) %&gt;%\n  mutate_each_(funs(factor(.)),cols)\n\n\n\n\n\n\n\n\ntarget =  NULL: No target variable is specified, so the function will analyze and visualize all numerical variables independently\n\n\n\nCode\ncar_resale %&gt;%\n  ExpNumViz(target = NULL, \n            nlim = 10, \n            Page = c(2,2))\n\n\n$`0`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntarget =  Price: Function will analyze numerical variables in relation to the “Price” column.\n\n\n\nCode\ncar_resale %&gt;%\n  ExpNumViz((target=\"Price\"),\n            nlim = 10,\n            Page=c(2,2))\n\n\n$`0`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncar_resale %&gt;%\n  ExpCatViz(target=NULL,\n            col = \"light green\",\n            clim=10,\n            margin=2,\n            Page = c(4,4),\n            sample=16)\n\n\n$`0`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nmodel &lt;- lm(Price ~ Age_08_04 + Mfg_Year + KM + \n              Weight + Guarantee_Period, data = car_resale)\nmodel\n\n\n\nCall:\nlm(formula = Price ~ Age_08_04 + Mfg_Year + KM + Weight + Guarantee_Period, \n    data = car_resale)\n\nCoefficients:\n     (Intercept)         Age_08_04          Mfg_Year                KM  \n      -2.637e+06        -1.409e+01         1.315e+03        -2.323e-02  \n          Weight  Guarantee_Period  \n       1.903e+01         2.770e+01  \n\n\n\n\n\n\n\nCode\ncheck_collinearity(model)\n\n\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term  VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n               KM 1.46 [ 1.37,  1.57]         1.21      0.68     [0.64, 0.73]\n           Weight 1.41 [ 1.32,  1.51]         1.19      0.71     [0.66, 0.76]\n Guarantee_Period 1.04 [ 1.01,  1.17]         1.02      0.97     [0.86, 0.99]\n\nHigh Correlation\n\n      Term   VIF     VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n Age_08_04 31.07 [28.08, 34.38]         5.57      0.03     [0.03, 0.04]\n  Mfg_Year 31.16 [28.16, 34.48]         5.58      0.03     [0.03, 0.04]\n\n\n\n\n\nWhen 1 - 5 : Low collinearity\nWhen 5 - 10: Middle collinearity\nAbove 10: Have high multi collinearity\n\n\n\nCode\ncheck_c &lt;- check_collinearity(model)\nplot(check_c)\n\n\n\n\n\n\n\n\n\n\nRemove the Mfg_Year as it has multi-collinearity\n\n\n\nCode\nmodel1 &lt;- lm(Price ~ Age_08_04 + KM + \n              Weight + Guarantee_Period, data = car_resale)\n\n\n\n\n\n\n\n\nCode\ncheck_n &lt;- check_normality(model1)\nplot(check_n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo check for homogeneity of variances\nBetter to have 2 graphs\n\n\n\nCode\ncheck_h &lt;- check_heteroscedasticity(model1)\n\nplot(check_h)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncheck_model(model1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ntbl_regression(model1, \n               intercept = TRUE) %&gt;%\n  add_glance_source_note(\n    label = list(sigma = \"\\U03c3\"),\n    include = c(\"r.squared\", \"adj.r.squared\",\n                \"AIC\", \"statistic\",\n                \"p.value\", \"sigma\")  \n  )\n\n\n\n\n\n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n-2,186\n-4,093, -278\n0.025\n    Age_08_04\n-119\n-125, -114\n&lt;0.001\n    KM\n-0.02\n-0.03, -0.02\n&lt;0.001\n    Weight\n20\n18, 21\n&lt;0.001\n    Guarantee_Period\n27\n2.1, 52\n0.034\n  \n  \n    \n      R² = 0.849; Adjusted R² = 0.848; AIC = 24,915; Statistic = 2,005; p-value = &lt;0.001; σ = 1,413\n    \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\n\n\n\n\n\nCode\nlibrary(parameters)\nmodel_parameters(model1)\n\n\nParameter        | Coefficient |       SE |              95% CI | t(1431) |      p\n----------------------------------------------------------------------------------\n(Intercept)      |    -2185.52 |   972.19 | [-4092.59, -278.45] |   -2.25 | 0.025 \nAge 08 04        |     -119.49 |     2.76 | [ -124.91, -114.08] |  -43.29 | &lt; .001\nKM               |       -0.02 | 1.20e-03 | [   -0.03,   -0.02] |  -20.04 | &lt; .001\nWeight           |       19.72 |     0.84 | [   18.08,   21.36] |   23.53 | &lt; .001\nGuarantee Period |       26.82 |    12.61 | [    2.08,   51.56] |    2.13 | 0.034 \n\n\n\n\n\n\nCode\np_model1 &lt;- parameters(model1)\n\n\n\n\n\n\n\nCode\nggcoefstats(model1, \n            output = \"plot\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#code-6",
    "href": "Take-home_Ex/Take-home_Ex01.html#code-6",
    "title": "Take-home Exercise 01",
    "section": "",
    "text": "Code\n#- DO NOT DELETE\n#| eval = FALSE \n\n# Load necessary libraries\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(stats)\n\n# Select relevant health-related columns\nrelevant_factors &lt;- heart_attack_2 %&gt;% \n  select(\"Gender\", \"Region\", \"Smoking_History\", \"Diabetes_History\", \"Hypertension_History\", \"Physical_Activity\",\n         \"Diet_Quality\", \"Alcohol_Consumption\", \"Family_History\", \"Cholesterol_Level_Category\", \"Heart_Attack_Occurrence\", \n         \"Stress_Levels_Category\", \"BMI_Category\", \"Heart_Rate_Category\", \"Systolic_BP_Category\", \"Diastolic_BP_Category\")\n\n# Convert categorical variables\n# Gender encoding\nrelevant_factors$Gender &lt;- factor(relevant_factors$Gender, levels = c(\"Male\", \"Female\"))\n\n# Region encoding\nrelevant_factors$Region &lt;- factor(relevant_factors$Region, levels = c(\"Rural\", \"Urban\"))\n\n# Smoking_History encoding\nrelevant_factors$Smoking_History &lt;- factor(relevant_factors$Smoking_History, levels = c(\"Yes\", \"No\"))\n\n# Diabetes_History encoding\nrelevant_factors$Diabetes_History &lt;- factor(relevant_factors$Diabetes_History, levels = c(\"Yes\", \"No\"))\n\n# Hypertension_History encoding\nrelevant_factors$Hypertension_History &lt;- factor(relevant_factors$Hypertension_History, levels = c(\"Yes\", \"No\"))\n\n# Physical_Activity is now numeric (Low = 0, Moderate = 1, High = 2)\nrelevant_factors$Physical_Activity &lt;- recode(relevant_factors$Physical_Activity, \"Low\" = 0, \"Moderate\" = 1, \"High\" = 2) %&gt;% as.numeric()\n\n# Diet_Quality is now numeric (Poor = 0, Average = 1, Good = 2)\nrelevant_factors$Diet_Quality &lt;- recode(relevant_factors$Diet_Quality, \"Poor\" = 0, \"Average\" = 1, \"Good\" = 2) %&gt;% as.numeric()\n\n# Alcohol_Consumption is now numeric (None = 0, Low = 0, Moderate = 1, High = 2)\nrelevant_factors$Alcohol_Consumption &lt;- recode(relevant_factors$Alcohol_Consumption, \"None\" = 0, \"Low\" = 0, \"Moderate\" = 1, \"High\" = 2) %&gt;% as.numeric()\n\n# Family_History encoding\nrelevant_factors$Family_History &lt;- factor(relevant_factors$Family_History, levels = c(\"Yes\", \"No\"))\n\n# Cholesterol_Level_Category is now numeric (Low = 0, Moderate = 1, High = 2)\nrelevant_factors$Cholesterol_Level_Category &lt;- recode(relevant_factors$Cholesterol_Level_Category, \"Low\" = 0, \"Moderate\" = 1, \"High\" = 2) %&gt;% as.numeric()\n\n# Stress_Levels_Category is now numeric (Minimal Stress = 0, Low Stress = 0, Moderate Stress = 1, High Stress = 2)\nrelevant_factors$Stress_Levels_Category &lt;- recode(relevant_factors$Stress_Levels_Category,\n  \"Miniminal_Stress\" = 0,\n  \"Low_Stress\" = 0,\n  \"Moderate_Stress\" = 1,\n  \"High_Stress\" = 2\n) %&gt;% as.numeric()\n\n# BMI_Category is now numeric (Underweight = 0, Normal_Weight = 1, Overweight = 2, Obese = 2)\nrelevant_factors$BMI_Category &lt;- recode(relevant_factors$BMI_Category, \"Underweight\" = 0, \"Normal_Weight\" = 1, \"Overweight\" = 2, \"Obese\" = 2) %&gt;% as.numeric()\n\n# Heart_Rate_Category is now numeric (Low = 0, Normal = 1, High = 2)\nrelevant_factors$Heart_Rate_Category &lt;- recode(relevant_factors$Heart_Rate_Category, \"Low\" = 0, \"Normal\" = 1, \"High\" = 2) %&gt;% as.numeric()\n\n# Systolic_BP_Category is now numeric (Normal = 0, Elevated = 1, Hypertension_Stage_1 = 2, Hypertension_Stage_2 = 2)\nrelevant_factors$Systolic_BP_Category &lt;- recode(relevant_factors$Systolic_BP_Category, \"Normal\" = 0, \"Elevated\" = 1, \"Hypertension_Stage_1\" = 2, \"Hypertension_Stage_2\" = 2) %&gt;% as.numeric()\n\n# Diastolic_BP_Category is now numeric (Normal = 0, Elevated = 1, Hypertension_Stage_1 = 2, Hypertension_Stage_2 = 2)\nrelevant_factors$Diastolic_BP_Category &lt;- recode(relevant_factors$Diastolic_BP_Category, \"Normal\" = 0, \"Elevated\" = 1, \"Hypertension_Stage_1\" = 2, \"Hypertension_Stage_2\" = 2) %&gt;% as.numeric()\n\n# Convert Heart_Attack_Occurrence to binary and ensure it remains a factor\nrelevant_factors$Heart_Attack_Occurrence &lt;- factor(ifelse(relevant_factors$Heart_Attack_Occurrence == \"Yes\", 1, 0), levels = c(0, 1))\n\n# Compute correlation matrix for numerical encoding\ncor_matrix &lt;- relevant_factors %&gt;% \n  mutate(across(where(is.factor), as.numeric)) %&gt;%\n  select(-Smoking_History, -Heart_Attack_Occurrence) %&gt;%\n  cor(use = \"complete.obs\")\n\nprint(cor_matrix)\n\n\n                                  Gender        Region Diabetes_History\nGender                      1.0000000000  0.0018273319    -0.0061491998\nRegion                      0.0018273319  1.0000000000    -0.0068285141\nDiabetes_History           -0.0061491998 -0.0068285141     1.0000000000\nHypertension_History       -0.0065795484  0.0096061837    -0.0110992995\nPhysical_Activity          -0.0076236037  0.0009938000    -0.0020764022\nDiet_Quality                0.0071961731 -0.0055712476    -0.0084098287\nAlcohol_Consumption        -0.0033616763  0.0048247117    -0.0005921596\nFamily_History              0.0153129606  0.0053180175    -0.0029149678\nCholesterol_Level_Category -0.0052850774 -0.0013870042     0.0042231518\nStress_Levels_Category     -0.0040980330  0.0006873657     0.0055567649\nBMI_Category               -0.0029024904 -0.0076272789     0.0113091595\nHeart_Rate_Category         0.0004961952 -0.0038853995    -0.0077468391\nSystolic_BP_Category       -0.0087429797  0.0037365868     0.0100145852\nDiastolic_BP_Category       0.0064315430  0.0023891323    -0.0070725390\n                           Hypertension_History Physical_Activity  Diet_Quality\nGender                            -0.0065795484     -0.0076236037  7.196173e-03\nRegion                             0.0096061837      0.0009938000 -5.571248e-03\nDiabetes_History                  -0.0110992995     -0.0020764022 -8.409829e-03\nHypertension_History               1.0000000000     -0.0076184686  1.603881e-03\nPhysical_Activity                 -0.0076184686      1.0000000000  8.475932e-03\nDiet_Quality                       0.0016038813      0.0084759322  1.000000e+00\nAlcohol_Consumption               -0.0006852643     -0.0030454964 -9.333696e-05\nFamily_History                    -0.0005563115     -0.0059558012  1.764614e-03\nCholesterol_Level_Category         0.0043603722     -0.0007832206 -5.139028e-03\nStress_Levels_Category             0.0042212032     -0.0045040665  3.973753e-03\nBMI_Category                      -0.0054824465     -0.0064385995  1.152475e-03\nHeart_Rate_Category               -0.0045361879     -0.0088025471 -1.148583e-03\nSystolic_BP_Category               0.0017269165     -0.0006363427 -9.383858e-03\nDiastolic_BP_Category              0.0048579096      0.0043700013 -1.982442e-03\n                           Alcohol_Consumption Family_History\nGender                           -3.361676e-03   0.0153129606\nRegion                            4.824712e-03   0.0053180175\nDiabetes_History                 -5.921596e-04  -0.0029149678\nHypertension_History             -6.852643e-04  -0.0005563115\nPhysical_Activity                -3.045496e-03  -0.0059558012\nDiet_Quality                     -9.333696e-05   0.0017646142\nAlcohol_Consumption               1.000000e+00   0.0008722224\nFamily_History                    8.722224e-04   1.0000000000\nCholesterol_Level_Category       -3.263919e-03   0.0013108137\nStress_Levels_Category           -9.775957e-03   0.0050022995\nBMI_Category                     -1.896134e-03  -0.0069851954\nHeart_Rate_Category               9.340584e-03   0.0014048223\nSystolic_BP_Category              8.639608e-04  -0.0071712444\nDiastolic_BP_Category             4.680930e-03   0.0034817498\n                           Cholesterol_Level_Category Stress_Levels_Category\nGender                                  -0.0052850774          -4.098033e-03\nRegion                                  -0.0013870042           6.873657e-04\nDiabetes_History                         0.0042231518           5.556765e-03\nHypertension_History                     0.0043603722           4.221203e-03\nPhysical_Activity                       -0.0007832206          -4.504067e-03\nDiet_Quality                            -0.0051390283           3.973753e-03\nAlcohol_Consumption                     -0.0032639192          -9.775957e-03\nFamily_History                           0.0013108137           5.002300e-03\nCholesterol_Level_Category               1.0000000000          -1.612134e-03\nStress_Levels_Category                  -0.0016121344           1.000000e+00\nBMI_Category                             0.0001787723          -4.912493e-04\nHeart_Rate_Category                      0.0078659486          -5.505661e-03\nSystolic_BP_Category                     0.0043199306          -3.973247e-05\nDiastolic_BP_Category                   -0.0021550677           3.230604e-03\n                            BMI_Category Heart_Rate_Category\nGender                     -0.0029024904        0.0004961952\nRegion                     -0.0076272789       -0.0038853995\nDiabetes_History            0.0113091595       -0.0077468391\nHypertension_History       -0.0054824465       -0.0045361879\nPhysical_Activity          -0.0064385995       -0.0088025471\nDiet_Quality                0.0011524752       -0.0011485829\nAlcohol_Consumption        -0.0018961340        0.0093405843\nFamily_History             -0.0069851954        0.0014048223\nCholesterol_Level_Category  0.0001787723        0.0078659486\nStress_Levels_Category     -0.0004912493       -0.0055056614\nBMI_Category                1.0000000000        0.0028365599\nHeart_Rate_Category         0.0028365599        1.0000000000\nSystolic_BP_Category       -0.0115940877        0.0008703383\nDiastolic_BP_Category       0.0020819695       -0.0011852861\n                           Systolic_BP_Category Diastolic_BP_Category\nGender                            -8.742980e-03           0.006431543\nRegion                             3.736587e-03           0.002389132\nDiabetes_History                   1.001459e-02          -0.007072539\nHypertension_History               1.726916e-03           0.004857910\nPhysical_Activity                 -6.363427e-04           0.004370001\nDiet_Quality                      -9.383858e-03          -0.001982442\nAlcohol_Consumption                8.639608e-04           0.004680930\nFamily_History                    -7.171244e-03           0.003481750\nCholesterol_Level_Category         4.319931e-03          -0.002155068\nStress_Levels_Category            -3.973247e-05           0.003230604\nBMI_Category                      -1.159409e-02           0.002081970\nHeart_Rate_Category                8.703383e-04          -0.001185286\nSystolic_BP_Category               1.000000e+00           0.009991349\nDiastolic_BP_Category              9.991349e-03           1.000000000\n\n\nCode\n# Run Logistic Regression Model\nlogistic_model &lt;- glm(Heart_Attack_Occurrence ~ ., data = relevant_factors, family = binomial())\n\n# Print Model Summary\nsummary(logistic_model)\n\n\n\nCall:\nglm(formula = Heart_Attack_Occurrence ~ ., family = binomial(), \n    data = relevant_factors)\n\nCoefficients:\n                             Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)                -2.657e+01  1.328e+04  -0.002    0.998\nGenderFemale                5.020e-13  4.113e+03   0.000    1.000\nRegionUrban                -3.541e-13  4.496e+03   0.000    1.000\nSmoking_HistoryNo           8.318e-13  4.488e+03   0.000    1.000\nDiabetes_HistoryNo         -3.122e-13  5.111e+03   0.000    1.000\nHypertension_HistoryNo     -3.399e-13  4.759e+03   0.000    1.000\nPhysical_Activity          -4.815e-15  2.649e+03   0.000    1.000\nDiet_Quality                5.352e-13  2.745e+03   0.000    1.000\nAlcohol_Consumption         3.620e-13  2.762e+03   0.000    1.000\nFamily_HistoryNo           -3.726e-13  4.497e+03   0.000    1.000\nCholesterol_Level_Category  3.330e-13  3.515e+03   0.000    1.000\nStress_Levels_Category     -9.302e-14  3.649e+03   0.000    1.000\nBMI_Category               -3.487e-13  3.129e+03   0.000    1.000\nHeart_Rate_Category        -3.048e-13  5.641e+03   0.000    1.000\nSystolic_BP_Category       -8.970e-14  2.477e+03   0.000    1.000\nDiastolic_BP_Category      -1.603e-13  2.799e+03   0.000    1.000\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 0.0000e+00  on 29999  degrees of freedom\nResidual deviance: 1.7405e-07  on 29984  degrees of freedom\nAIC: 32\n\nNumber of Fisher Scoring iterations: 25\n\n\n:::\n\n\nCode\n# Generate correlation matrix plot using enhanced styling\nggstatsplot::ggcorrmat(\n  data = relevant_factors, \n  cor.vars = 1:ncol(relevant_factors), \n  ggcorrplot.args = list(\n    outline.color = \"black\",  # Add black outline for clarity\n    hc.order = TRUE,          # Hierarchical clustering to group correlated variables\n    tl.cex = 10               # Adjust text size for readability\n  ),\n  title    = \"Correlogram for Heart Attack Risk Factors\",\n  subtitle = \"Significance level: p &lt; 0.05\"\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Load necessary library\nlibrary(writexl)  # Or use openxlsx for more features\n\n# Define file path to save the Excel file\nfile_path &lt;- \"C:/andreaysh/ISSS608/try.xlsx\"\n\n# Export chi-square test results to Excel\nwrite_xlsx(heart_attack_2, path = file_path)\n\n# Confirm export\nprint(paste(\"File saved to:\", file_path))\n\n\n[1] \"File saved to: C:/andreaysh/ISSS608/try.xlsx\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCh"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#code-7",
    "href": "Take-home_Ex/Take-home_Ex01.html#code-7",
    "title": "Take-home Exercise 01",
    "section": "",
    "text": "Code\n#- DO NOT DELETE\n#| eval = FALSE \n\n# Load necessary libraries\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(stats)\n\n# Select relevant health-related columns\nrelevant_factors &lt;- heart_attack_2 %&gt;% \n  select(\"Gender\", \"Region\", \"Smoking_History\", \"Diabetes_History\", \"Hypertension_History\", \"Physical_Activity\",\n         \"Diet_Quality\", \"Alcohol_Consumption\", \"Family_History\", \"Cholesterol_Level_Category\", \"Heart_Attack_Occurrence\", \n         \"Stress_Levels_Category\", \"BMI_Category\", \"Heart_Rate_Category\", \"Systolic_BP_Category\", \"Diastolic_BP_Category\")\n\n# Convert categorical variables\n# Gender encoding\nrelevant_factors$Gender &lt;- factor(relevant_factors$Gender, levels = c(\"Male\", \"Female\"))\n\n# Region encoding\nrelevant_factors$Region &lt;- factor(relevant_factors$Region, levels = c(\"Rural\", \"Urban\"))\n\n# Smoking_History encoding\nrelevant_factors$Smoking_History &lt;- factor(relevant_factors$Smoking_History, levels = c(\"Yes\", \"No\"))\n\n# Diabetes_History encoding\nrelevant_factors$Diabetes_History &lt;- factor(relevant_factors$Diabetes_History, levels = c(\"Yes\", \"No\"))\n\n# Hypertension_History encoding\nrelevant_factors$Hypertension_History &lt;- factor(relevant_factors$Hypertension_History, levels = c(\"Yes\", \"No\"))\n\n# Physical_Activity is now numeric (Low = 0, Moderate = 1, High = 2)\nrelevant_factors$Physical_Activity &lt;- recode(relevant_factors$Physical_Activity, \"Low\" = 0, \"Moderate\" = 1, \"High\" = 2) %&gt;% as.numeric()\n\n# Diet_Quality is now numeric (Poor = 0, Average = 1, Good = 2)\nrelevant_factors$Diet_Quality &lt;- recode(relevant_factors$Diet_Quality, \"Poor\" = 0, \"Average\" = 1, \"Good\" = 2) %&gt;% as.numeric()\n\n# Alcohol_Consumption is now numeric (None = 0, Low = 0, Moderate = 1, High = 2)\nrelevant_factors$Alcohol_Consumption &lt;- recode(relevant_factors$Alcohol_Consumption, \"None\" = 0, \"Low\" = 0, \"Moderate\" = 1, \"High\" = 2) %&gt;% as.numeric()\n\n# Family_History encoding\nrelevant_factors$Family_History &lt;- factor(relevant_factors$Family_History, levels = c(\"Yes\", \"No\"))\n\n# Cholesterol_Level_Category is now numeric (Low = 0, Moderate = 1, High = 2)\nrelevant_factors$Cholesterol_Level_Category &lt;- recode(relevant_factors$Cholesterol_Level_Category, \"Low\" = 0, \"Moderate\" = 1, \"High\" = 2) %&gt;% as.numeric()\n\n# Stress_Levels_Category is now numeric (Minimal Stress = 0, Low Stress = 0, Moderate Stress = 1, High Stress = 2)\nrelevant_factors$Stress_Levels_Category &lt;- recode(relevant_factors$Stress_Levels_Category,\n  \"Miniminal_Stress\" = 0,\n  \"Low_Stress\" = 0,\n  \"Moderate_Stress\" = 1,\n  \"High_Stress\" = 2\n) %&gt;% as.numeric()\n\n# BMI_Category is now numeric (Underweight = 0, Normal_Weight = 1, Overweight = 2, Obese = 2)\nrelevant_factors$BMI_Category &lt;- recode(relevant_factors$BMI_Category, \"Underweight\" = 0, \"Normal_Weight\" = 1, \"Overweight\" = 2, \"Obese\" = 2) %&gt;% as.numeric()\n\n# Heart_Rate_Category is now numeric (Low = 0, Normal = 1, High = 2)\nrelevant_factors$Heart_Rate_Category &lt;- recode(relevant_factors$Heart_Rate_Category, \"Low\" = 0, \"Normal\" = 1, \"High\" = 2) %&gt;% as.numeric()\n\n# Systolic_BP_Category is now numeric (Normal = 0, Elevated = 1, Hypertension_Stage_1 = 2, Hypertension_Stage_2 = 2)\nrelevant_factors$Systolic_BP_Category &lt;- recode(relevant_factors$Systolic_BP_Category, \"Normal\" = 0, \"Elevated\" = 1, \"Hypertension_Stage_1\" = 2, \"Hypertension_Stage_2\" = 2) %&gt;% as.numeric()\n\n# Diastolic_BP_Category is now numeric (Normal = 0, Elevated = 1, Hypertension_Stage_1 = 2, Hypertension_Stage_2 = 2)\nrelevant_factors$Diastolic_BP_Category &lt;- recode(relevant_factors$Diastolic_BP_Category, \"Normal\" = 0, \"Elevated\" = 1, \"Hypertension_Stage_1\" = 2, \"Hypertension_Stage_2\" = 2) %&gt;% as.numeric()\n\n# Convert Heart_Attack_Occurrence to binary and ensure it remains a factor\nrelevant_factors$Heart_Attack_Occurrence &lt;- factor(ifelse(relevant_factors$Heart_Attack_Occurrence == \"Yes\", 1, 0), levels = c(0, 1))\n\n# Compute correlation matrix for numerical encoding\ncor_matrix &lt;- relevant_factors %&gt;% \n  mutate(across(where(is.factor), as.numeric)) %&gt;%\n  select(-Smoking_History, -Heart_Attack_Occurrence) %&gt;%\n  cor(use = \"complete.obs\")\n\nprint(cor_matrix)\n\n\n                                  Gender        Region Diabetes_History\nGender                      1.0000000000  0.0018273319    -0.0061491998\nRegion                      0.0018273319  1.0000000000    -0.0068285141\nDiabetes_History           -0.0061491998 -0.0068285141     1.0000000000\nHypertension_History       -0.0065795484  0.0096061837    -0.0110992995\nPhysical_Activity          -0.0076236037  0.0009938000    -0.0020764022\nDiet_Quality                0.0071961731 -0.0055712476    -0.0084098287\nAlcohol_Consumption        -0.0033616763  0.0048247117    -0.0005921596\nFamily_History              0.0153129606  0.0053180175    -0.0029149678\nCholesterol_Level_Category -0.0052850774 -0.0013870042     0.0042231518\nStress_Levels_Category     -0.0040980330  0.0006873657     0.0055567649\nBMI_Category               -0.0029024904 -0.0076272789     0.0113091595\nHeart_Rate_Category         0.0004961952 -0.0038853995    -0.0077468391\nSystolic_BP_Category       -0.0087429797  0.0037365868     0.0100145852\nDiastolic_BP_Category       0.0064315430  0.0023891323    -0.0070725390\n                           Hypertension_History Physical_Activity  Diet_Quality\nGender                            -0.0065795484     -0.0076236037  7.196173e-03\nRegion                             0.0096061837      0.0009938000 -5.571248e-03\nDiabetes_History                  -0.0110992995     -0.0020764022 -8.409829e-03\nHypertension_History               1.0000000000     -0.0076184686  1.603881e-03\nPhysical_Activity                 -0.0076184686      1.0000000000  8.475932e-03\nDiet_Quality                       0.0016038813      0.0084759322  1.000000e+00\nAlcohol_Consumption               -0.0006852643     -0.0030454964 -9.333696e-05\nFamily_History                    -0.0005563115     -0.0059558012  1.764614e-03\nCholesterol_Level_Category         0.0043603722     -0.0007832206 -5.139028e-03\nStress_Levels_Category             0.0042212032     -0.0045040665  3.973753e-03\nBMI_Category                      -0.0054824465     -0.0064385995  1.152475e-03\nHeart_Rate_Category               -0.0045361879     -0.0088025471 -1.148583e-03\nSystolic_BP_Category               0.0017269165     -0.0006363427 -9.383858e-03\nDiastolic_BP_Category              0.0048579096      0.0043700013 -1.982442e-03\n                           Alcohol_Consumption Family_History\nGender                           -3.361676e-03   0.0153129606\nRegion                            4.824712e-03   0.0053180175\nDiabetes_History                 -5.921596e-04  -0.0029149678\nHypertension_History             -6.852643e-04  -0.0005563115\nPhysical_Activity                -3.045496e-03  -0.0059558012\nDiet_Quality                     -9.333696e-05   0.0017646142\nAlcohol_Consumption               1.000000e+00   0.0008722224\nFamily_History                    8.722224e-04   1.0000000000\nCholesterol_Level_Category       -3.263919e-03   0.0013108137\nStress_Levels_Category           -9.775957e-03   0.0050022995\nBMI_Category                     -1.896134e-03  -0.0069851954\nHeart_Rate_Category               9.340584e-03   0.0014048223\nSystolic_BP_Category              8.639608e-04  -0.0071712444\nDiastolic_BP_Category             4.680930e-03   0.0034817498\n                           Cholesterol_Level_Category Stress_Levels_Category\nGender                                  -0.0052850774          -4.098033e-03\nRegion                                  -0.0013870042           6.873657e-04\nDiabetes_History                         0.0042231518           5.556765e-03\nHypertension_History                     0.0043603722           4.221203e-03\nPhysical_Activity                       -0.0007832206          -4.504067e-03\nDiet_Quality                            -0.0051390283           3.973753e-03\nAlcohol_Consumption                     -0.0032639192          -9.775957e-03\nFamily_History                           0.0013108137           5.002300e-03\nCholesterol_Level_Category               1.0000000000          -1.612134e-03\nStress_Levels_Category                  -0.0016121344           1.000000e+00\nBMI_Category                             0.0001787723          -4.912493e-04\nHeart_Rate_Category                      0.0078659486          -5.505661e-03\nSystolic_BP_Category                     0.0043199306          -3.973247e-05\nDiastolic_BP_Category                   -0.0021550677           3.230604e-03\n                            BMI_Category Heart_Rate_Category\nGender                     -0.0029024904        0.0004961952\nRegion                     -0.0076272789       -0.0038853995\nDiabetes_History            0.0113091595       -0.0077468391\nHypertension_History       -0.0054824465       -0.0045361879\nPhysical_Activity          -0.0064385995       -0.0088025471\nDiet_Quality                0.0011524752       -0.0011485829\nAlcohol_Consumption        -0.0018961340        0.0093405843\nFamily_History             -0.0069851954        0.0014048223\nCholesterol_Level_Category  0.0001787723        0.0078659486\nStress_Levels_Category     -0.0004912493       -0.0055056614\nBMI_Category                1.0000000000        0.0028365599\nHeart_Rate_Category         0.0028365599        1.0000000000\nSystolic_BP_Category       -0.0115940877        0.0008703383\nDiastolic_BP_Category       0.0020819695       -0.0011852861\n                           Systolic_BP_Category Diastolic_BP_Category\nGender                            -8.742980e-03           0.006431543\nRegion                             3.736587e-03           0.002389132\nDiabetes_History                   1.001459e-02          -0.007072539\nHypertension_History               1.726916e-03           0.004857910\nPhysical_Activity                 -6.363427e-04           0.004370001\nDiet_Quality                      -9.383858e-03          -0.001982442\nAlcohol_Consumption                8.639608e-04           0.004680930\nFamily_History                    -7.171244e-03           0.003481750\nCholesterol_Level_Category         4.319931e-03          -0.002155068\nStress_Levels_Category            -3.973247e-05           0.003230604\nBMI_Category                      -1.159409e-02           0.002081970\nHeart_Rate_Category                8.703383e-04          -0.001185286\nSystolic_BP_Category               1.000000e+00           0.009991349\nDiastolic_BP_Category              9.991349e-03           1.000000000\n\n\nCode\n# Run Logistic Regression Model\nlogistic_model &lt;- glm(Heart_Attack_Occurrence ~ ., data = relevant_factors, family = binomial())\n\n# Print Model Summary\nsummary(logistic_model)\n\n\n\nCall:\nglm(formula = Heart_Attack_Occurrence ~ ., family = binomial(), \n    data = relevant_factors)\n\nCoefficients:\n                             Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)                -2.657e+01  1.328e+04  -0.002    0.998\nGenderFemale                5.020e-13  4.113e+03   0.000    1.000\nRegionUrban                -3.541e-13  4.496e+03   0.000    1.000\nSmoking_HistoryNo           8.318e-13  4.488e+03   0.000    1.000\nDiabetes_HistoryNo         -3.122e-13  5.111e+03   0.000    1.000\nHypertension_HistoryNo     -3.399e-13  4.759e+03   0.000    1.000\nPhysical_Activity          -4.815e-15  2.649e+03   0.000    1.000\nDiet_Quality                5.352e-13  2.745e+03   0.000    1.000\nAlcohol_Consumption         3.620e-13  2.762e+03   0.000    1.000\nFamily_HistoryNo           -3.726e-13  4.497e+03   0.000    1.000\nCholesterol_Level_Category  3.330e-13  3.515e+03   0.000    1.000\nStress_Levels_Category     -9.302e-14  3.649e+03   0.000    1.000\nBMI_Category               -3.487e-13  3.129e+03   0.000    1.000\nHeart_Rate_Category        -3.048e-13  5.641e+03   0.000    1.000\nSystolic_BP_Category       -8.970e-14  2.477e+03   0.000    1.000\nDiastolic_BP_Category      -1.603e-13  2.799e+03   0.000    1.000\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 0.0000e+00  on 29999  degrees of freedom\nResidual deviance: 1.7405e-07  on 29984  degrees of freedom\nAIC: 32\n\nNumber of Fisher Scoring iterations: 25\n\n\n:::\n\n\nCode\n# Generate correlation matrix plot using enhanced styling\nggstatsplot::ggcorrmat(\n  data = relevant_factors, \n  cor.vars = 1:ncol(relevant_factors), \n  ggcorrplot.args = list(\n    outline.color = \"black\",  # Add black outline for clarity\n    hc.order = TRUE,          # Hierarchical clustering to group correlated variables\n    tl.cex = 10               # Adjust text size for readability\n  ),\n  title    = \"Correlogram for Heart Attack Risk Factors\",\n  subtitle = \"Significance level: p &lt; 0.05\"\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Load necessary library\nlibrary(writexl)  # Or use openxlsx for more features\n\n# Define file path to save the Excel file\nfile_path &lt;- \"C:/andreaysh/ISSS608/try.xlsx\"\n\n# Export chi-square test results to Excel\nwrite_xlsx(heart_attack_2, path = file_path)\n\n# Confirm export\nprint(paste(\"File saved to:\", file_path))\n\n\n[1] \"File saved to: C:/andreaysh/ISSS608/try.xlsx\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCh"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#code-8",
    "href": "Take-home_Ex/Take-home_Ex01.html#code-8",
    "title": "Take-home Exercise 01",
    "section": "",
    "text": "Code\n#- DO NOT DELETE\n#| eval = FALSE \n\n# Load necessary libraries\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(stats)\n\n# Select relevant health-related columns\nrelevant_factors &lt;- heart_attack_2 %&gt;% \n  select(\"Gender\", \"Region\", \"Smoking_History\", \"Diabetes_History\", \"Hypertension_History\", \"Physical_Activity\",\n         \"Diet_Quality\", \"Alcohol_Consumption\", \"Family_History\", \"Cholesterol_Level_Category\", \"Heart_Attack_Occurrence\", \n         \"Stress_Levels_Category\", \"BMI_Category\", \"Heart_Rate_Category\", \"Systolic_BP_Category\", \"Diastolic_BP_Category\")\n\n# Convert categorical variables\n# Gender encoding\nrelevant_factors$Gender &lt;- factor(relevant_factors$Gender, levels = c(\"Male\", \"Female\"))\n\n# Region encoding\nrelevant_factors$Region &lt;- factor(relevant_factors$Region, levels = c(\"Rural\", \"Urban\"))\n\n# Smoking_History encoding\nrelevant_factors$Smoking_History &lt;- factor(relevant_factors$Smoking_History, levels = c(\"Yes\", \"No\"))\n\n# Diabetes_History encoding\nrelevant_factors$Diabetes_History &lt;- factor(relevant_factors$Diabetes_History, levels = c(\"Yes\", \"No\"))\n\n# Hypertension_History encoding\nrelevant_factors$Hypertension_History &lt;- factor(relevant_factors$Hypertension_History, levels = c(\"Yes\", \"No\"))\n\n# Physical_Activity is now numeric (Low = 0, Moderate = 1, High = 2)\nrelevant_factors$Physical_Activity &lt;- recode(relevant_factors$Physical_Activity, \"Low\" = 0, \"Moderate\" = 1, \"High\" = 2) %&gt;% as.numeric()\n\n# Diet_Quality is now numeric (Poor = 0, Average = 1, Good = 2)\nrelevant_factors$Diet_Quality &lt;- recode(relevant_factors$Diet_Quality, \"Poor\" = 0, \"Average\" = 1, \"Good\" = 2) %&gt;% as.numeric()\n\n# Alcohol_Consumption is now numeric (None = 0, Low = 0, Moderate = 1, High = 2)\nrelevant_factors$Alcohol_Consumption &lt;- recode(relevant_factors$Alcohol_Consumption, \"None\" = 0, \"Low\" = 0, \"Moderate\" = 1, \"High\" = 2) %&gt;% as.numeric()\n\n# Family_History encoding\nrelevant_factors$Family_History &lt;- factor(relevant_factors$Family_History, levels = c(\"Yes\", \"No\"))\n\n# Cholesterol_Level_Category is now numeric (Low = 0, Moderate = 1, High = 2)\nrelevant_factors$Cholesterol_Level_Category &lt;- recode(relevant_factors$Cholesterol_Level_Category, \"Low\" = 0, \"Moderate\" = 1, \"High\" = 2) %&gt;% as.numeric()\n\n# Stress_Levels_Category is now numeric (Minimal Stress = 0, Low Stress = 0, Moderate Stress = 1, High Stress = 2)\nrelevant_factors$Stress_Levels_Category &lt;- recode(relevant_factors$Stress_Levels_Category,\n  \"Miniminal_Stress\" = 0,\n  \"Low_Stress\" = 0,\n  \"Moderate_Stress\" = 1,\n  \"High_Stress\" = 2\n) %&gt;% as.numeric()\n\n# BMI_Category is now numeric (Underweight = 0, Normal_Weight = 1, Overweight = 2, Obese = 2)\nrelevant_factors$BMI_Category &lt;- recode(relevant_factors$BMI_Category, \"Underweight\" = 0, \"Normal_Weight\" = 1, \"Overweight\" = 2, \"Obese\" = 2) %&gt;% as.numeric()\n\n# Heart_Rate_Category is now numeric (Low = 0, Normal = 1, High = 2)\nrelevant_factors$Heart_Rate_Category &lt;- recode(relevant_factors$Heart_Rate_Category, \"Low\" = 0, \"Normal\" = 1, \"High\" = 2) %&gt;% as.numeric()\n\n# Systolic_BP_Category is now numeric (Normal = 0, Elevated = 1, Hypertension_Stage_1 = 2, Hypertension_Stage_2 = 2)\nrelevant_factors$Systolic_BP_Category &lt;- recode(relevant_factors$Systolic_BP_Category, \"Normal\" = 0, \"Elevated\" = 1, \"Hypertension_Stage_1\" = 2, \"Hypertension_Stage_2\" = 2) %&gt;% as.numeric()\n\n# Diastolic_BP_Category is now numeric (Normal = 0, Elevated = 1, Hypertension_Stage_1 = 2, Hypertension_Stage_2 = 2)\nrelevant_factors$Diastolic_BP_Category &lt;- recode(relevant_factors$Diastolic_BP_Category, \"Normal\" = 0, \"Elevated\" = 1, \"Hypertension_Stage_1\" = 2, \"Hypertension_Stage_2\" = 2) %&gt;% as.numeric()\n\n# Convert Heart_Attack_Occurrence to binary and ensure it remains a factor\nrelevant_factors$Heart_Attack_Occurrence &lt;- factor(ifelse(relevant_factors$Heart_Attack_Occurrence == \"Yes\", 1, 0), levels = c(0, 1))\n\n# Compute correlation matrix for numerical encoding\ncor_matrix &lt;- relevant_factors %&gt;% \n  mutate(across(where(is.factor), as.numeric)) %&gt;%\n  select(-Smoking_History, -Heart_Attack_Occurrence) %&gt;%\n  cor(use = \"complete.obs\")\n\nprint(cor_matrix)\n\n\n                                  Gender        Region Diabetes_History\nGender                      1.0000000000  0.0018273319    -0.0061491998\nRegion                      0.0018273319  1.0000000000    -0.0068285141\nDiabetes_History           -0.0061491998 -0.0068285141     1.0000000000\nHypertension_History       -0.0065795484  0.0096061837    -0.0110992995\nPhysical_Activity          -0.0076236037  0.0009938000    -0.0020764022\nDiet_Quality                0.0071961731 -0.0055712476    -0.0084098287\nAlcohol_Consumption        -0.0033616763  0.0048247117    -0.0005921596\nFamily_History              0.0153129606  0.0053180175    -0.0029149678\nCholesterol_Level_Category -0.0052850774 -0.0013870042     0.0042231518\nStress_Levels_Category     -0.0040980330  0.0006873657     0.0055567649\nBMI_Category               -0.0029024904 -0.0076272789     0.0113091595\nHeart_Rate_Category         0.0004961952 -0.0038853995    -0.0077468391\nSystolic_BP_Category       -0.0087429797  0.0037365868     0.0100145852\nDiastolic_BP_Category       0.0064315430  0.0023891323    -0.0070725390\n                           Hypertension_History Physical_Activity  Diet_Quality\nGender                            -0.0065795484     -0.0076236037  7.196173e-03\nRegion                             0.0096061837      0.0009938000 -5.571248e-03\nDiabetes_History                  -0.0110992995     -0.0020764022 -8.409829e-03\nHypertension_History               1.0000000000     -0.0076184686  1.603881e-03\nPhysical_Activity                 -0.0076184686      1.0000000000  8.475932e-03\nDiet_Quality                       0.0016038813      0.0084759322  1.000000e+00\nAlcohol_Consumption               -0.0006852643     -0.0030454964 -9.333696e-05\nFamily_History                    -0.0005563115     -0.0059558012  1.764614e-03\nCholesterol_Level_Category         0.0043603722     -0.0007832206 -5.139028e-03\nStress_Levels_Category             0.0042212032     -0.0045040665  3.973753e-03\nBMI_Category                      -0.0054824465     -0.0064385995  1.152475e-03\nHeart_Rate_Category               -0.0045361879     -0.0088025471 -1.148583e-03\nSystolic_BP_Category               0.0017269165     -0.0006363427 -9.383858e-03\nDiastolic_BP_Category              0.0048579096      0.0043700013 -1.982442e-03\n                           Alcohol_Consumption Family_History\nGender                           -3.361676e-03   0.0153129606\nRegion                            4.824712e-03   0.0053180175\nDiabetes_History                 -5.921596e-04  -0.0029149678\nHypertension_History             -6.852643e-04  -0.0005563115\nPhysical_Activity                -3.045496e-03  -0.0059558012\nDiet_Quality                     -9.333696e-05   0.0017646142\nAlcohol_Consumption               1.000000e+00   0.0008722224\nFamily_History                    8.722224e-04   1.0000000000\nCholesterol_Level_Category       -3.263919e-03   0.0013108137\nStress_Levels_Category           -9.775957e-03   0.0050022995\nBMI_Category                     -1.896134e-03  -0.0069851954\nHeart_Rate_Category               9.340584e-03   0.0014048223\nSystolic_BP_Category              8.639608e-04  -0.0071712444\nDiastolic_BP_Category             4.680930e-03   0.0034817498\n                           Cholesterol_Level_Category Stress_Levels_Category\nGender                                  -0.0052850774          -4.098033e-03\nRegion                                  -0.0013870042           6.873657e-04\nDiabetes_History                         0.0042231518           5.556765e-03\nHypertension_History                     0.0043603722           4.221203e-03\nPhysical_Activity                       -0.0007832206          -4.504067e-03\nDiet_Quality                            -0.0051390283           3.973753e-03\nAlcohol_Consumption                     -0.0032639192          -9.775957e-03\nFamily_History                           0.0013108137           5.002300e-03\nCholesterol_Level_Category               1.0000000000          -1.612134e-03\nStress_Levels_Category                  -0.0016121344           1.000000e+00\nBMI_Category                             0.0001787723          -4.912493e-04\nHeart_Rate_Category                      0.0078659486          -5.505661e-03\nSystolic_BP_Category                     0.0043199306          -3.973247e-05\nDiastolic_BP_Category                   -0.0021550677           3.230604e-03\n                            BMI_Category Heart_Rate_Category\nGender                     -0.0029024904        0.0004961952\nRegion                     -0.0076272789       -0.0038853995\nDiabetes_History            0.0113091595       -0.0077468391\nHypertension_History       -0.0054824465       -0.0045361879\nPhysical_Activity          -0.0064385995       -0.0088025471\nDiet_Quality                0.0011524752       -0.0011485829\nAlcohol_Consumption        -0.0018961340        0.0093405843\nFamily_History             -0.0069851954        0.0014048223\nCholesterol_Level_Category  0.0001787723        0.0078659486\nStress_Levels_Category     -0.0004912493       -0.0055056614\nBMI_Category                1.0000000000        0.0028365599\nHeart_Rate_Category         0.0028365599        1.0000000000\nSystolic_BP_Category       -0.0115940877        0.0008703383\nDiastolic_BP_Category       0.0020819695       -0.0011852861\n                           Systolic_BP_Category Diastolic_BP_Category\nGender                            -8.742980e-03           0.006431543\nRegion                             3.736587e-03           0.002389132\nDiabetes_History                   1.001459e-02          -0.007072539\nHypertension_History               1.726916e-03           0.004857910\nPhysical_Activity                 -6.363427e-04           0.004370001\nDiet_Quality                      -9.383858e-03          -0.001982442\nAlcohol_Consumption                8.639608e-04           0.004680930\nFamily_History                    -7.171244e-03           0.003481750\nCholesterol_Level_Category         4.319931e-03          -0.002155068\nStress_Levels_Category            -3.973247e-05           0.003230604\nBMI_Category                      -1.159409e-02           0.002081970\nHeart_Rate_Category                8.703383e-04          -0.001185286\nSystolic_BP_Category               1.000000e+00           0.009991349\nDiastolic_BP_Category              9.991349e-03           1.000000000\n\n\nCode\n# Run Logistic Regression Model\nlogistic_model &lt;- glm(Heart_Attack_Occurrence ~ ., data = relevant_factors, family = binomial())\n\n# Print Model Summary\nsummary(logistic_model)\n\n\n\nCall:\nglm(formula = Heart_Attack_Occurrence ~ ., family = binomial(), \n    data = relevant_factors)\n\nCoefficients:\n                             Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)                -2.657e+01  1.328e+04  -0.002    0.998\nGenderFemale                5.020e-13  4.113e+03   0.000    1.000\nRegionUrban                -3.541e-13  4.496e+03   0.000    1.000\nSmoking_HistoryNo           8.318e-13  4.488e+03   0.000    1.000\nDiabetes_HistoryNo         -3.122e-13  5.111e+03   0.000    1.000\nHypertension_HistoryNo     -3.399e-13  4.759e+03   0.000    1.000\nPhysical_Activity          -4.815e-15  2.649e+03   0.000    1.000\nDiet_Quality                5.352e-13  2.745e+03   0.000    1.000\nAlcohol_Consumption         3.620e-13  2.762e+03   0.000    1.000\nFamily_HistoryNo           -3.726e-13  4.497e+03   0.000    1.000\nCholesterol_Level_Category  3.330e-13  3.515e+03   0.000    1.000\nStress_Levels_Category     -9.302e-14  3.649e+03   0.000    1.000\nBMI_Category               -3.487e-13  3.129e+03   0.000    1.000\nHeart_Rate_Category        -3.048e-13  5.641e+03   0.000    1.000\nSystolic_BP_Category       -8.970e-14  2.477e+03   0.000    1.000\nDiastolic_BP_Category      -1.603e-13  2.799e+03   0.000    1.000\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 0.0000e+00  on 29999  degrees of freedom\nResidual deviance: 1.7405e-07  on 29984  degrees of freedom\nAIC: 32\n\nNumber of Fisher Scoring iterations: 25\n\n\n:::\n\n\nCode\n# Generate correlation matrix plot using enhanced styling\nggstatsplot::ggcorrmat(\n  data = relevant_factors, \n  cor.vars = 1:ncol(relevant_factors), \n  ggcorrplot.args = list(\n    outline.color = \"black\",  # Add black outline for clarity\n    hc.order = TRUE,          # Hierarchical clustering to group correlated variables\n    tl.cex = 10               # Adjust text size for readability\n  ),\n  title    = \"Correlogram for Heart Attack Risk Factors\",\n  subtitle = \"Significance level: p &lt; 0.05\"\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Load necessary library\nlibrary(writexl)  # Or use openxlsx for more features\n\n# Define file path to save the Excel file\nfile_path &lt;- \"C:/andreaysh/ISSS608/try.xlsx\"\n\n# Export chi-square test results to Excel\nwrite_xlsx(heart_attack_2, path = file_path)\n\n# Confirm export\nprint(paste(\"File saved to:\", file_path))\n\n\n[1] \"File saved to: C:/andreaysh/ISSS608/try.xlsx\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCh"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01_extra.html",
    "href": "Take-home_Ex/Take-home_Ex01_extra.html",
    "title": "Take-home_Ex01_extra",
    "section": "",
    "text": ":::"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#summary-and-findings",
    "href": "Take-home_Ex/Take-home_Ex01.html#summary-and-findings",
    "title": "Take-home Exercise 01",
    "section": "",
    "text": "Age alone is not the strongest predictor—lifestyle & health factors play a bigger role.\nIn this analysis, high cholesterol levels (&gt;200 mg/dL) are a primary risk factor for heart attacks.\nGender differences exist, but cholesterol remains the dominant factor.\nTargeted interventions should focus on reducing cholesterol and managing hypertension rather than stress alone"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07.html",
    "href": "Hands-on_Ex/Hands-on_Ex07.html",
    "title": "Hands-on Exercise 07",
    "section": "",
    "text": "With the assistance of ChatGPT"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex07.html#visualising-and-analysing-time-oriented-data",
    "href": "Hands-on_Ex/Hands-on_Ex07.html#visualising-and-analysing-time-oriented-data",
    "title": "Hands-on Exercise 07",
    "section": "7. Visualising and Analysing Time-oriented Data",
    "text": "7. Visualising and Analysing Time-oriented Data\n\n7.1 Learning Outcome\nIn this hands-on exercise, we will use R packages to create several visualizations:\n\nCreate a calendar heatmap using ggplot2\nCreate a cycle plot using ggplot2\nCreate a slopegraph\nCreate a horizon chart\n\n\n\n7.2 Installing and launching R packages\n\n\nCode\npacman::p_load(scales, viridis, lubridate, ggthemes,\n               gridExtra, readxl, knitr, data.table, \n               tidyverse, CGPfunctions)\n\n\n\n\n7.3 Importing data and data preparation\nThe following code imports the eventlog.csv file into our R environment.\n\n\nCode\nattacks &lt;- read_csv(\"data/eventlog.csv\")\n\n\n\n7.3.1 Installing and launching R packages\nWe will use kable() to review the structure of the imported data frame.\n\n\nCode\nkable(head(attacks))\n\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\nThe dataset includes three columns: timestamp, source_country, and tz.\n\ntimestamp: Contains date-time values in POSIXct format.\nsource_country: Represents the attack source using ISO 3166-1 alpha-2 country codes.\ntz: Stores the time zone of the source IP address.\n\n\n\n7.3.2 Data preparation\nBefore creating the calendar heatmap, we will write a function to derive two new fields:\n\nwkday (weekday) and\nhour\n\nfrom the timestamp.\nStep 1: Deriving weekday and hour of day fields\n\n\nCode\nmake_hr_wkday &lt;- function(ts, sc, tz) {\n  real_times &lt;- ymd_hms(ts, \n                        tz = tz[1], \n                        quiet = TRUE)\n  dt &lt;- data.table(source_country = sc,\n                   wkday = weekdays(real_times),\n                   hour = hour(real_times))\n  return(dt)\n  }\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nymd_hms() and hour() are from lubridate package, and\nweekdays() is a base R function.\n\n\n\nStep 2: Deriving the attacks tibble data frame\n\n\nCode\nwkday_levels &lt;- c('Saturday', 'Friday', \n                  'Thursday', 'Wednesday', \n                  'Tuesday', 'Monday', \n                  'Sunday')\n\nattacks &lt;- attacks %&gt;%\n  group_by(tz) %&gt;%\n  do(make_hr_wkday(.$timestamp, \n                   .$source_country, \n                   .$tz)) %&gt;% \n  ungroup() %&gt;% \n  mutate(wkday = factor(\n    wkday, levels = wkday_levels),\n    hour  = factor(\n      hour, levels = 0:23))\n\n\nTable below shows the tidy tibble table after processing.\n\n\nCode\nkable(head(attacks))\n\n\n\n\n\ntz\nsource_country\nwkday\nhour\n\n\n\n\nAfrica/Cairo\nBG\nSaturday\n20\n\n\nAfrica/Cairo\nTW\nSunday\n6\n\n\nAfrica/Cairo\nTW\nSunday\n8\n\n\nAfrica/Cairo\nCN\nSunday\n11\n\n\nAfrica/Cairo\nUS\nSunday\n15\n\n\nAfrica/Cairo\nCA\nMonday\n11\n\n\n\n\n\n\n\n\n7.4 Building the calendar heatmaps\n\n\nCode\ngrouped &lt;- attacks %&gt;% \n  count(wkday, hour) %&gt;% \n  ungroup() %&gt;%\n  na.omit()\n\nggplot(grouped, \n       aes(hour, \n           wkday, \n           fill = n)) + \ngeom_tile(color = \"white\", \n          size = 0.1) + \ntheme_tufte(base_family = \"Helvetica\") + \ncoord_equal() +\nscale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\nlabs(x = NULL, \n     y = NULL, \n     title = \"Attacks by weekday and time of day\") +\ntheme(axis.ticks = element_blank(),\n      plot.title = element_text(hjust = 0.5),\n      legend.title = element_text(size = 8),\n      legend.text = element_text(size = 6) )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThings to learn from the code\n\n\n\n\nA tibble data table (grouped) is created by aggregating attack data based on the wkday and hour fields.\nA new field, n, is added by using the group_by() and count() functions.\nna.omit() is used to remove missing values.\ngeom_tile() plots tiles (grids) at each (x, y) position, with color and size specifying the border color and line thickness.\ntheme_tufte() from the ggthemes package is used to eliminate unnecessary chart elements.\ncoord_equal() ensures the plot maintains a 1:1 aspect ratio.\nscale_fill_gradient() applies a two-color gradient (low to high).\n\n\n\n\n7.4.1 Building multiple calendar heatmaps\nTasks assigned: Building multiple heat maps for the top four countries with the highest number of attacks\nStep 1: Deriving attack by country object\nTo determine the top four countries with the highest number of attacks, the data needs to be processed by\n\nCount Attacks: Aggregate the number of attacks per country\nCalculate Percentage: Compute the proportion of attacks for each country\nStore Results: Save the output as a tibble data frame\n\n\n\nCode\nattacks_by_country &lt;- count(\n  attacks, source_country) %&gt;%\n  mutate(percent = percent(n/sum(n))) %&gt;%\n  arrange(desc(n))\n\n\nStep 2: Preparing the tidy data frame Next, we will extract the attack records of the top 4 countries from attacks data frame and save the data in a new tibble data frame (i.e. top4_attacks).\n\n\nCode\ntop4 &lt;- attacks_by_country$source_country[1:4]\ntop4_attacks &lt;- attacks %&gt;%\n  filter(source_country %in% top4) %&gt;%\n  count(source_country, wkday, hour) %&gt;%\n  ungroup() %&gt;%\n  mutate(source_country = factor(\n    source_country, levels = top4)) %&gt;%\n  na.omit()\n\n\n\n\n7.4.2 Plotting multiple calendar heatmaps\nStep 3: Plotting the Multiple Calender Heat map by using ggplot2 package.\n\n\nCode\nggplot(top4_attacks, \n       aes(hour, \n           wkday, \n           fill = n)) + \n  geom_tile(color = \"white\", \n          size = 0.1) + \n  theme_tufte(base_family = \"Helvetica\") + \n  coord_equal() +\n  scale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\n  facet_wrap(~source_country, ncol = 2) +\n  labs(x = NULL, y = NULL, \n     title = \"Attacks on top 4 countries by weekday and time of day\") +\n  theme(axis.ticks = element_blank(),\n        axis.text.x = element_text(size = 7),\n        plot.title = element_text(hjust = 0.5),\n        legend.title = element_text(size = 8),\n        legend.text = element_text(size = 6) )\n\n\n\n\n\n\n\n\n\n\n\n\n7.5 Plotting cycle plot\nThe below section will covers how to create a cycle plot using ggplot2 to visualize time-series patterns and trends in visitor arrivals from Vietnam programmatically.\n\n7.5.1 Importing data\nIn this exercise, arrivals_by_air.xlsx is imported using read_excel() from the readxl package and stored as a tibble data frame named air.\n\n\nCode\nair &lt;- read_excel(\"data/arrivals_by_air.xlsx\")\n\n\n\n\n7.5.2 Deriving month and year fields\nTwo new fields called month and year will be derived from Month-Year field\n\n\nCode\nair$month &lt;- factor(month(air$`Month-Year`), \n                    levels=1:12, \n                    labels=month.abb, \n                    ordered=TRUE) \nair$year &lt;- year(ymd(air$`Month-Year`))\n\n\n\n\n7.5.3 Extracting the target country\nThe code below is used to extract data for the target country (i.e. Vietnam).\n\n\nCode\nVietnam &lt;- air %&gt;% \n  select(`Vietnam`, \n         month, \n         year) %&gt;%\n  filter(year &gt;= 2010)\n\n\n\n\n7.5.4 Computing year average arrivals by month\nThe code uses group_by() and summarise() of dplyr to compute year average arrivals by month.\n\n\nCode\nhline.data &lt;- Vietnam %&gt;% \n  group_by(month) %&gt;%\n  summarise(avgvalue = mean(`Vietnam`))\n\n\n\n\n7.5.5 Plotting the cycle plot\nThe code below is used to plot the cycle plot.\n\n\nCode\nggplot() + \n  geom_line(data=Vietnam,\n            aes(x=year, \n                y=`Vietnam`, \n                group=month), \n            colour=\"pink\") +\n  geom_hline(aes(yintercept=avgvalue), \n             data=hline.data, \n             linetype=6, \n             colour=\"blue\", \n             size=0.5) + \n  facet_grid(~month) +\n  labs(axis.text.x = element_blank(),\n       title = \"Visitor arrivals from Vietnam by air, Jan 2010-Dec 2019\") +\n  xlab(\"\") +\n  ylab(\"No. of Visitors\") +\n  theme_tufte(base_family = \"Helvetica\")\n\n\n\n\n\n\n\n\n\n\n\n\n7.6 Plotting slopegraph\nIn this section we will learn how to plot a slopegraph by using R.\nBefore getting start, make sure that CGPfunctions has been installed and loaded onto R environment.\nTo learn more about the function, we can refer to Using newggslopegraph\nnewggslopegraph() and its arguments can be referenced at this link.\n\n7.6.1 Importing the data\nWe will use the code below to import the rice data set into R environment.\n\n\nCode\nrice &lt;- read_csv(\"data/rice.csv\")\n\n\nWe will check the dataset using below\n\nglimpse(): provides a transposed overview of a dataset, showing variables and their types in a concise format.\nhead(): displays the first few rows of a dataset (default is 6 rows) to give a quick preview of the data.\nsummary(): generates a statistical summary of each variable, including measures like mean, median, and range for numeric data.\nduplicated():returns a logical vector indicating which elements or rows in a vector or data frame are duplicates.\nSum(is.na()): counts the number of missing values (NA) in each column of the data frame.\nspec(): use spec() to quickly inspect the column\n\n\nglimpse()head()summary()duplicated()sum(is.na())spec()\n\n\n\n\nCode\nglimpse(rice)\n\n\nRows: 550\nColumns: 4\n$ Country    &lt;chr&gt; \"China\", \"China\", \"China\", \"China\", \"China\", \"China\", \"Chin…\n$ Year       &lt;dbl&gt; 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970,…\n$ Yield      &lt;dbl&gt; 20787, 23700, 26833, 28289, 29667, 31445, 31006, 31868, 314…\n$ Production &lt;dbl&gt; 56217601, 65675288, 76439280, 85853780, 90705630, 98403990,…\n\n\n\n\n\n\nCode\nhead(rice)\n\n\n# A tibble: 6 × 4\n  Country  Year Yield Production\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n1 China    1961 20787   56217601\n2 China    1962 23700   65675288\n3 China    1963 26833   76439280\n4 China    1964 28289   85853780\n5 China    1965 29667   90705630\n6 China    1966 31445   98403990\n\n\n\n\n\n\nCode\nsummary(rice)\n\n\n   Country               Year          Yield         Production       \n Length:550         Min.   :1961   Min.   :12299   Min.   :   764158  \n Class :character   1st Qu.:1973   1st Qu.:21722   1st Qu.:  6515902  \n Mode  :character   Median :1986   Median :30548   Median : 13351700  \n                    Mean   :1986   Mean   :34945   Mean   : 33829459  \n                    3rd Qu.:1998   3rd Qu.:44713   3rd Qu.: 33488900  \n                    Max.   :2010   Max.   :75967   Max.   :202771840  \n\n\n\n\n\n\nCode\nrice[duplicated(rice),]\n\n\n# A tibble: 0 × 4\n# ℹ 4 variables: Country &lt;chr&gt;, Year &lt;dbl&gt;, Yield &lt;dbl&gt;, Production &lt;dbl&gt;\n\n\n\n\n\n\nCode\nsum(is.na(rice))  \n\n\n[1] 0\n\n\n\n\n\n\nCode\nspec(rice)\n\n\ncols(\n  Country = col_character(),\n  Year = col_double(),\n  Yield = col_double(),\n  Production = col_double()\n)\n\n\n\n\n\nThe rice tibble contains 3 attributes, as shown above:\n\nCategorical attributes: Country\nContinuous attributes: Year Yield Production\n\n\n\n7.6.2 Plotting the slopegraph\nThe code below will be used to plot a basic slopegraph\n\n\nCode\nrice %&gt;% \n  mutate(Year = factor(Year)) %&gt;%\n  filter(Year %in% c(1961, 1980)) %&gt;%\n  newggslopegraph(Year, Yield, Country,\n                Title = \"Rice Yield of Top 11 Asian Counties\",\n                SubTitle = \"1961-1980\",\n                Caption = NULL)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nFor effective data visualisation design, factor() is used convert the value type of Year field from numeric to factor.\n\n\n\n\n\n\n8.0 References\n\nKam, T.S(2024). Visual Statistical Analysis.\n\n\n\n9.0 Takeaway\n\n\n\n\n\n\nKey takeaways\n\n\n\n\nLearn how to plot a calendar heatmap,\nLearn how to create a cycle plot (time-series analysis)\nLearn how to plot slopegraph which is use to compare trends over time - using newggslopegraph() from CGPfunctions\nKey R packages used - ggplot2, lubridate, dplyr & tidyr, ggthemes, and CGPfunctions\n\n\n\n\n\n9.1 Further exploration\n\nUsing rice dataset - To explore how Yield (or produdction) changes over time for China\n\nObservations:\n\nRice yield in China steadily rises from the 1960s to the 2010s.\nBy the 1980s, it surpasses the overall average.\nGrowth continues despite some minor dips.\nYield levels now are multiple times higher than in the early 1960s.\nPost-2000, growth appears to slow slightly, as China is moving away from agricultural production.\n\n\n\nCode\n# Filter for China only\nrice_china &lt;- rice %&gt;%\n  filter(Country == \"China\")\n\n# Calculate the overall average Yield\navg_yield &lt;- mean(rice_china$Yield, na.rm = TRUE)\n\n# Plot a simple line chart with average line\nggplot(rice_china, aes(x = Year, y = Yield)) +\n  geom_line(color = \"black\") +\n  geom_hline(yintercept = avg_yield, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Annual Rice Yield in China\",\n       x = \"Year\",\n       y = \"Yield\") +\n  theme_tufte()\n\n\n\n\n\n\n\n\n\n\nUsing AVERP dataset - To use slopegraph to explore consumer items over time\n\nObservations:\n\nSignificant Price Increase for Certain Items: Cod Fish (Per Kilogram) saw a substantial increase from 48.6 in 2014 to 66.0 in 2022. Threadfin (Kurau) (Per Kilogram) also experienced a sharp rise from 51.2 to 58.5.\nModerate Price Increases Across Several Items: Items like Pork Rib Bones, Streaky Pork, and Squids show steady increases\nDiverging Trends Between Items: While many items show a gradual increase, seafood and meat products show the highest price surges.Plant-based or non-perishable goods seem to have less extreme price fluctuations.\n\n\n\nCode\n# Load necessary libraries\nlibrary(tidyverse)\nlibrary(CGPfunctions)\n\n# Import dataset\ndf &lt;- read_csv(\"data/AVERP.csv\")\n\n# Convert Date column to Date format\ndf$Date &lt;- as.Date(df$Date, format=\"%m/%d/%Y\")\n\n# Extract the year\ndf$Year &lt;- format(df$Date, \"%Y\")\n\n# Select two significant years (earliest and latest)\nyear_min &lt;- min(df$Year)\nyear_max &lt;- max(df$Year)\n\n# Filter dataset for the two selected years\ndf_filtered &lt;- df %&gt;%\n  filter(Year %in% c(year_min, year_max)) %&gt;%\n  group_by(`Consumer Items`, Year) %&gt;%\n  summarise(Avg_Value = round(mean(Values, na.rm = TRUE), 1)) %&gt;%  # Round to 1 decimal place\n  ungroup()\n\n# Convert Year to factor for better visualization\ndf_filtered$Year &lt;- as.factor(df_filtered$Year)\n\n# Plot the slopegraph\nnewggslopegraph(df_filtered, Year, Avg_Value, `Consumer Items`,\n                Title = \"Slopegraph of Consumer Items Over Time\",\n                SubTitle = paste(year_min, \"-\", year_max),\n                Caption = \"Prepared using R\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02.html",
    "href": "Take-home_Ex/Take-home_Ex02.html",
    "title": "Take-home Exercise 02",
    "section": "",
    "text": "All values are in Million Dollars\n\n\n\n\nSince Mr. Donald Trump assumed office as the President of the United States on January 20, 2025, global trade has been a highly scrutinized topic. Given Singapore’s role as a key global trade hub, understanding its trade dynamics is crucial in assessing the potential impact of shifting geopolitical and economic policies.\nIn this take-home exercise, we apply newly acquired data visualization and analytical techniques to explore Singapore’s Trade in Services - Singapore’s exports and imports of services between Singapore and the rest of the world. By leveraging statistical tools and visualization methods, this study aims to uncover key trends, service category distributions, and trade imbalances while providing deeper insights into how different service sectors contribute to Singapore’s economy.\n\n\n\nUsing data from the Department of Statistics Singapore, DOS on Trade In Services By Services Category, this analysis applies Exploratory Data (EDA) and data visualization techniques to:\n\nAssess three existing visualizations from this page, identifying each of these visualization pros and cons.\nRedesign and enhance these visualizations using ggplot2 and other R packages to improve data interpretation, visual appeal, and accessibility.\nConduct time-series analysis and forecasting to identify key trends in Singapore’s trade in services, evaluate growth patterns across service categories, and predict potential future trade movements.\n\n\n\n\n\n\n\nThe following R packages will be loaded for this exercise using pacman::p_load():\n\nreadxl: Reads Excel files (.xls and .xlsx) into R\nwritexl: Writes data frames to Excel files (.xlsx).\ntidyverse: A collection of packages for data manipulation, visualization, and modeling.\ntimetk: Provides time series analysis and forecasting tools.\nforecast: Implements forecasting methods like ARIMA, ETS, and more.\nggplot2: A powerful package for data visualization using the grammar of graphics.\nplotly: Creates interactive plots, including 3D and web-based visualizations.\ntreemapify: Enables the creation of treemap visualizations in ggplot2.\npatchwork: Helps combine multiple ggplot2 plots into a single layout.\ndplyr: Provides fast and intuitive data manipulation functions.\nCGPfunctions: Offers additional plotting and visualization functions.\nrnaturalearth: Provides world map data for geographic visualizations.\nrnaturalearthdata: Supplies natural Earth vector data for spatial analysis.\nsf: Supports simple features for spatial data (GIS).\nviridis: Provides color palettes for better visualization accessibility.\nggrepel: Improves text labeling in ggplot2 by avoiding overlapping labels.\nggHoriPlot: Creates horizontal bar plots for ggplot2.\nggthemes: Adds additional themes and styles for ggplot2.\ntidymodels: A collection of packages for machine learning and modeling.\nmodeltime: A framework for time series forecasting using machine learning and statistical models.\n\n\n\nCode\npacman::p_load(readxl, openxlsx, data.table, tidyverse, timetk, plotly, forecast, ggplot2, CGPfunctions, rnaturalearth, rnaturalearthdata, sf, viridis, ggrepel, ggHoriPlot, ggthemes, tidymodels, timetk, modeltime)\n\n\n\n\n\n\n\n\n\n\nThe original visualizations analyzed in this analysis are sourced from here.\n\n\n\n\nIn the below section, we will evaluate the effectiveness of the above visualization by identifying its pros and cons, focusing on aspects such as (1) clarity, and (2) visual appeal:\n\nClarity: How well the data is presented and understood\n\n\n\n\n\n\n\n\n\nPros\nCons\nSuggested fixes\n\n\n\n\nClear categorization of exports and imports - The chart effectively differentiates between exports and imports\nYear-specific colors make trend analysis difficult – Each year is assigned a different color, making it hard to track trends across time.\nUse consistent colors for exports and imports across all years (Implemented by converting them into line charts).\n\n\nTotal trade values are highlighted – The total trade (Exports + Imports) is displayed for each year.\nFloating “Total” labels can be easily overlooked – They are placed above bars separately, which may lead to misinterpretation.\nIntroduce a line chart for total trade trends instead of floating labels.\n\n\nGrowth rate (CAGR) and trade balance data are included – Additional insights are provided.\nTrade balance is not well integrated – It is displayed separately at the bottom instead of being visually linked to the bars.\nRepresent trade balance directly as a bar chart (Green for surplus, Red for deficit)\n\n\n\n\nVisual appeal: How visually engaging and effective the design is\n\n\n\n\n\n\n\n\n\nPros\nCons\nSuggested fixes\n\n\n\n\nEngaging use of colors and icons – Makes the visualization appealing and eye-catching.\nOveruse of colors creates clutter – Different colors for each year make it visually overwhelming.\nReduce unnecessary color variations and simplify color coding.\n\n\n\n\n\n\n\n\n\n\n\nThe code chunk below imports the Trade In Services By Services Category dataset, downloaded from Department of Statistics Singapore, DOS, using the read_excel() function from the readxl package.\n\n\nCode\ntrade_services &lt;- read_excel(\"data/Trade In Services By Services Category_base.xlsx\")\n\n\n\n\n\n\nglimpse(): provides a transposed overview of a dataset, showing variables and their types in a concise format.\nhead(): displays the first few rows of a dataset (default is 6 rows) to give a quick preview of the data.\nsummary(): generates a statistical summary of each variable, including measures like mean, median, and range for numeric data.\nduplicated():returns a logical vector indicating which elements or rows in a vector or data frame are duplicates.\ncolSums(is.na()): counts the number of missing values (NA) in each column of the data frame.\nstr(): use str() to display the column names, data types, and a preview of the data.\n\n\nglimpse()head()summary()duplicated()colSum(is.na())str())\n\n\n\n\nCode\nglimpse(trade_services)\n\n\nRows: 51\nColumns: 26\n$ `Data Series` &lt;chr&gt; \"Total Trade In Services\", \"Exports Of Services\", \"Manuf…\n$ `2024`        &lt;dbl&gt; 997749.8, 528568.3, 471.0, 10820.1, 172971.1, 145993.3, …\n$ `2023`        &lt;dbl&gt; 919117.0, 481009.2, 490.7, 10981.0, 149537.4, 124222.5, …\n$ `2022`        &lt;dbl&gt; 877252.5, 468190.5, 685.5, 10117.9, 189649.0, 167209.2, …\n$ `2021`        &lt;dbl&gt; 715093.5, 382492.4, 489.9, 8833.5, 144220.0, 130499.3, 1…\n$ `2020`        &lt;dbl&gt; 590035.1, 300004.6, 236.8, 8172.9, 93560.0, 79857.9, 137…\n$ `2019`        &lt;dbl&gt; 592824.3, 307215.9, 266.9, 9663.3, 94272.1, 76545.7, 177…\n$ `2018`        &lt;dbl&gt; 561271.0, 287141.4, 370.3, 9410.0, 88888.8, 71746.4, 171…\n$ `2017`        &lt;dbl&gt; 493353.8, 241568.0, 247.1, 7712.0, 69993.5, 54547.9, 154…\n$ `2016`        &lt;dbl&gt; 431109.3, 211835.8, 284.8, 8418.5, 59213.4, 45873.0, 133…\n$ `2015`        &lt;dbl&gt; 432922.3, 210622.7, 346.5, 9315.2, 64097.1, 50798.1, 132…\n$ `2014`        &lt;dbl&gt; 406020.8, 194843.2, 424.4, 9853.1, 63918.8, 50917.2, 130…\n$ `2013`        &lt;dbl&gt; 365055.0, 177719.3, 283.2, 10767.2, 57830.9, 45929.4, 11…\n$ `2012`        &lt;dbl&gt; 327866.3, 161769.2, 249.6, 9053.1, 55586.3, 42864.3, 127…\n$ `2011`        &lt;dbl&gt; 298227.8, 150013.0, 260.4, 9342.9, 53523.0, 41416.7, 121…\n$ `2010`        &lt;dbl&gt; 273929.7, 136872.3, 289.5, 8648.4, 52606.6, 41214.6, 113…\n$ `2009`        &lt;dbl&gt; 238962.6, 117832.0, 323.4, 9128.1, 43365.7, 33042.6, 103…\n$ `2008`        &lt;dbl&gt; 254282.0, 126155.0, 452.1, 8354.6, 51108.8, 38561.9, 125…\n$ `2007`        &lt;dbl&gt; 223936.4, 110796.6, 492.6, 6605.6, 43642.8, 31104.6, 125…\n$ `2006`        &lt;dbl&gt; 195966.0, 92674.8, 534.7, 5701.0, 35877.1, 24748.0, 1112…\n$ `2005`        &lt;dbl&gt; 167532.8, 75904.8, 315.0, 4797.7, 32435.1, 21286.1, 1114…\n$ `2004`        &lt;dbl&gt; 150911.1, 66795.8, 353.2, 3450.5, 28630.3, 18702.3, 9928…\n$ `2003`        &lt;dbl&gt; 122427.9, 52966.2, 303.1, 2883.1, 23343.2, 15022.2, 8321…\n$ `2002`        &lt;dbl&gt; 109710.5, 49936.3, 369.5, 3071.8, 21539.3, 12656.8, 8882…\n$ `2001`        &lt;dbl&gt; 102892.5, 46286.1, 195.0, 2099.5, 20497.9, 12079.2, 8418…\n$ `2000`        &lt;dbl&gt; 96452.4, 44854.8, 202.4, 1755.3, 20379.3, 11595.5, 8783.…\n\n\n\n\n\n\nCode\nhead(trade_services)\n\n\n# A tibble: 6 × 26\n  `Data Series`   `2024` `2023` `2022` `2021` `2020` `2019` `2018` `2017` `2016`\n  &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Total Trade In… 9.98e5 9.19e5 8.77e5 7.15e5 5.90e5 5.93e5 5.61e5 4.93e5 4.31e5\n2 Exports Of Ser… 5.29e5 4.81e5 4.68e5 3.82e5 3.00e5 3.07e5 2.87e5 2.42e5 2.12e5\n3 Manufacturing … 4.71e2 4.91e2 6.85e2 4.90e2 2.37e2 2.67e2 3.70e2 2.47e2 2.85e2\n4 Maintenance An… 1.08e4 1.10e4 1.01e4 8.83e3 8.17e3 9.66e3 9.41e3 7.71e3 8.42e3\n5 Transport       1.73e5 1.50e5 1.90e5 1.44e5 9.36e4 9.43e4 8.89e4 7.00e4 5.92e4\n6 Freight         1.46e5 1.24e5 1.67e5 1.30e5 7.99e4 7.65e4 7.17e4 5.45e4 4.59e4\n# ℹ 16 more variables: `2015` &lt;dbl&gt;, `2014` &lt;dbl&gt;, `2013` &lt;dbl&gt;, `2012` &lt;dbl&gt;,\n#   `2011` &lt;dbl&gt;, `2010` &lt;dbl&gt;, `2009` &lt;dbl&gt;, `2008` &lt;dbl&gt;, `2007` &lt;dbl&gt;,\n#   `2006` &lt;dbl&gt;, `2005` &lt;dbl&gt;, `2004` &lt;dbl&gt;, `2003` &lt;dbl&gt;, `2002` &lt;dbl&gt;,\n#   `2001` &lt;dbl&gt;, `2000` &lt;dbl&gt;\n\n\n\n\n\n\nCode\nsummary(trade_services)\n\n\n Data Series             2024               2023               2022         \n Length:51          Min.   :    57.8   Min.   :    56.2   Min.   :    59.6  \n Class :character   1st Qu.:  1855.3   1st Qu.:  1817.6   1st Qu.:  1603.0  \n Mode  :character   Median : 13495.7   Median : 12907.1   Median : 11700.4  \n                    Mean   : 71188.6   Mean   : 65608.8   Mean   : 63405.1  \n                    3rd Qu.: 45456.5   3rd Qu.: 43514.1   3rd Qu.: 37732.8  \n                    Max.   :997749.8   Max.   :919117.0   Max.   :877252.5  \n      2021               2020               2019               2018         \n Min.   :    62.4   Min.   :    57.5   Min.   :    52.1   Min.   :    66.8  \n 1st Qu.:  1350.0   1st Qu.:  1209.1   1st Qu.:  1134.8   1st Qu.:  1051.2  \n Median :  9180.3   Median :  9124.8   Median : 10250.8   Median :  9410.0  \n Mean   : 51559.2   Mean   : 42075.8   Mean   : 41792.4   Mean   : 39491.2  \n 3rd Qu.: 35308.2   3rd Qu.: 30748.8   3rd Qu.: 29536.0   3rd Qu.: 27016.0  \n Max.   :715093.5   Max.   :590035.1   Max.   :592824.3   Max.   :561271.0  \n      2017               2016               2015               2014         \n Min.   :    56.1   Min.   :    72.0   Min.   :    46.9   Min.   :    56.9  \n 1st Qu.:   957.6   1st Qu.:   811.6   1st Qu.:   809.8   1st Qu.:   739.1  \n Median :  7848.2   Median :  6458.8   Median :  6194.0   Median :  6021.0  \n Mean   : 34523.8   Mean   : 29987.0   Mean   : 30256.4   Mean   : 28284.6  \n 3rd Qu.: 23594.4   3rd Qu.: 22798.2   3rd Qu.: 23150.7   3rd Qu.: 22757.6  \n Max.   :493353.8   Max.   :431109.3   Max.   :432922.3   Max.   :406020.8  \n      2013               2012               2011               2010         \n Min.   :    78.8   Min.   :    84.6   Min.   :    62.5   Min.   :    64.0  \n 1st Qu.:   661.6   1st Qu.:   656.4   1st Qu.:   617.9   1st Qu.:   587.1  \n Median :  4647.2   Median :  4034.2   Median :  3397.2   Median :  3493.8  \n Mean   : 25332.7   Mean   : 22691.6   Mean   : 20633.9   Mean   : 19039.0  \n 3rd Qu.: 21875.2   3rd Qu.: 19465.0   3rd Qu.: 17328.2   3rd Qu.: 16731.5  \n Max.   :365055.0   Max.   :327866.3   Max.   :298227.8   Max.   :273929.7  \n      2009               2008               2007               2006         \n Min.   :    51.0   Min.   :    76.9   Min.   :   130.4   Min.   :    48.6  \n 1st Qu.:   579.8   1st Qu.:   536.3   1st Qu.:   441.2   1st Qu.:   413.8  \n Median :  3015.6   Median :  3238.9   Median :  2213.8   Median :  2080.4  \n Mean   : 16671.3   Mean   : 17870.3   Mean   : 15789.5   Mean   : 13848.0  \n 3rd Qu.: 13932.2   3rd Qu.: 14221.4   3rd Qu.: 13344.4   3rd Qu.: 11489.8  \n Max.   :238962.6   Max.   :254282.0   Max.   :223936.4   Max.   :195966.0  \n      2005               2004               2003               2002         \n Min.   :    24.7   Min.   :    17.3   Min.   :    17.2   Min.   :    14.3  \n 1st Qu.:   313.1   1st Qu.:   284.9   1st Qu.:   228.2   1st Qu.:   212.3  \n Median :  1524.5   Median :  1374.8   Median :  1108.7   Median :  1001.9  \n Mean   : 11792.0   Mean   : 10647.8   Mean   :  8587.8   Mean   :  7648.4  \n 3rd Qu.: 10741.8   3rd Qu.:  9462.4   3rd Qu.:  7513.7   3rd Qu.:  8339.2  \n Max.   :167532.8   Max.   :150911.1   Max.   :122427.9   Max.   :109710.5  \n      2001               2000        \n Min.   :    22.6   Min.   :   13.6  \n 1st Qu.:   188.7   1st Qu.:  170.6  \n Median :   778.0   Median :  794.0  \n Mean   :  7219.0   Mean   : 6794.9  \n 3rd Qu.:  8164.2   3rd Qu.: 7509.9  \n Max.   :102892.5   Max.   :96452.4  \n\n\n\n\n\n\nCode\ntrade_services[duplicated(trade_services),]\n\n\n# A tibble: 0 × 26\n# ℹ 26 variables: Data Series &lt;chr&gt;, 2024 &lt;dbl&gt;, 2023 &lt;dbl&gt;, 2022 &lt;dbl&gt;,\n#   2021 &lt;dbl&gt;, 2020 &lt;dbl&gt;, 2019 &lt;dbl&gt;, 2018 &lt;dbl&gt;, 2017 &lt;dbl&gt;, 2016 &lt;dbl&gt;,\n#   2015 &lt;dbl&gt;, 2014 &lt;dbl&gt;, 2013 &lt;dbl&gt;, 2012 &lt;dbl&gt;, 2011 &lt;dbl&gt;, 2010 &lt;dbl&gt;,\n#   2009 &lt;dbl&gt;, 2008 &lt;dbl&gt;, 2007 &lt;dbl&gt;, 2006 &lt;dbl&gt;, 2005 &lt;dbl&gt;, 2004 &lt;dbl&gt;,\n#   2003 &lt;dbl&gt;, 2002 &lt;dbl&gt;, 2001 &lt;dbl&gt;, 2000 &lt;dbl&gt;\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nEnsure that there are no duplicated columns, if not will have to investigate further.\n\n\n\n\n\n\n\nCode\ncolSums(is.na(trade_services))\n\n\nData Series        2024        2023        2022        2021        2020 \n          0           0           0           0           0           0 \n       2019        2018        2017        2016        2015        2014 \n          0           0           0           0           0           0 \n       2013        2012        2011        2010        2009        2008 \n          0           0           0           0           0           0 \n       2007        2006        2005        2004        2003        2002 \n          0           0           0           0           0           0 \n       2001        2000 \n          0           0 \n\n\n\n\n\n\n\n\nNote\n\n\n\n\nEnsure that there are no NA values, if not will have to investigate further.\nPossibility to use drop_na() function to drop rows where any specified column contains a missing value.\n\n\n\n\n\n\n\nCode\nstr(trade_services)\n\n\ntibble [51 × 26] (S3: tbl_df/tbl/data.frame)\n $ Data Series: chr [1:51] \"Total Trade In Services\" \"Exports Of Services\" \"Manufacturing Services On Physical Inputs Owned By Others\" \"Maintenance And Repair Services\" ...\n $ 2024       : num [1:51] 997750 528568 471 10820 172971 ...\n $ 2023       : num [1:51] 919117 481009 491 10981 149537 ...\n $ 2022       : num [1:51] 877253 468191 686 10118 189649 ...\n $ 2021       : num [1:51] 715094 382492 490 8834 144220 ...\n $ 2020       : num [1:51] 590035 300005 237 8173 93560 ...\n $ 2019       : num [1:51] 592824 307216 267 9663 94272 ...\n $ 2018       : num [1:51] 561271 287141 370 9410 88889 ...\n $ 2017       : num [1:51] 493354 241568 247 7712 69994 ...\n $ 2016       : num [1:51] 431109 211836 285 8418 59213 ...\n $ 2015       : num [1:51] 432922 210623 346 9315 64097 ...\n $ 2014       : num [1:51] 406021 194843 424 9853 63919 ...\n $ 2013       : num [1:51] 365055 177719 283 10767 57831 ...\n $ 2012       : num [1:51] 327866 161769 250 9053 55586 ...\n $ 2011       : num [1:51] 298228 150013 260 9343 53523 ...\n $ 2010       : num [1:51] 273930 136872 290 8648 52607 ...\n $ 2009       : num [1:51] 238963 117832 323 9128 43366 ...\n $ 2008       : num [1:51] 254282 126155 452 8355 51109 ...\n $ 2007       : num [1:51] 223936 110797 493 6606 43643 ...\n $ 2006       : num [1:51] 195966 92675 535 5701 35877 ...\n $ 2005       : num [1:51] 167533 75905 315 4798 32435 ...\n $ 2004       : num [1:51] 150911 66796 353 3450 28630 ...\n $ 2003       : num [1:51] 122428 52966 303 2883 23343 ...\n $ 2002       : num [1:51] 109711 49936 370 3072 21539 ...\n $ 2001       : num [1:51] 102893 46286 195 2100 20498 ...\n $ 2000       : num [1:51] 96452 44855 202 1755 20379 ...\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nEnsure that all variables are correctly classified by data type; recast variable types if needed.\nVariables are correctly classified - where categorical variables are classified as character, while continuous variables are classified as double.\n\n\n\n\n\n\nThe trade_services tibble contains 26 attributes, as shown above.\nThe following preprocessing checks were conducted as part of data preparation:\n\n\n\n\n\n\nPreprocessing Checks\n\n\n\n\nVerified that the correct data types were loaded in the trade_services dataset using glimpse() and str()\nEnsured there were no duplicate variable names using duplicated() in the dataset\nChecked for missing values using colSums(is.na())\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\ntrade_services dataset will be used for both DataVis makeover 1 & 2\n\n\n\n\n\n\nAfter importing the trade_services dataset, we will filter for the three rows - Exports of Services, Imports of Services, and Total Trade in Services, which are essential for recreating the original visualization.\n\n\nCode\n# Select rows where 'Data Series' is either \"Exports Of Services\" or \"Imports Of Services\"\nexport_vs_import &lt;- trade_services %&gt;%\n  filter(`Data Series` %in% c(\"Exports Of Services\", \"Imports Of Services\", \"Total Trade In Services\"))\n\n# View the filtered data\nprint(export_vs_import)\n\n\n# A tibble: 3 × 26\n  `Data Series`   `2024` `2023` `2022` `2021` `2020` `2019` `2018` `2017` `2016`\n  &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Total Trade In… 9.98e5 9.19e5 8.77e5 7.15e5 5.90e5 5.93e5 5.61e5 4.93e5 4.31e5\n2 Exports Of Ser… 5.29e5 4.81e5 4.68e5 3.82e5 3.00e5 3.07e5 2.87e5 2.42e5 2.12e5\n3 Imports Of Ser… 4.69e5 4.38e5 4.09e5 3.33e5 2.90e5 2.86e5 2.74e5 2.52e5 2.19e5\n# ℹ 16 more variables: `2015` &lt;dbl&gt;, `2014` &lt;dbl&gt;, `2013` &lt;dbl&gt;, `2012` &lt;dbl&gt;,\n#   `2011` &lt;dbl&gt;, `2010` &lt;dbl&gt;, `2009` &lt;dbl&gt;, `2008` &lt;dbl&gt;, `2007` &lt;dbl&gt;,\n#   `2006` &lt;dbl&gt;, `2005` &lt;dbl&gt;, `2004` &lt;dbl&gt;, `2003` &lt;dbl&gt;, `2002` &lt;dbl&gt;,\n#   `2001` &lt;dbl&gt;, `2000` &lt;dbl&gt;\n\n\n\n\n\n\nKey makeover changes:\n1️⃣ Overall design improvements:\n\nReplaced bar chart with a line chart for exports and imports –&gt; Helps in tracking trends more effectively over time instead of color-coded bars per year.\nIntroduced a dashed line for total trade values –&gt; Previously, total trade was only displayed as floating labels above bars, which could be overlooked.\nChanged color usage –&gt; Original chart had different colors for each year, making trend analysis harder. Now, consistent colors are used:\n\nExports are in red,\nImports are in blue,\nTotal Trade is in black (dashed),\nTrade Surplus is in green, and\nTrade Deficit is in red.\n\n\n2️⃣ Trade Balance Integration:\n\nIntegration of key insights –&gt; Previously trade balance was displayed separately as a small section at the bottom. Now, I have integrated it as a bar chart within the main graph\n\ngreen bars represent trade surplus, and\nred bars will represent trade deficit.\n\n\n\n\n\n\n\n\nOverall\n\n\n\n\nThe new chart is cleaner, and provides a better narrative of how exports, imports, total trade, and trade balance evolve over time.\nThe new chart simplifies year-on-year trend analysis, making it easier to identify patterns in Singapore’s international trade.\n\n\n\n\nEnhance graph()Code()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Filter relevant rows (Exports, Imports, and Total Trade)\nexport_vs_import &lt;- trade_services %&gt;%\n  filter(`Data Series` %in% c(\"Exports Of Services\", \"Imports Of Services\", \"Total Trade In Services\"))\n\n# Convert dataset from wide to long format for ggplot\ndata_long &lt;- export_vs_import %&gt;%\n  pivot_longer(cols = -`Data Series`, names_to = \"Year\", values_to = \"Value\") %&gt;%\n  mutate(Year = as.numeric(Year)) %&gt;%\n  filter(Year &gt;= 2020 & Year &lt;= 2024)\n\n# Extract `Total Trade In Services` from dataset\ntotal_trade_data &lt;- data_long %&gt;%\n  filter(`Data Series` == \"Total Trade In Services\") %&gt;%\n  select(Year, Value) %&gt;%\n  rename(Total_Trade = Value)\n\n# Calculate Trade Balance (Exports - Imports)\ntrade_balance &lt;- data_long %&gt;%\n  spread(`Data Series`, Value) %&gt;%\n  mutate(Services_Trade_Balance = `Exports Of Services` - `Imports Of Services`) %&gt;%\n  select(Year, Services_Trade_Balance)\n\n# Merge Trade Balance & Total Trade into DataFrame\ndata_long &lt;- left_join(data_long, trade_balance, by = \"Year\")\ndata_long &lt;- left_join(data_long, total_trade_data, by = \"Year\")\n\n# Increase plotting window size\noptions(repr.plot.width=16, repr.plot.height=9)  \n\n# Create the plot\np &lt;- ggplot(data_long) +\n\n  # **Line Chart for Exports**\n  geom_line(data = subset(data_long, `Data Series` == \"Exports Of Services\"), \n            aes(x = Year, y = Value, color = \"Exports\"), linewidth = 1.5) +\n  \n  # **Data Labels for Exports**\n  geom_text(data = subset(data_long, `Data Series` == \"Exports Of Services\"), \n            aes(x = Year, y = Value, label = round(Value, 1)), \n            vjust = -1, size = 2.5, color = \"red\") +\n\n  # **Line Chart for Imports**\n  geom_line(data = subset(data_long, `Data Series` == \"Imports Of Services\"), \n            aes(x = Year, y = Value, color = \"Imports\"), linewidth = 1.5) +\n\n  # **Data Labels for Imports**\n  geom_text(data = subset(data_long, `Data Series` == \"Imports Of Services\"), \n            aes(x = Year, y = Value, label = round(Value, 1)), \n            vjust = 1.5, size = 2.5, color = \"blue\") +\n\n  # **Line Chart for Total Trade (Black, Dashed)**\n  geom_line(data = total_trade_data,\n            aes(x = Year, y = Total_Trade, color = \"Total Trade\"), linewidth = 1.5, linetype = \"dashed\") +\n\n  # **Labels for Total Trade**\n  geom_text(data = total_trade_data,\n            aes(x = Year, y = Total_Trade + 20000, label = round(Total_Trade, 1)), \n            vjust = -0.5, size = 2.5, color = \"black\") +\n\n  # **Bar Chart for Trade Balance (Surplus = Green, Deficit = Red)**\n  geom_bar(data = trade_balance, \n           aes(x = Year, y = Services_Trade_Balance, \n               fill = ifelse(Services_Trade_Balance &gt;= 0, \"Surplus\", \"Deficit\")), \n           stat = \"identity\", width = 0.5) +\n\n  # **Move Labels to the Top of Trade Balance Bars**\n  geom_text(data = trade_balance, \n            aes(x = Year, y = Services_Trade_Balance + 5000, \n                label = round(Services_Trade_Balance, 1)), \n            vjust = -0.5, size = 2.5, color = \"black\") +\n\n  # **Title & Labels**\n  labs(title = \"Trends in International Trade in Services (2020-2024)\",\n       subtitle = \"Exports, Imports & Total Trade as Lines, Trade Balance as Bars\",\n       x = \"Year\", y = \"S$ Billion\") +\n\n  # **Custom Theme**\n  theme_minimal(base_size = 10) +  \n\n  # **Color Customization**\n  scale_color_manual(name = \"Category\", values = c(\"Exports\" = \"red\", \"Imports\" = \"blue\", \"Total Trade\" = \"black\")) +\n  scale_fill_manual(name = \"Trade Balance\", values = c(\"Surplus\" = \"green\", \"Deficit\" = \"red\")) +\n\n  # **Formatting**\n  theme(\n        legend.position = \"bottom\",\n        panel.grid.major.y = element_line(color = \"gray\", linetype = \"dashed\"),\n        axis.text.x = element_text(face = \"bold\"))\n\n# Display the plot\nprint(p)\n\n\n\n\n\n\n\n\n\nI conducted time-series analysis to analyze trends in exports, imports, and total trade from 2000 to 2024. First, I visualized the historical data using ggplot2, ensuring clear differentiation of trends by assigning red for exports, blue for imports, and black for total trade. This provided insights into the overall growth patterns and fluctuations over time.\nKey observations:\n\nSteady growth: Singapore’s total trade in services has shown a strong upward trend, reflecting its role as a global trade hub.\nExports Lead Imports: While both have grown, exports consistently exceed imports, indicating a positive trade balance.\nImpact of Global Events: Periods of slower growth (e.g., 2008-2012) align with economic disruptions like the Global Financial Crisis.\nAccelerated Growth After 2018: Likely driven by Singapore’s push for digital services, financial innovation, and trade agreements.\n\n\nGraph()Code()\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(forecast)\nlibrary(plotly)\n\n# Convert data from wide to long format\nexport_vs_import_long &lt;- export_vs_import %&gt;%\n  pivot_longer(cols = -`Data Series`, names_to = \"Year\", values_to = \"Value\") %&gt;%\n  mutate(Year = as.numeric(Year)) %&gt;%\n  arrange(Year)  # Ensure years are sorted in ascending order\n\n# Create separate data frames for each category to ensure proper order\nexports_data &lt;- export_vs_import_long %&gt;% filter(`Data Series` == \"Exports Of Services\")\nimports_data &lt;- export_vs_import_long %&gt;% filter(`Data Series` == \"Imports Of Services\")\ntotal_trade_data &lt;- export_vs_import_long %&gt;% filter(`Data Series` == \"Total Trade In Services\")\n\n# Convert to time-series format (with correctly sorted values)\nexports_ts &lt;- ts(exports_data$Value, start = exports_data$Year[1], frequency = 1)\nimports_ts &lt;- ts(imports_data$Value, start = imports_data$Year[1], frequency = 1)\ntotal_trade_ts &lt;- ts(total_trade_data$Value, start = total_trade_data$Year[1], frequency = 1)\n\n# Create Data Frame for Plotting\ndf &lt;- data.frame(\n  Year = rep(exports_data$Year, 3),\n  Value = c(exports_data$Value, imports_data$Value, total_trade_data$Value),\n  Category = rep(c(\"Exports\", \"Imports\", \"Total Trade\"), each = nrow(exports_data))\n)\n\n# Create a ggplot with explicit group aesthetic\np &lt;- ggplot(df, aes(x = Year, y = Value, color = Category, group = Category, \n                     text = paste0(\"Year: \", Year, \"&lt;br&gt;Value: \", round(Value, 1)))) +\n  geom_line(linewidth = 1.2) +  # Use linewidth instead of size (ggplot2 3.4.0+)\n  scale_color_manual(values = c(\"Exports\" = \"red\", \"Imports\" = \"blue\", \"Total Trade\" = \"black\")) +\n  ggtitle(\"Export vs Import vs Total Trade Trends\") +\n  ylab(\"Value (in Millions)\") + xlab(\"Year\") +\n  theme_minimal()\n\n# Convert to interactive plot with tooltips\ninteractive_plot &lt;- ggplotly(p, tooltip = \"text\")\n\n# Display interactive plot\ninteractive_plot\n\n\n\n\n\n\n\n\n\n\nBased on the model accuracy metrics, ARIMA outperforms ETS in forecasting both exports and imports. ARIMA consistently shows lower RMSE, MAE, and MAPE values, indicating that its predictions are more precise and closer to actual observations. Thus, ARIMA will be the preferred model for time series forecasting.\n\nResult()Code()\n\n\n\n\n                    ME     RMSE       MAE        MPE      MAPE      MASE\nTraining set  2250.362 11275.74  8425.338 -0.1740338  7.973583 0.5737539\nTest set     66791.835 86554.64 77415.700 13.2622158 16.803450 5.2719028\n                   ACF1\nTraining set 0.05311589\nTest set             NA\n\n\n                    ME     RMSE       MAE       MPE      MAPE      MASE\nTraining set  3516.706 10885.93  7472.406  2.582523  5.207104 0.5088606\nTest set     54848.687 74121.02 67064.995 10.655012 14.727052 4.5670340\n                  ACF1\nTraining set -0.109228\nTest set            NA\n\n\n# A tibble: 4 × 5\n  Model Type      RMSE    MAE  MAPE\n  &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 ETS   Exports 86555. 77416.  16.8\n2 ETS   Imports 89216. 74235.  17.4\n3 ARIMA Exports 74121. 67065.  14.7\n4 ARIMA Imports 81957. 68397.  16.0\n\n\n\n\n\n\nCode\nlibrary(forecast)\nlibrary(tidyverse)\nlibrary(ggplot2)\n\n# Set forecast horizon (e.g., last 5 years)\nhorizon &lt;- 5\n\n# Ensure there are enough observations for training and test\nif (length(exports_ts) &lt;= horizon | length(imports_ts) &lt;= horizon) {\n  stop(\"Not enough data points for proper train-test split!\")\n}\n\n# Split data: Use last `horizon` years as the test set\ntrain_exports &lt;- head(exports_ts, length(exports_ts) - horizon)\ntest_exports &lt;- tail(exports_ts, horizon)\n\ntrain_imports &lt;- head(imports_ts, length(imports_ts) - horizon)\ntest_imports &lt;- tail(imports_ts, horizon)\n\n# Fit ETS models\nets_exports &lt;- ets(train_exports)\nets_imports &lt;- ets(train_imports)\n\n# Fit ARIMA models\narima_exports &lt;- auto.arima(train_exports)\narima_imports &lt;- auto.arima(train_imports)\n\n# 🔹 Generate Forecasts BEFORE extracting forecast values\nets_forecast_exports &lt;- forecast(ets_exports, h = horizon)\nets_forecast_imports &lt;- forecast(ets_imports, h = horizon)\n\narima_forecast_exports &lt;- forecast(arima_exports, h = horizon)\narima_forecast_imports &lt;- forecast(arima_imports, h = horizon)\n\n# 🔹 Extract the forecasted mean values\nets_forecast_exports_values &lt;- as.numeric(ets_forecast_exports$mean)\nets_forecast_imports_values &lt;- as.numeric(ets_forecast_imports$mean)\n\narima_forecast_exports_values &lt;- as.numeric(arima_forecast_exports$mean)\narima_forecast_imports_values &lt;- as.numeric(arima_forecast_imports$mean)\n\n# Ensure test sets are numeric\ntest_exports &lt;- as.numeric(test_exports)\ntest_imports &lt;- as.numeric(test_imports)\n\n# Ensure test and forecast lengths match\nif (length(test_exports) != length(ets_forecast_exports_values)) {\n  test_exports &lt;- head(test_exports, length(ets_forecast_exports_values))\n}\n\nif (length(test_imports) != length(ets_forecast_imports_values)) {\n  test_imports &lt;- head(test_imports, length(ets_forecast_imports_values))\n}\n\n# 🔹 Use accuracy() correctly with the model object\nets_accuracy_exports &lt;- forecast::accuracy(ets_forecast_exports, test_exports)\nets_accuracy_imports &lt;- forecast::accuracy(ets_forecast_imports, test_imports)\n\narima_accuracy_exports &lt;- forecast::accuracy(arima_forecast_exports, test_exports)\narima_accuracy_imports &lt;- forecast::accuracy(arima_forecast_imports, test_imports)\n\nprint(ets_accuracy_exports)\nprint(arima_accuracy_exports)\n\n# 🔹 Combine accuracy results into a structured dataframe\naccuracy_df &lt;- tibble(\n  Model = rep(c(\"ETS\", \"ARIMA\"), each = 2),\n  Type = rep(c(\"Exports\", \"Imports\"), times = 2),\n  RMSE = c(ets_accuracy_exports[\"Test set\", \"RMSE\"], \n           ets_accuracy_imports[\"Test set\", \"RMSE\"],\n           arima_accuracy_exports[\"Test set\", \"RMSE\"], \n           arima_accuracy_imports[\"Test set\", \"RMSE\"]),\n  MAE = c(ets_accuracy_exports[\"Test set\", \"MAE\"], \n          ets_accuracy_imports[\"Test set\", \"MAE\"],\n          arima_accuracy_exports[\"Test set\", \"MAE\"], \n          arima_accuracy_imports[\"Test set\", \"MAE\"]),\n  MAPE = c(ets_accuracy_exports[\"Test set\", \"MAPE\"], \n           ets_accuracy_imports[\"Test set\", \"MAPE\"],\n           arima_accuracy_exports[\"Test set\", \"MAPE\"], \n           arima_accuracy_imports[\"Test set\", \"MAPE\"])\n)\n\n# Print final accuracy dataframe\nprint(accuracy_df)\n\n\n\n\n\n\n\n\nNext, I applied ARIMA modeling (auto.arima()) using the forecast package to predict trade values for the next five years (2025-2029).\n\nARIMA Forecast for Exports()ARIMA Forecast for Imports()ARIMA Forecast for Total Trade()Combined Forecast()Code()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(forecast)\n\n# Apply ARIMA Model\narima_exports &lt;- auto.arima(exports_ts)\narima_imports &lt;- auto.arima(imports_ts)\narima_total_trade &lt;- auto.arima(total_trade_ts)\n\n# Forecast for the next 5 years\narima_forecast_exports &lt;- forecast(arima_exports, h = 5)\narima_forecast_imports &lt;- forecast(arima_imports, h = 5)\narima_forecast_total_trade &lt;- forecast(arima_total_trade, h = 5)\n\n# Plot ARIMA forecasts\nautoplot(arima_forecast_exports) + ggtitle(\"ARIMA Forecast for Exports\")\nautoplot(arima_forecast_imports) + ggtitle(\"ARIMA Forecast for Imports\")\nautoplot(arima_forecast_total_trade) + ggtitle(\"ARIMA Forecast for Total Trade\")\n\n# Convert forecasts to data frames for ggplot\ndf_exports &lt;- as.data.frame(arima_forecast_exports) %&gt;% mutate(Year = seq(max(exports_data$Year) + 1, by = 1, length.out = 5), Category = \"Exports\")\ndf_imports &lt;- as.data.frame(arima_forecast_imports) %&gt;% mutate(Year = seq(max(imports_data$Year) + 1, by = 1, length.out = 5), Category = \"Imports\")\ndf_total_trade &lt;- as.data.frame(arima_forecast_total_trade) %&gt;% mutate(Year = seq(max(total_trade_data$Year) + 1, by = 1, length.out = 5), Category = \"Total Trade\")\n\n# Combine all forecasts\ndf_forecast &lt;- bind_rows(df_exports, df_imports, df_total_trade)\n\n# Create the forecast plot\nggplot(df_forecast, aes(x = Year, y = `Point Forecast`, color = Category)) +\n  geom_line(linewidth = 1.5) +  # Use linewidth instead of size (ggplot2 3.4.0+)\n  scale_color_manual(values = c(\"Exports\" = \"red\", \"Imports\" = \"blue\", \"Total Trade\" = \"black\")) +\n  ggtitle(\"ARIMA Forecast for Exports, Imports, and Total Trade\") +\n  ylab(\"Forecasted Value (in Millions)\") + xlab(\"Year\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWe will also be testing the accuracy of the forecast model to evaluate its performance and reliability.\nStandard market practices for evaluating forecast accuracy rely on key metrics such as MAPE, RMSE, MAE, and ACF1.\n\nBased on these measures, the imports forecast is highly accurate (MAPE ~4.84%), while exports and total trade forecasts fall within the “good” range (MAPE ~6.10% and ~5.23%).\nThe absence of significant autocorrelation in residuals suggests the models effectively capture trends.\nThe high RMSE values, particularly for total trade, indicate potential reliability issues due to large fluctuations in data.\n\n\nForecast accuracy for Exports()Forecast accuracy for Imports()Forecast accuracy for Total Trade()Code()\n\n\n\n\n### Forecast Accuracy for Exports ###\n\n\n                      ME      RMSE       MAE        MPE      MAPE       MASE\nTraining set    5842.817  21775.54  13733.86   2.777663  6.099909  0.6402954\nTest set     -215550.380 217655.84 215550.38 -53.417606 53.417606 10.0493182\n                    ACF1\nTraining set -0.09397345\nTest set              NA\n\n\n\n\n\n\n\n### Forecast Accuracy for Imports ###\n\n\n                      ME      RMSE       MAE        MPE      MAPE       MASE\nTraining set    3887.432  15372.93  10045.78   1.659329  4.840996  0.5509206\nTest set     -182952.628 184188.44 182952.63 -49.695067 49.695067 10.0333023\n                   ACF1\nTraining set 0.03707208\nTest set             NA\n\n\n\n\n\n\n\n### Forecast Accuracy for Total Trade ###\n\n\n                      ME      RMSE       MAE        MPE      MAPE       MASE\nTraining set    9537.844  35750.43  23074.03   2.170273  5.229894  0.5884102\nTest set     -399957.217 403059.64 399957.22 -51.756773 51.756773 10.1992978\n                    ACF1\nTraining set -0.02527777\nTest set              NA\n\n\n\n\n\n\nCode\n# Compute and print accuracy\ncat(\"### Forecast Accuracy for Exports ###\\n\")\nprint(forecast::accuracy(arima_forecast_exports, test_exports))\n\n# Print forecast accuracy for imports\ncat(\"\\n### Forecast Accuracy for Imports ###\\n\")\nprint(forecast::accuracy(arima_forecast_imports, test_imports))\n\n# Print forecast accuracy for total trade\ncat(\"\\n### Forecast Accuracy for Total Trade ###\\n\")\nprint(forecast::accuracy(arima_forecast_total_trade, test_total_trade))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe original visualizations analyzed in this analysis are sourced from here.\n\n\n\n\n\nIn the below section, we will evaluate the effectiveness of the above visualization by identifying its pros and cons, focusing on aspects such as (1) clarity, and (2) visual appeal:\n\nClarity: How well the data is presented and understood\n\n\n\n\n\n\n\n\n\nPros\nCons\nSuggested fixes\n\n\n\n\nClearly labeled service categories with values and percentages – Each category is distinctly labeled, making it easy to understand the contributions of each service type.\nPie charts are not effective for comparing data slices – When multiple categories have close values, it is difficult to differentiate them accurately.\nReplace pie charts with bar charts or grouped bar charts - for clearer comparison of service categories.\n\n\nPercentage values enhance clarity – Helps users quickly understand each category’s contribution to total exports/ imports.\nPie charts are only useful when there are limited categories – Too many slices make it difficult to interpret, as smaller sections become unreadable.\nReplace with a bar chart or grouped bar chart to improve readability.\n\n\n\nThe pie chart format makes trend comparison difficult – Pie charts only represent a single year and do not show how exports have changed over time.\nAdd a trend chart (e.g., line graph or bar chart) – Including historical data can help identify patterns and trends over multiple years.\n\n\n\n\nVisual appeal: How visually engaging and effective the design is\n\n\n\n\n\n\n\n\n\nPros\nCons\nSuggested fixes\n\n\n\n\nGood use of icons to represent different services – This makes the chart more engaging and helps with intuitive understanding.\nPie charts distort perception – it’s hard to accurately compare slice sizes, especially when they are similar.\nUse a bar chart, grouped bar chart or treemaps, which allows for more proportional representation of each service category.\n\n\nWell-structured layout with categories spread around the chart - The service categories for both exports and imports maintain a similar placement\nPie chart format is too cluttered – The large number of slices makes it difficult to read smaller segments and their labels.\nUse a bar chart, grouped bar chart or treemaps for better spacing and spatial representation.\n\n\n\nToo many elements are packed into the chart, making it overwhelming.\nSimplify by removing non-essential elements - i.e: icons\n\n\n\nNo legend for color segmentation – The chart uses various colors for different service categories, but there is no clear legend explaining their grouping or significance.\nInclude a legend or categorize colors meaningfully – Assign gradient based color scale to visually emphasize trade volume intensity.\n\n\n\n\n\n\n\n\n\nLimitations of pie charts in data visualization\n\n\n\n\nDistorted perception: Pie chart makes comparisons difficult due to the reliance on angles and areas rather than a common baseline.\nDifficult to compare similar data slices: Can be misleading when there are many segments/ similar-sized portions making it hard to interpret differences accurately.\nSpace constraints: Pie chart can take up more space than necessary and can clutter dashboards or reports.\nPoor for trend analysis: Pie charts only show a single point in time and do not help in comparing trends over multiple years.\n\n\n\n\n\n\n\n\n\nWhy are the use of pie charts be frown upon in data visualization?\n\n\n\n\nRefer to this page to find out why the use of pie charts are discouraged.\n\n\n\n\n\n\n\n\n\nAfter importing the trade_services data set, we will filter for the key categories that contribute to the exports/ imports of services in Singapore, excluding subcategories. The 12 major categories are identified based on indentation, as shown below:\n\n\n\n\n\n\n\nData Series\nShortened label\n\n\n\n\nManufacturing Services On Physical Inputs Owned By Others\nManuf. Services\n\n\nMaintenance And Repair Services\nMaintenance & Repair\n\n\nTransport\nTransport\n\n\nTravel\nTravel\n\n\nInsurance\nInsurance\n\n\nGovernment Goods And Services\nGovt. Services\n\n\nConstruction\nConstruction\n\n\nFinancial\nFinancial Services\n\n\nTelecommunications, Computer & Information\nTelecom & IT\n\n\nCharges For The Use Of Intellectual Property\nIntellectual Property\n\n\nPersonal, Cultural And Recreational\nCultural & Recreational\n\n\nOther Business Services\nOther Biz Services\n\n\n\nWe first clean the “Data Series” column by removing leading and trailing spaces to ensure consistency. Next, we identify the positions of “Exports Of Services” and “Imports Of Services” in the dataset, extracting only the rows between these markers for exports and those following the imports marker for imports. We then filter both datasets to retain only the 12 major service categories as hghlighted above.\nFinally, we combine the cleaned exports and imports data into a single structured table, adding a “Trade Type” column to differentiate between imports/ exports.\n\n\nCode\n# Trim leading/trailing spaces from 'Data Series' column\ntrade_services$`Data Series` &lt;- trimws(trade_services$`Data Series`)\n\n# Identify row indices for \"Exports Of Services\" and \"Imports Of Services\"\nexport_start_idx &lt;- which(trade_services$`Data Series` == \"Exports Of Services\")\nimport_start_idx &lt;- which(trade_services$`Data Series` == \"Imports Of Services\")\n\n# Extract rows between \"Exports Of Services\" and \"Imports Of Services\"\nexports_df &lt;- trade_services[(export_start_idx + 1):(import_start_idx - 1), ]\nimports_df &lt;- trade_services[(import_start_idx + 1):nrow(trade_services), ]\n\n# Define the 12 major categories\nmajor_categories &lt;- c(\n  \"Manufacturing Services On Physical Inputs Owned By Others\",\n  \"Maintenance And Repair Services\",\n  \"Transport\",\n  \"Travel\",\n  \"Insurance\",\n  \"Government Goods And Services\",\n  \"Construction\",\n  \"Financial\",\n  \"Telecommunications, Computer & Information\",\n  \"Charges For The Use Of Intellectual Property\",\n  \"Personal, Cultural And Recreational\",\n  \"Other Business Services\"\n)\n\n# Filter only the major categories for exports and imports\nexports_major &lt;- exports_df %&gt;% filter(`Data Series` %in% major_categories)\nimports_major &lt;- imports_df %&gt;% filter(`Data Series` %in% major_categories)\n\n# Add a column to indicate whether it's exports or imports\nexports_major$`Trade Type` &lt;- \"Exports\"\nimports_major$`Trade Type` &lt;- \"Imports\"\n\n# Combine into a single dataframe\nfinal_trade_df &lt;- bind_rows(exports_major, imports_major)\n\n# Reorder columns: Move \"Trade Type\" to the second column\nfinal_trade_df &lt;- final_trade_df %&gt;% select(`Data Series`, `Trade Type`, everything())\n\n# View the final structured data\nprint(final_trade_df)\n\n\n# A tibble: 24 × 27\n   `Data Series`   `Trade Type` `2024` `2023` `2022` `2021` `2020` `2019` `2018`\n   &lt;chr&gt;           &lt;chr&gt;         &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 Manufacturing … Exports      4.71e2 4.91e2 6.85e2 4.90e2   237.   267.   370.\n 2 Maintenance An… Exports      1.08e4 1.10e4 1.01e4 8.83e3  8173.  9663.  9410 \n 3 Transport       Exports      1.73e5 1.50e5 1.90e5 1.44e5 93560  94272. 88889.\n 4 Travel          Exports      3.19e4 2.79e4 1.57e4 5.40e3  7527. 27755. 27367.\n 5 Insurance       Exports      1.26e4 1.11e4 1.15e4 9.13e3  8438   8817.  9485.\n 6 Government Goo… Exports      4.77e2 4.64e2 4.44e2 4.22e2   412.   419.   414.\n 7 Construction    Exports      2.10e3 2.04e3 1.62e3 1.20e3  1272.  1742.  1642.\n 8 Financial       Exports      7.16e4 6.53e4 5.61e4 5.31e4 49688. 45944. 42840.\n 9 Telecommunicat… Exports      4.11e4 3.93e4 3.42e4 3.19e4 26826. 20252. 20533.\n10 Charges For Th… Exports      2.63e4 2.42e4 1.79e4 1.64e4 12866. 12297. 12084.\n# ℹ 14 more rows\n# ℹ 18 more variables: `2017` &lt;dbl&gt;, `2016` &lt;dbl&gt;, `2015` &lt;dbl&gt;, `2014` &lt;dbl&gt;,\n#   `2013` &lt;dbl&gt;, `2012` &lt;dbl&gt;, `2011` &lt;dbl&gt;, `2010` &lt;dbl&gt;, `2009` &lt;dbl&gt;,\n#   `2008` &lt;dbl&gt;, `2007` &lt;dbl&gt;, `2006` &lt;dbl&gt;, `2005` &lt;dbl&gt;, `2004` &lt;dbl&gt;,\n#   `2003` &lt;dbl&gt;, `2002` &lt;dbl&gt;, `2001` &lt;dbl&gt;, `2000` &lt;dbl&gt;\n\n\n\n\n\nKey makeover changes:\n1️⃣ Single chart for imports and exports:\n\nCombined visualization –&gt; Instead of two separate pie charts, the new bar chart combines both, allowing for side-by-side comparison.\n\n2️⃣ Improved readability and comparability:\n\nGrouped bars (red for exports, blue for imports) –&gt; Offer a clearer visual distinction between trade types.\nSorted categories (largest to smallest) –&gt; Focus attention on key contributors, ensuring the most impactful sectors are easily identified.\n\n3️⃣ Simplified layout:\n\nRemoved unnecessary graphics (i.e.: icons, maps) –&gt;Decluttered the chart to emphasize trade data information.\n\n4️⃣ Interactive insights:\n\nHover tooltips –&gt; Display category names, trade values, and percentages shares, making the chart more dynamic and user-friendly.\n\n\n\n\n\n\n\nOverall\n\n\n\n\nBetter clarity, accuracy, and direct comparison in a single view.\nEliminates redundancy and improves clarity for faster data interpretation.\n\n\n\n\nEnhance graph()Code()\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Load required libraries\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(plotly)  # For interactive tooltips\n\n# Filter data for the year 2024\ntrade_2024 &lt;- final_trade_df %&gt;%\n  select(`Data Series`, `Trade Type`, `2024`) %&gt;%\n  rename(Value = `2024`)\n\n# Shorten long category labels for better readability\ntrade_2024$`Data Series` &lt;- gsub(\"Manufacturing Services On Physical Inputs Owned By Others\", \"Manuf. Services\", trade_2024$`Data Series`)\ntrade_2024$`Data Series` &lt;- gsub(\"Maintenance And Repair Services\", \"Maintenance & Repair\", trade_2024$`Data Series`)\ntrade_2024$`Data Series` &lt;- gsub(\"Government Goods And Services\", \"Govt. Services\", trade_2024$`Data Series`)\ntrade_2024$`Data Series` &lt;- gsub(\"Financial\", \"Financial Services\", trade_2024$`Data Series`)\ntrade_2024$`Data Series` &lt;- gsub(\"Telecommunications, Computer & Information\", \"Telecom & IT\", trade_2024$`Data Series`)\ntrade_2024$`Data Series` &lt;- gsub(\"Charges For The Use Of Intellectual Property\", \"Intellectual Property\", trade_2024$`Data Series`)\ntrade_2024$`Data Series` &lt;- gsub(\"Personal, Cultural And Recreational\", \"Cultural & Recreational\", trade_2024$`Data Series`)\ntrade_2024$`Data Series` &lt;- gsub(\"Other Business Services\", \"Other Biz Services\", trade_2024$`Data Series`)\n\n# Compute Total Exports & Imports for Percentage Calculation\ntotal_exports &lt;- sum(trade_2024$Value[trade_2024$`Trade Type` == \"Exports\"], na.rm = TRUE)\ntotal_imports &lt;- sum(trade_2024$Value[trade_2024$`Trade Type` == \"Imports\"], na.rm = TRUE)\n\n# Add Percentage Column\ntrade_2024 &lt;- trade_2024 %&gt;%\n  mutate(Percentage = ifelse(`Trade Type` == \"Exports\", \n                             Value / total_exports * 100, \n                             Value / total_imports * 100))\n\n# Sum values for sorting (descending order)\ncategory_order &lt;- trade_2024 %&gt;%\n  group_by(`Data Series`) %&gt;%\n  summarise(Total_Trade = sum(Value, na.rm = TRUE)) %&gt;%\n  arrange(desc(Total_Trade)) %&gt;%\n  pull(`Data Series`)\n\n# Convert `Data Series` to factor for correct sorting\ntrade_2024$`Data Series` &lt;- factor(trade_2024$`Data Series`, levels = category_order)\n\n# Define custom colors for Exports (Red) and Imports (Blue)\ncustom_colors &lt;- c(\"Exports\" = \"red\", \"Imports\" = \"blue\")\n\n# Create the ggplot object\ngg &lt;- ggplot(trade_2024, aes(x = `Data Series`, y = Value, fill = `Trade Type`,\n                             text = paste(\"Category:\", `Data Series`, \n                                          \"&lt;br&gt;Trade Type:\", `Trade Type`, \n                                          \"&lt;br&gt;Value: S$\", scales::comma(Value),\n                                          \"&lt;br&gt;Share:\", round(Percentage, 1), \"%\"))) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.8)) +  # Side-by-side bars\n  scale_fill_manual(values = custom_colors) +  # Apply custom colors\n  scale_y_continuous(labels = scales::comma) +  # Format y-axis labels\n  labs(title = \"Singapore Trade in Services - 2024\",\n       subtitle = \"Side-by-Side Comparison of Imports and Exports for 12 Service Categories\",\n       x = \"Service Category\",\n       y = \"Trade Value\",\n       fill = \"Trade Type\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),  # Rotate x-axis labels\n        legend.position = \"bottom\")  \n\n# Convert to interactive plot with tooltips\nggplotly(gg, tooltip = \"text\")\n\n\n\n\n\n\n\n\nKey makeover changes:\n1️⃣ Transition from Pie charts to Treemaps\n\nPie charts are replaced with treemaps –&gt; Treemaps uses area size to provide a better spatial comparison instead of circular slices in pie charts\n\n2️⃣ Added plotly inteactive features:\n\nEnabled interactive treempas using plotly –&gt; Allows for hovering over categories to explore trade value more dynamically.\n\n3️⃣ Gradient-based color scale for trade value:\n\nApplied green gradient (darker green = higher value, lighter green = lower value) –&gt; Visually emphasizes trade volume intensity instead of random colors in pie charts.\n\n\nEnhance graph()Code()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Load required libraries\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(treemap)\nlibrary(plotly)\n\n# Filter data for the year 2024\ntrade_2024 &lt;- final_trade_df %&gt;%\n  select(`Data Series`, `Trade Type`, `2024`) %&gt;%\n  rename(Value = `2024`)\n\n# Shorten long category labels for better readability\ntrade_2024 &lt;- trade_2024 %&gt;%\n  mutate(Short_Label = case_when(\n    `Data Series` == \"Manufacturing Services On Physical Inputs Owned By Others\" ~ \"Manuf. Services\",\n    `Data Series` == \"Maintenance And Repair Services\" ~ \"Maintenance & Repair\",\n    `Data Series` == \"Government Goods And Services\" ~ \"Govt. Services\",\n    `Data Series` == \"Financial\" ~ \"Financial Services\",\n    `Data Series` == \"Telecommunications, Computer & Information\" ~ \"Telecom & IT\",\n    `Data Series` == \"Charges For The Use Of Intellectual Property\" ~ \"Intellectual Property\",\n    `Data Series` == \"Personal, Cultural And Recreational\" ~ \"Cultural & Recreational\",\n    `Data Series` == \"Other Business Services\" ~ \"Other Biz Services\",\n    TRUE ~ `Data Series`\n  ))\n\n# Split the dataset into Exports and Imports\ntrade_exports &lt;- trade_2024 %&gt;% filter(`Trade Type` == \"Exports\")\ntrade_imports &lt;- trade_2024 %&gt;% filter(`Trade Type` == \"Imports\")\n\n# Create Treemap Data for Exports\ntreemap_exports &lt;- treemap(trade_exports,\n                           index = \"Short_Label\",\n                           vSize = \"Value\",\n                           type = \"index\",\n                           palette = \"Blues\",\n                           title = \"Exports Treemap - 2024\",\n                           draw = FALSE)\n\n# Create Treemap Data for Imports\ntreemap_imports &lt;- treemap(trade_imports,\n                           index = \"Short_Label\",\n                           vSize = \"Value\",\n                           type = \"index\",\n                           palette = \"Oranges\",\n                           title = \"Imports Treemap - 2024\",\n                           draw = FALSE)\n\n# Convert Exports Treemap Data to Plotly (Darker Green for Higher Values)\np1_interactive &lt;- plot_ly(\n  data = treemap_exports$tm,\n  labels = ~Short_Label,\n  parents = \"\",\n  values = ~vSize,\n  text = ~paste(\"Category:\", Short_Label, \"&lt;br&gt;Value:\", vSize),\n  type = \"treemap\",\n  textinfo = \"label+text\",\n  marker = list(\n    colorscale = list(c(0, 1), c(\"#d9f2d9\", \"#006400\")),  # Light to Dark Green\n    cmin = min(treemap_exports$tm$vSize),\n    cmax = max(treemap_exports$tm$vSize),\n    colorbar = list(title = \"Trade Value\")\n  )\n) %&gt;%\n  layout(title = \"Exports Treemap - 2024\")\n\n# Convert Imports Treemap Data to Plotly (Darker Green for Higher Values)\np2_interactive &lt;- plot_ly(\n  data = treemap_imports$tm,\n  labels = ~Short_Label,\n  parents = \"\",\n  values = ~vSize,\n  text = ~paste(\"Category:\", Short_Label, \"&lt;br&gt;Value:\", vSize),\n  type = \"treemap\",\n  textinfo = \"label+text\",\n  marker = list(\n    colorscale = list(c(0, 1), c(\"#d9f2d9\", \"#006400\")),  # Light to Dark Green\n    cmin = min(treemap_imports$tm$vSize),\n    cmax = max(treemap_imports$tm$vSize),\n    colorbar = list(title = \"Trade Value\")\n  )\n) %&gt;%\n  layout(title = \"Imports Treemap - 2024\")\n\np1_interactive\np2_interactive\n\n\n\n\n\n\n\n\n\n\n\nKey observations:\n\nRapid Growth: Singapore’s service exports have grown significantly, reinforcing its role as a global trade hub.\nLeading Sectors: Transport and Other Business Services dominate, showing the steepest growth.\nFinancial & Digital Expansion: Financial Services, Telecom & IT, and Intellectual Property have surged, driven by digitalization and economic policies.\nResilience & Acceleration: Despite slower growth around 2012 (likely due to economic shocks), post-2018 expansion highlights policy-driven trade growth.\n\n\n\n\n\n\n\nNote\n\n\n\n\nSingapore’s services exports continue to thrive, reflecting its strong economic positioning\n\n\n\n\nTime series()Code()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n#| fig-width: 8\n#| fig-height: 6\n\n# Load required libraries\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(CGPfunctions)\n\n# Ensure column names are trimmed properly\ncolnames(final_trade_df) &lt;- trimws(colnames(final_trade_df))\n\n# Shorten long category labels for better readability (Fixed repetitive label issue)\nfinal_trade_df$`Data Series` &lt;- gsub(\"Manufacturing Services On Physical Inputs Owned By Others\", \"Manuf. Services\", final_trade_df$`Data Series`)\nfinal_trade_df$`Data Series` &lt;- gsub(\"Maintenance And Repair Services\", \"Maintenance & Repair\", final_trade_df$`Data Series`)\nfinal_trade_df$`Data Series` &lt;- gsub(\"Government Goods And Services\", \"Govt. Services\", final_trade_df$`Data Series`)\nfinal_trade_df$`Data Series` &lt;- gsub(\"^Financial$\", \"Financial Services\", final_trade_df$`Data Series`)  \nfinal_trade_df$`Data Series` &lt;- gsub(\"Telecommunications, Computer & Information\", \"Telecom & IT\", final_trade_df$`Data Series`)\nfinal_trade_df$`Data Series` &lt;- gsub(\"Charges For The Use Of Intellectual Property\", \"Intellectual Property\", final_trade_df$`Data Series`)\nfinal_trade_df$`Data Series` &lt;- gsub(\"Personal, Cultural And Recreational\", \"Cultural & Recreational\", final_trade_df$`Data Series`)\nfinal_trade_df$`Data Series` &lt;- gsub(\"Other Business Services\", \"Other Biz Services\", final_trade_df$`Data Series`)\n\n# Filter for Exports only\nexports_df &lt;- final_trade_df %&gt;% filter(`Trade Type` == \"Exports\")\n\n# Select relevant columns: Data Series, 2000, 2012, 2024\nexports_slope &lt;- exports_df %&gt;%\n  select(`Data Series`, `2000`, `2012`, `2024`) %&gt;%\n  drop_na()\n\n# Keep only the top 10 services in 2024 for clarity\nexports_slope &lt;- exports_slope %&gt;% arrange(desc(`2024`)) %&gt;% head(10)\n\n# Convert data to long format for `newggslopegraph()`\nexports_long &lt;- exports_slope %&gt;%\n  pivot_longer(cols = c(`2000`, `2012`, `2024`), names_to = \"Year\", values_to = \"Value\")\n\n# Convert Year to character (Fix for newggslopegraph)\nexports_long$Year &lt;- as.character(exports_long$Year)\n\n# Increase figure height to improve label spacing\noptions(repr.plot.width = 14, repr.plot.height = 12)\n\n# Create slopegraph with **2000, 2012, and 2024**\nnewggslopegraph(dataframe = exports_long,\n                Times = Year,   \n                Measurement = Value,\n                Grouping = `Data Series`,\n                Title = \"Trade in Services (Exports) - Slopegraph (2000-2012-2024)\",\n                SubTitle = \"Top 10 Services by Export Value\",\n                LineThickness = 1.0,\n                DataTextSize = 2.5)\n\n\n\n\n\n\n\n\nKey observations:\n\nStrong Growth in Imports: Similar to exports, Singapore’s service imports have grown significantly, reflecting its global trade connectivity.\nTransport & Other Business Services Lead: These two sectors dominate imports, mirroring export trends, but at slightly lower values.\nRising Financial Services & Telecom & IT: These sectors have seen notable import growth, indicating increasing reliance on foreign expertise and digital services.\nHigher Import Dependency in Intellectual Property: Compared to exports, imports in Intellectual Property have increased faster, suggesting rising licensing and royalty payments.\n\n\n\n\n\n\n\nNote\n\n\n\n\nSingapore’s service imports align with its export-driven economy, with strong growth in digital services, finance, and transport.\n\n\n\n\nTime series()Code()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n#| fig-width: 8\n#| fig-height: 6\n\n# Load required libraries\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(CGPfunctions)\n\n# Ensure column names are trimmed properly\ncolnames(final_trade_df) &lt;- trimws(colnames(final_trade_df))\n\n# Shorten long category labels for better readability (Fixed repetitive label issue)\nfinal_trade_df$`Data Series` &lt;- gsub(\"Manufacturing Services On Physical Inputs Owned By Others\", \"Manuf. Services\", final_trade_df$`Data Series`)\nfinal_trade_df$`Data Series` &lt;- gsub(\"Maintenance And Repair Services\", \"Maintenance & Repair\", final_trade_df$`Data Series`)\nfinal_trade_df$`Data Series` &lt;- gsub(\"Government Goods And Services\", \"Govt. Services\", final_trade_df$`Data Series`)\nfinal_trade_df$`Data Series` &lt;- gsub(\"^Financial$\", \"Financial Services\", final_trade_df$`Data Series`)  \nfinal_trade_df$`Data Series` &lt;- gsub(\"Telecommunications, Computer & Information\", \"Telecom & IT\", final_trade_df$`Data Series`)\nfinal_trade_df$`Data Series` &lt;- gsub(\"Charges For The Use Of Intellectual Property\", \"Intellectual Property\", final_trade_df$`Data Series`)\nfinal_trade_df$`Data Series` &lt;- gsub(\"Personal, Cultural And Recreational\", \"Cultural & Recreational\", final_trade_df$`Data Series`)\nfinal_trade_df$`Data Series` &lt;- gsub(\"Other Business Services\", \"Other Biz Services\", final_trade_df$`Data Series`)\n\n# Filter for Imports only\nimports_df &lt;- final_trade_df %&gt;% filter(`Trade Type` == \"Imports\")\n\n# Select relevant columns: Data Series, 2000, 2012, 2024\nimports_slope &lt;- imports_df %&gt;%\n  select(`Data Series`, `2000`, `2012`, `2024`) %&gt;%\n  drop_na()\n\n# Keep only the top 10 services in 2024 for clarity\nimports_slope &lt;- imports_slope %&gt;% arrange(desc(`2024`)) %&gt;% head(10)\n\n# Convert data to long format for `newggslopegraph()`\nimports_long &lt;- imports_slope %&gt;%\n  pivot_longer(cols = c(`2000`, `2012`, `2024`), names_to = \"Year\", values_to = \"Value\")\n\n# Convert Year to character (Fix for newggslopegraph)\nimports_long$Year &lt;- as.character(imports_long$Year)\n\n# Increase figure height to improve label spacing\noptions(repr.plot.width = 14, repr.plot.height = 12)\n\n# Create slopegraph with **2000, 2012, and 2024**\nnewggslopegraph(dataframe = imports_long,\n                Times = Year,   \n                Measurement = Value,\n                Grouping = `Data Series`,\n                Title = \"Trade in Services (Imports) - Slopegraph (2000-2012-2024)\",\n                SubTitle = \"Top 10 Services by Import Value\",\n                LineThickness = 1.0,\n                DataTextSize = 2.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe original visualizations analyzed in this analysis are sourced from here.\n\n\n\n\n\nIn the below section, we will evaluate the effectiveness of the above visualization by identifying its pros and cons, focusing on aspects such as (1) clarity, and (2) visual appeal:\n\nClarity: How well the data is presented and understood\n\n\n\n\n\n\n\n\n\nPros\nCons\nSuggested fixes\n\n\n\n\nClear regional differentiation - The color-coded regions (North America, Europe, Asia, Oceania) make it easier to distinguish areas.\nColor scheme inconsistency - The colors used for regions in the world map do not match the second visualization, leading to confusion.\nEnsure consistent colors across both graphics (e.g., North America should be red in both images).\n\n\nEasy country identification - The use of country flags provides quick recognition of major trading partners on the world map.\nLack of legend for flags - It is unclear whether the flags represent top import/export partners or something else.\nInclude a small legend explaining the significance of the flags.\n\n\nWell-placed title - The main title is clear and ensures users immediately understand the topic.\nWorld map does not indicate rankings - It is difficult to determine the largest trading partners.\nAdd ranking indicators - e.g.: numbered markers, or a gradient effect on the map.\n\n\n\nNeed to scroll to see rankings - Users must scroll down to understand that U.S is the top trading partner, followed by EU, and China.\nOverlay rankings on the world map so users can see top trading partners at a glance.\n\n\n\nLack of clear distinction between Regions, Countries, and Economic & Political Unions - The visualization presents them together, making it difficult to see how rankings differ.\nIntroduce clear labels or grouping for Regions, Countries, and Economic & Political Unions in both the world map and bar chart.\n\n\n\n\nVisual appeal: How visually engaging and effective the design is\n\n\n\n\n\n\n\n\n\nPros\nCons\nSuggested fixes\n\n\n\n\nEngaging world map design - The dotted world map with vibrant colors creates a modern and appealing look.\nColor inconsistency between the two visuals - North America is red in the map but blue in the bar chart.\nUse a unified color scheme so regions in both visuals match.\n\n\n\n\n\n\n\n\n\n\n\nThe code chunk below imports the (1) Exports of Services by Major Trading Partner datasets, and (2) Imports of Services by Major Trading Partner datasets downloaded from the Department of Statistics Singapore (DOS), using the read_excel() function from the readxl package. These datasets contain trade value of exports and imports of services by major trading partner across various 2000 to 2023, which will be processed and analyzed in subsequent steps.\n\n\nCode\nexport_partners &lt;- read_excel(\"data/Exports Of Services By Major Trading Partner_base.xlsx\")\n\n\n\n\nCode\nimport_partners &lt;- read_excel(\"data/Imports Of Services By Major Trading Partner_base.xlsx\")\n\n\n\n\n\n\nglimpse(): provides a transposed overview of a dataset, showing variables and their types in a concise format.\nhead(): displays the first few rows of a dataset (default is 6 rows) to give a quick preview of the data.\nsummary(): generates a statistical summary of each variable, including measures like mean, median, and range for numeric data.\nduplicated():returns a logical vector indicating which elements or rows in a vector or data frame are duplicates.\ncolSums(is.na()): counts the number of missing values (NA) in each column of the data frame.\nstr(): use str() to display the column names, data types, and a preview of the data.\n\n\nglimpse()head()summary()duplicated()colSum(is.na())str())\n\n\n\n\nCode\nglimpse(export_partners)\n\n\nRows: 68\nColumns: 25\n$ `Data Series` &lt;chr&gt; \"Asia\", \"Bangladesh\", \"Brunei Darussalam\", \"Cambodia\", \"…\n$ `2023`        &lt;dbl&gt; 170787.7, 985.7, 960.1, 475.6, 21686.1, 9909.4, 8542.6, …\n$ `2022`        &lt;dbl&gt; 174243.3, 903.9, 891.2, 389.5, 21307.3, 9730.1, 8366.6, …\n$ `2021`        &lt;dbl&gt; 147125.9, 674.8, 579.0, 256.0, 18126.2, 7817.0, 6222.5, …\n$ `2020`        &lt;dbl&gt; 115989.3, 581.6, 533.4, 250.3, 15063.2, 6337.6, 5351.4, …\n$ `2019`        &lt;dbl&gt; 114573.1, 642.4, 556.4, 259.1, 11273.2, 6383.4, 6781.8, …\n$ `2018`        &lt;dbl&gt; 104182.5, 664.0, 495.9, 290.4, 10466.3, 5957.8, 5895.2, …\n$ `2017`        &lt;dbl&gt; 85093.3, 547.7, 463.9, 226.8, 8239.5, 5326.3, 5615.7, 25…\n$ `2016`        &lt;dbl&gt; 70461.8, 482.6, 420.5, 196.1, 7351.8, 4422.4, 5088.6, 23…\n$ `2015`        &lt;dbl&gt; 69881.1, 424.3, 465.4, 114.3, 7260.0, 4236.4, 5032.0, 17…\n$ `2014`        &lt;dbl&gt; 62802.7, 384.3, 520.4, 86.3, 5841.4, 4276.0, 4837.1, 159…\n$ `2013`        &lt;dbl&gt; 55949.9, 347.5, 474.4, 89.2, 5184.6, 4857.4, 4535.8, 155…\n$ `2012`        &lt;dbl&gt; 50496.3, 349.1, 464.1, 115.0, 5156.3, 4375.0, 4062.7, 14…\n$ `2011`        &lt;dbl&gt; 48318.2, 333.5, 468.5, 109.2, 5104.8, 4244.7, 3537.8, 13…\n$ `2010`        &lt;dbl&gt; 46254.5, 272.6, 508.1, 78.4, 4928.8, 4089.3, 3291.1, 114…\n$ `2009`        &lt;chr&gt; \"40325.300000000003\", \"209.3\", \"324.39999999999998\", \"99…\n$ `2008`        &lt;dbl&gt; 43935.5, 264.8, 272.6, 139.7, 4676.6, 3502.5, 3321.9, 93…\n$ `2007`        &lt;dbl&gt; 39789.5, 284.0, 248.3, 88.1, 4429.7, 3191.3, 3026.0, 136…\n$ `2006`        &lt;dbl&gt; 33098.0, 285.5, 120.6, 110.8, 3555.3, 2448.5, 2969.8, 10…\n$ `2005`        &lt;chr&gt; \"28070.400000000001\", \"274.60000000000002\", \"123.3\", \"77…\n$ `2004`        &lt;chr&gt; \"25457.1\", \"188.9\", \"138.6\", \"68.7\", \"2313.6999999999998…\n$ `2003`        &lt;chr&gt; \"18942\", \"157\", \"151.4\", \"45.3\", \"2059.6999999999998\", \"…\n$ `2002`        &lt;chr&gt; \"17741.7\", \"118.3\", \"144.30000000000001\", \"37.9\", \"1912\"…\n$ `2001`        &lt;chr&gt; \"16226.5\", \"121.8\", \"97.8\", \"29.8\", \"1866.5\", \"735.4\", \"…\n$ `2000`        &lt;chr&gt; \"15816.9\", \"89.7\", \"100.5\", \"18.399999999999999\", \"1860.…\n\n\n\n\n\n\nCode\nhead(export_partners)\n\n\n# A tibble: 6 × 25\n  `Data Series`   `2023` `2022` `2021` `2020` `2019` `2018` `2017` `2016` `2015`\n  &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Asia            1.71e5 1.74e5 1.47e5 1.16e5 1.15e5 1.04e5 85093. 70462. 69881.\n2 Bangladesh      9.86e2 9.04e2 6.75e2 5.82e2 6.42e2 6.64e2   548.   483.   424.\n3 Brunei Darussa… 9.60e2 8.91e2 5.79e2 5.33e2 5.56e2 4.96e2   464.   420.   465.\n4 Cambodia        4.76e2 3.89e2 2.56e2 2.50e2 2.59e2 2.90e2   227.   196.   114.\n5 Hong Kong       2.17e4 2.13e4 1.81e4 1.51e4 1.13e4 1.05e4  8240.  7352.  7260 \n6 India           9.91e3 9.73e3 7.82e3 6.34e3 6.38e3 5.96e3  5326.  4422.  4236.\n# ℹ 15 more variables: `2014` &lt;dbl&gt;, `2013` &lt;dbl&gt;, `2012` &lt;dbl&gt;, `2011` &lt;dbl&gt;,\n#   `2010` &lt;dbl&gt;, `2009` &lt;chr&gt;, `2008` &lt;dbl&gt;, `2007` &lt;dbl&gt;, `2006` &lt;dbl&gt;,\n#   `2005` &lt;chr&gt;, `2004` &lt;chr&gt;, `2003` &lt;chr&gt;, `2002` &lt;chr&gt;, `2001` &lt;chr&gt;,\n#   `2000` &lt;chr&gt;\n\n\n\n\n\n\nCode\nsummary(export_partners)\n\n\n Data Series             2023               2022               2021         \n Length:68          Min.   :   296.0   Min.   :   169.8   Min.   :   191.5  \n Class :character   1st Qu.:   789.3   1st Qu.:   685.9   1st Qu.:   560.1  \n Mode  :character   Median :  3133.4   Median :  3536.1   Median :  2923.7  \n                    Mean   : 13585.2   Mean   : 13476.6   Mean   : 11404.5  \n                    3rd Qu.:  9860.0   3rd Qu.:  9770.0   3rd Qu.:  8172.4  \n                    Max.   :170787.7   Max.   :174243.3   Max.   :147125.9  \n      2020               2019               2018               2017        \n Min.   :   104.1   Min.   :    93.3   Min.   :    85.0   Min.   :   67.9  \n 1st Qu.:   430.4   1st Qu.:   461.6   1st Qu.:   434.7   1st Qu.:  351.5  \n Median :  2219.9   Median :  1751.8   Median :  1659.9   Median : 1156.8  \n Mean   :  8714.4   Mean   :  8282.8   Mean   :  7774.1   Mean   : 6370.7  \n 3rd Qu.:  6733.2   3rd Qu.:  6483.0   3rd Qu.:  5910.9   3rd Qu.: 5398.6  \n Max.   :115989.3   Max.   :114573.1   Max.   :104182.5   Max.   :85093.3  \n      2016              2015              2014              2013        \n Min.   :   72.0   Min.   :   72.1   Min.   :   61.5   Min.   :   66.5  \n 1st Qu.:  295.7   1st Qu.:  370.0   1st Qu.:  311.5   1st Qu.:  307.6  \n Median : 1173.2   Median : 1549.8   Median : 1377.8   Median : 1177.2  \n Mean   : 5541.1   Mean   : 5569.5   Mean   : 5011.7   Mean   : 4548.1  \n 3rd Qu.: 4588.9   3rd Qu.: 5121.5   3rd Qu.: 4416.3   3rd Qu.: 4545.9  \n Max.   :70461.8   Max.   :69881.1   Max.   :62802.7   Max.   :55949.9  \n      2012              2011              2010             2009          \n Min.   :   77.3   Min.   :   33.1   Min.   :   17.5   Length:68         \n 1st Qu.:  279.5   1st Qu.:  233.1   1st Qu.:  262.3   Class :character  \n Median : 1008.1   Median : 1023.5   Median :  769.5   Mode  :character  \n Mean   : 4081.9   Mean   : 3751.8   Mean   : 3455.8                     \n 3rd Qu.: 3777.1   3rd Qu.: 3442.4   3rd Qu.: 3007.9                     \n Max.   :50496.3   Max.   :48318.2   Max.   :46254.5                     \n      2008              2007              2006             2005          \n Min.   :    5.4   Min.   :    1.8   Min.   :    1.6   Length:68         \n 1st Qu.:  204.1   1st Qu.:  179.8   1st Qu.:  146.8   Class :character  \n Median :  683.1   Median :  534.7   Median :  441.1   Mode  :character  \n Mean   : 3270.8   Mean   : 2834.3   Mean   : 2384.7                     \n 3rd Qu.: 2840.8   3rd Qu.: 2727.7   3rd Qu.: 2028.2                     \n Max.   :43935.5   Max.   :39789.5   Max.   :33098.0                     \n     2004               2003               2002               2001          \n Length:68          Length:68          Length:68          Length:68         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n     2000          \n Length:68         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n\n\n\n\n\nCode\nexport_partners[duplicated(export_partners),]\n\n\n# A tibble: 0 × 25\n# ℹ 25 variables: Data Series &lt;chr&gt;, 2023 &lt;dbl&gt;, 2022 &lt;dbl&gt;, 2021 &lt;dbl&gt;,\n#   2020 &lt;dbl&gt;, 2019 &lt;dbl&gt;, 2018 &lt;dbl&gt;, 2017 &lt;dbl&gt;, 2016 &lt;dbl&gt;, 2015 &lt;dbl&gt;,\n#   2014 &lt;dbl&gt;, 2013 &lt;dbl&gt;, 2012 &lt;dbl&gt;, 2011 &lt;dbl&gt;, 2010 &lt;dbl&gt;, 2009 &lt;chr&gt;,\n#   2008 &lt;dbl&gt;, 2007 &lt;dbl&gt;, 2006 &lt;dbl&gt;, 2005 &lt;chr&gt;, 2004 &lt;chr&gt;, 2003 &lt;chr&gt;,\n#   2002 &lt;chr&gt;, 2001 &lt;chr&gt;, 2000 &lt;chr&gt;\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nEnsure that there are no duplicated columns, if not will have to investigate further.\n\n\n\n\n\n\n\nCode\ncolSums(is.na(export_partners))\n\n\nData Series        2023        2022        2021        2020        2019 \n          0           0           0           0           0           0 \n       2018        2017        2016        2015        2014        2013 \n          0           0           0           0           0           0 \n       2012        2011        2010        2009        2008        2007 \n          0           0           0           0           0           0 \n       2006        2005        2004        2003        2002        2001 \n          0           0           0           0           0           0 \n       2000 \n          0 \n\n\n\n\n\n\n\n\nNote\n\n\n\n\nEnsure that there are no NA values, if not will have to investigate further.\nPossibility to use the replace_na() function to replace missing values with 0 in specified columns.\n\n\n\n\n\n\n\nCode\nstr(export_partners)\n\n\ntibble [68 × 25] (S3: tbl_df/tbl/data.frame)\n $ Data Series: chr [1:68] \"Asia\" \"Bangladesh\" \"Brunei Darussalam\" \"Cambodia\" ...\n $ 2023       : num [1:68] 170788 986 960 476 21686 ...\n $ 2022       : num [1:68] 174243 904 891 390 21307 ...\n $ 2021       : num [1:68] 147126 675 579 256 18126 ...\n $ 2020       : num [1:68] 115989 582 533 250 15063 ...\n $ 2019       : num [1:68] 114573 642 556 259 11273 ...\n $ 2018       : num [1:68] 104183 664 496 290 10466 ...\n $ 2017       : num [1:68] 85093 548 464 227 8240 ...\n $ 2016       : num [1:68] 70462 483 420 196 7352 ...\n $ 2015       : num [1:68] 69881 424 465 114 7260 ...\n $ 2014       : num [1:68] 62802.7 384.3 520.4 86.3 5841.4 ...\n $ 2013       : num [1:68] 55949.9 347.5 474.4 89.2 5184.6 ...\n $ 2012       : num [1:68] 50496 349 464 115 5156 ...\n $ 2011       : num [1:68] 48318 334 468 109 5105 ...\n $ 2010       : num [1:68] 46254.5 272.6 508.1 78.4 4928.8 ...\n $ 2009       : chr [1:68] \"40325.300000000003\" \"209.3\" \"324.39999999999998\" \"99.3\" ...\n $ 2008       : num [1:68] 43936 265 273 140 4677 ...\n $ 2007       : num [1:68] 39789.5 284 248.3 88.1 4429.7 ...\n $ 2006       : num [1:68] 33098 286 121 111 3555 ...\n $ 2005       : chr [1:68] \"28070.400000000001\" \"274.60000000000002\" \"123.3\" \"77.099999999999994\" ...\n $ 2004       : chr [1:68] \"25457.1\" \"188.9\" \"138.6\" \"68.7\" ...\n $ 2003       : chr [1:68] \"18942\" \"157\" \"151.4\" \"45.3\" ...\n $ 2002       : chr [1:68] \"17741.7\" \"118.3\" \"144.30000000000001\" \"37.9\" ...\n $ 2001       : chr [1:68] \"16226.5\" \"121.8\" \"97.8\" \"29.8\" ...\n $ 2000       : chr [1:68] \"15816.9\" \"89.7\" \"100.5\" \"18.399999999999999\" ...\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nEnsure that all variables are correctly classified by data type; recast variable types if needed.\nVariables are correctly classified - where categorical variables are classified as character, while continuous variables are classified as double.\n\n\n\n\n\n\n\n\n\n\nglimpse()head()summary()duplicated()colSum(is.na())str())\n\n\n\n\nCode\nglimpse(import_partners)\n\n\nRows: 68\nColumns: 25\n$ `Data Series` &lt;chr&gt; \"Asia\", \"Bangladesh\", \"Brunei Darussalam\", \"Cambodia\", \"…\n$ `2023`        &lt;dbl&gt; 139497.1, 468.8, 196.8, 121.2, 20255.7, 13532.6, 3517.7,…\n$ `2022`        &lt;dbl&gt; 136729.5, 599.1, 83.5, 133.6, 19762.7, 12913.9, 3632.4, …\n$ `2021`        &lt;dbl&gt; 115022.1, 441.8, 53.6, 150.8, 20830.3, 10462.6, 3358.9, …\n$ `2020`        &lt;dbl&gt; 93351.0, 392.9, 69.7, 159.6, 14410.3, 8780.3, 2949.9, 22…\n$ `2019`        &lt;dbl&gt; 83242.3, 380.9, 49.2, 217.9, 13093.5, 7813.9, 2794.3, 23…\n$ `2018`        &lt;dbl&gt; 78004.9, 318.7, 82.2, 116.5, 12234.9, 6612.3, 2494.2, 28…\n$ `2017`        &lt;dbl&gt; 65654.5, 251.9, 73.5, 112.7, 10874.2, 5461.9, 2245.5, 32…\n$ `2016`        &lt;chr&gt; \"55208.6\", \"233.9\", \"96.6\", \"92.6\", \"8845.4\", \"4549.6000…\n$ `2015`        &lt;dbl&gt; 52852.1, 192.7, 67.1, 82.1, 8664.1, 4035.6, 2170.2, 215.…\n$ `2014`        &lt;dbl&gt; 48565.4, 234.8, 53.1, 65.8, 7674.3, 3839.5, 1990.2, 148.…\n$ `2013`        &lt;dbl&gt; 40683.5, 176.8, 48.9, 63.5, 5096.7, 3555.7, 1920.0, 138.…\n$ `2012`        &lt;dbl&gt; 36390.3, 160.0, 58.3, 127.8, 4233.6, 3039.8, 1816.8, 113…\n$ `2011`        &lt;dbl&gt; 34902.8, 149.4, 41.3, 133.2, 3944.7, 2979.5, 1733.9, 74.…\n$ `2010`        &lt;dbl&gt; 31712.3, 141.0, 36.6, 110.7, 4279.6, 2700.1, 1780.2, 99.…\n$ `2009`        &lt;dbl&gt; 27360.1, 106.5, 40.2, 102.9, 3503.1, 2352.3, 1566.5, 62.…\n$ `2008`        &lt;dbl&gt; 28848.5, 82.4, 109.6, 112.4, 3623.9, 2345.5, 1673.7, 60.…\n$ `2007`        &lt;dbl&gt; 24679.0, 79.9, 101.2, 100.2, 3520.0, 1869.5, 1587.4, 47.…\n$ `2006`        &lt;chr&gt; \"22188.6\", \"105.7\", \"49.6\", \"205.6\", \"2938.8\", \"1512.1\",…\n$ `2005`        &lt;chr&gt; \"19161.400000000001\", \"91.2\", \"36.299999999999997\", \"69.…\n$ `2004`        &lt;chr&gt; \"18110.8\", \"82.2\", \"60.6\", \"58\", \"2222.6999999999998\", \"…\n$ `2003`        &lt;chr&gt; \"12607.1\", \"70.900000000000006\", \"49.3\", \"34.5\", \"1685.2…\n$ `2002`        &lt;chr&gt; \"10987.5\", \"77.5\", \"37.799999999999997\", \"30\", \"1585.9\",…\n$ `2001`        &lt;chr&gt; \"9702.7000000000007\", \"39.700000000000003\", \"29.4\", \"20.…\n$ `2000`        &lt;chr&gt; \"8639.7999999999993\", \"38.799999999999997\", \"32.6\", \"9\",…\n\n\n\n\n\n\nCode\nhead(import_partners)\n\n\n# A tibble: 6 × 25\n  `Data Series`   `2023` `2022` `2021` `2020` `2019` `2018` `2017` `2016` `2015`\n  &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1 Asia            1.39e5 1.37e5 1.15e5 9.34e4 8.32e4 7.80e4 6.57e4 55208… 5.29e4\n2 Bangladesh      4.69e2 5.99e2 4.42e2 3.93e2 3.81e2 3.19e2 2.52e2 233.9  1.93e2\n3 Brunei Darussa… 1.97e2 8.35e1 5.36e1 6.97e1 4.92e1 8.22e1 7.35e1 96.6   6.71e1\n4 Cambodia        1.21e2 1.34e2 1.51e2 1.60e2 2.18e2 1.16e2 1.13e2 92.6   8.21e1\n5 Hong Kong       2.03e4 1.98e4 2.08e4 1.44e4 1.31e4 1.22e4 1.09e4 8845.4 8.66e3\n6 India           1.35e4 1.29e4 1.05e4 8.78e3 7.81e3 6.61e3 5.46e3 4549.… 4.04e3\n# ℹ 15 more variables: `2014` &lt;dbl&gt;, `2013` &lt;dbl&gt;, `2012` &lt;dbl&gt;, `2011` &lt;dbl&gt;,\n#   `2010` &lt;dbl&gt;, `2009` &lt;dbl&gt;, `2008` &lt;dbl&gt;, `2007` &lt;dbl&gt;, `2006` &lt;chr&gt;,\n#   `2005` &lt;chr&gt;, `2004` &lt;chr&gt;, `2003` &lt;chr&gt;, `2002` &lt;chr&gt;, `2001` &lt;chr&gt;,\n#   `2000` &lt;chr&gt;\n\n\n\n\n\n\nCode\nsummary(import_partners)\n\n\n Data Series             2023               2022               2021         \n Length:68          Min.   :    93.1   Min.   :    83.5   Min.   :    53.6  \n Class :character   1st Qu.:   608.7   1st Qu.:   565.6   1st Qu.:   448.8  \n Mode  :character   Median :  2333.1   Median :  2153.3   Median :  1762.8  \n                    Mean   : 11464.3   Mean   : 10862.6   Mean   :  9301.5  \n                    3rd Qu.:  8176.1   3rd Qu.:  8600.9   3rd Qu.:  6637.0  \n                    Max.   :139497.1   Max.   :136729.5   Max.   :115022.1  \n      2020              2019              2018              2017        \n Min.   :   69.7   Min.   :   49.2   Min.   :   82.2   Min.   :   73.5  \n 1st Qu.:  344.2   1st Qu.:  309.6   1st Qu.:  310.2   1st Qu.:  274.6  \n Median : 1669.7   Median : 1556.3   Median : 1565.8   Median : 1411.4  \n Mean   : 7982.5   Mean   : 7263.2   Mean   : 7045.8   Mean   : 6413.4  \n 3rd Qu.: 5550.0   3rd Qu.: 5604.6   3rd Qu.: 5620.9   3rd Qu.: 6136.3  \n Max.   :93351.0   Max.   :83242.3   Max.   :78004.9   Max.   :65654.5  \n     2016                2015              2014              2013        \n Length:68          Min.   :   34.3   Min.   :   40.3   Min.   :   48.9  \n Class :character   1st Qu.:  217.2   1st Qu.:  250.5   1st Qu.:  192.3  \n Mode  :character   Median : 1142.8   Median : 1000.9   Median :  901.6  \n                    Mean   : 5567.2   Mean   : 5337.7   Mean   : 4623.7  \n                    3rd Qu.: 4324.4   3rd Qu.: 5590.8   3rd Qu.: 5151.0  \n                    Max.   :52852.1   Max.   :50558.2   Max.   :44479.7  \n      2012              2011              2010              2009         \n Min.   :   43.5   Min.   :   27.2   Min.   :    4.7   Min.   :    4.50  \n 1st Qu.:  160.7   1st Qu.:  141.7   1st Qu.:  109.5   1st Qu.:   98.22  \n Median :  628.3   Median :  606.6   Median :  438.5   Median :  380.20  \n Mean   : 4006.5   Mean   : 3466.0   Mean   : 3164.2   Mean   : 2726.97  \n 3rd Qu.: 3773.5   3rd Qu.: 3178.5   3rd Qu.: 2564.5   3rd Qu.: 2279.35  \n Max.   :36390.3   Max.   :34902.8   Max.   :31712.3   Max.   :27360.10  \n      2008               2007              2006               2005          \n Min.   :    4.50   Min.   :    8.90   Length:68          Length:68         \n 1st Qu.:   99.97   1st Qu.:   85.83   Class :character   Class :character  \n Median :  407.45   Median :  280.55   Mode  :character   Mode  :character  \n Mean   : 2769.65   Mean   : 2348.02                                        \n 3rd Qu.: 2253.03   3rd Qu.: 1720.08                                        \n Max.   :28848.50   Max.   :24679.00                                        \n     2004               2003               2002               2001          \n Length:68          Length:68          Length:68          Length:68         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n     2000          \n Length:68         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n\n\n\n\n\nCode\nimport_partners[duplicated(import_partners),]\n\n\n# A tibble: 0 × 25\n# ℹ 25 variables: Data Series &lt;chr&gt;, 2023 &lt;dbl&gt;, 2022 &lt;dbl&gt;, 2021 &lt;dbl&gt;,\n#   2020 &lt;dbl&gt;, 2019 &lt;dbl&gt;, 2018 &lt;dbl&gt;, 2017 &lt;dbl&gt;, 2016 &lt;chr&gt;, 2015 &lt;dbl&gt;,\n#   2014 &lt;dbl&gt;, 2013 &lt;dbl&gt;, 2012 &lt;dbl&gt;, 2011 &lt;dbl&gt;, 2010 &lt;dbl&gt;, 2009 &lt;dbl&gt;,\n#   2008 &lt;dbl&gt;, 2007 &lt;dbl&gt;, 2006 &lt;chr&gt;, 2005 &lt;chr&gt;, 2004 &lt;chr&gt;, 2003 &lt;chr&gt;,\n#   2002 &lt;chr&gt;, 2001 &lt;chr&gt;, 2000 &lt;chr&gt;\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nEnsure that there are no duplicated columns, if not will have to investigate further.\n\n\n\n\n\n\n\nCode\ncolSums(is.na(import_partners))\n\n\nData Series        2023        2022        2021        2020        2019 \n          0           0           0           0           0           0 \n       2018        2017        2016        2015        2014        2013 \n          0           0           0           0           0           0 \n       2012        2011        2010        2009        2008        2007 \n          0           0           0           0           0           0 \n       2006        2005        2004        2003        2002        2001 \n          0           0           0           0           0           0 \n       2000 \n          0 \n\n\n\n\n\n\n\n\nNote\n\n\n\n\nEnsure that there are no NA values, if not will have to investigate further.\nPossibility to use the replace_na() function to replace missing values with 0 in specified columns.\n\n\n\n\n\n\n\nCode\nstr(import_partners)\n\n\ntibble [68 × 25] (S3: tbl_df/tbl/data.frame)\n $ Data Series: chr [1:68] \"Asia\" \"Bangladesh\" \"Brunei Darussalam\" \"Cambodia\" ...\n $ 2023       : num [1:68] 139497 469 197 121 20256 ...\n $ 2022       : num [1:68] 136729.5 599.1 83.5 133.6 19762.7 ...\n $ 2021       : num [1:68] 115022.1 441.8 53.6 150.8 20830.3 ...\n $ 2020       : num [1:68] 93351 392.9 69.7 159.6 14410.3 ...\n $ 2019       : num [1:68] 83242.3 380.9 49.2 217.9 13093.5 ...\n $ 2018       : num [1:68] 78004.9 318.7 82.2 116.5 12234.9 ...\n $ 2017       : num [1:68] 65654.5 251.9 73.5 112.7 10874.2 ...\n $ 2016       : chr [1:68] \"55208.6\" \"233.9\" \"96.6\" \"92.6\" ...\n $ 2015       : num [1:68] 52852.1 192.7 67.1 82.1 8664.1 ...\n $ 2014       : num [1:68] 48565.4 234.8 53.1 65.8 7674.3 ...\n $ 2013       : num [1:68] 40683.5 176.8 48.9 63.5 5096.7 ...\n $ 2012       : num [1:68] 36390.3 160 58.3 127.8 4233.6 ...\n $ 2011       : num [1:68] 34902.8 149.4 41.3 133.2 3944.7 ...\n $ 2010       : num [1:68] 31712.3 141 36.6 110.7 4279.6 ...\n $ 2009       : num [1:68] 27360.1 106.5 40.2 102.9 3503.1 ...\n $ 2008       : num [1:68] 28848.5 82.4 109.6 112.4 3623.9 ...\n $ 2007       : num [1:68] 24679 79.9 101.2 100.2 3520 ...\n $ 2006       : chr [1:68] \"22188.6\" \"105.7\" \"49.6\" \"205.6\" ...\n $ 2005       : chr [1:68] \"19161.400000000001\" \"91.2\" \"36.299999999999997\" \"69.900000000000006\" ...\n $ 2004       : chr [1:68] \"18110.8\" \"82.2\" \"60.6\" \"58\" ...\n $ 2003       : chr [1:68] \"12607.1\" \"70.900000000000006\" \"49.3\" \"34.5\" ...\n $ 2002       : chr [1:68] \"10987.5\" \"77.5\" \"37.799999999999997\" \"30\" ...\n $ 2001       : chr [1:68] \"9702.7000000000007\" \"39.700000000000003\" \"29.4\" \"20.9\" ...\n $ 2000       : chr [1:68] \"8639.7999999999993\" \"38.799999999999997\" \"32.6\" \"9\" ...\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nEnsure that all variables are correctly classified by data type; recast variable types if needed.\nVariables are correctly classified - where categorical variables are classified as character, while continuous variables are classified as double.\n\n\n\n\n\n\n\n\n\nTo ensure consistency, column names were converted to lowercase, and spaces were replaced with underscores. Since numerical values were stored as different data types in the datasets, all year columns (2000-2023) were converted to numeric format to prevent data type mismatches. Additionally, any missing (NA) values or dashes (“-”) were replaced with 0 to ensure data integrity. A “category” column was added to distinguish between Exports and Imports. Finally, the two datasets were combined using bind_rows().\n\n\n\n\n\n\nImportant\n\n\n\n\nYear columns (2000-2023) in both Exports and Imports dataset were converted to numeric format to ensure consistency and prevent data type mismatches.\nThere are missing (NA) values and “-” characters in the dataset.\nReplace NA values and “-” with 0 using the mutate() and replace() functions.\n\n\n\n\n\nCode\n# Ensure column names are consistent\ncolnames(export_partners) &lt;- tolower(gsub(\" \", \"_\", colnames(export_partners)))\ncolnames(import_partners) &lt;- tolower(gsub(\" \", \"_\", colnames(import_partners)))\n\n# Convert all numeric year columns to numeric type safely by handling non-numeric values and replacing NA and \"-\" with 0\nexport_partners &lt;- export_partners %&gt;% mutate(across(matches(\"^\\\\d{4}$\"), ~suppressWarnings(as.numeric(.)))) %&gt;%\n    mutate(across(matches(\"^\\\\d{4}$\"), ~replace(., is.na(.) | . == \"-\", 0)))\nimport_partners &lt;- import_partners %&gt;% mutate(across(matches(\"^\\\\d{4}$\"), ~suppressWarnings(as.numeric(.)))) %&gt;%\n    mutate(across(matches(\"^\\\\d{4}$\"), ~replace(., is.na(.) | . == \"-\", 0)))\n\n# Add a column to indicate the category (Exports, Imports)\nexport_partners &lt;- export_partners %&gt;% mutate(category = \"Exports\")\nimport_partners &lt;- import_partners %&gt;% mutate(category = \"Imports\")\n\n# Combine both datasets\ncombined_data &lt;- bind_rows(export_partners, import_partners)\n\n\n\n\n\nThe combined_data tibble contains 26 attributes, as shown below.\nThe following preprocessing checks were conducted as part of data preparation:\n\n\n\n\n\n\nPreprocessing Checks\n\n\n\n\nVerified that the correct data types were loaded in the combined_data dataset using glimpse() and str()\nEnsured there were no duplicate variable names using duplicated() in the dataset\nChecked for missing values using colSums(is.na())\n\n\n\n\nglimpse()head()summary()duplicated()colSum(is.na())str())\n\n\n\n\nCode\nglimpse(combined_data)\n\n\nRows: 136\nColumns: 26\n$ data_series &lt;chr&gt; \"Asia\", \"Bangladesh\", \"Brunei Darussalam\", \"Cambodia\", \"Ho…\n$ `2023`      &lt;dbl&gt; 170787.7, 985.7, 960.1, 475.6, 21686.1, 9909.4, 8542.6, 81…\n$ `2022`      &lt;dbl&gt; 174243.3, 903.9, 891.2, 389.5, 21307.3, 9730.1, 8366.6, 58…\n$ `2021`      &lt;dbl&gt; 147125.9, 674.8, 579.0, 256.0, 18126.2, 7817.0, 6222.5, 50…\n$ `2020`      &lt;dbl&gt; 115989.3, 581.6, 533.4, 250.3, 15063.2, 6337.6, 5351.4, 35…\n$ `2019`      &lt;dbl&gt; 114573.1, 642.4, 556.4, 259.1, 11273.2, 6383.4, 6781.8, 25…\n$ `2018`      &lt;dbl&gt; 104182.5, 664.0, 495.9, 290.4, 10466.3, 5957.8, 5895.2, 24…\n$ `2017`      &lt;dbl&gt; 85093.3, 547.7, 463.9, 226.8, 8239.5, 5326.3, 5615.7, 254.…\n$ `2016`      &lt;dbl&gt; 70461.8, 482.6, 420.5, 196.1, 7351.8, 4422.4, 5088.6, 231.…\n$ `2015`      &lt;dbl&gt; 69881.1, 424.3, 465.4, 114.3, 7260.0, 4236.4, 5032.0, 175.…\n$ `2014`      &lt;dbl&gt; 62802.7, 384.3, 520.4, 86.3, 5841.4, 4276.0, 4837.1, 159.2…\n$ `2013`      &lt;dbl&gt; 55949.9, 347.5, 474.4, 89.2, 5184.6, 4857.4, 4535.8, 155.1…\n$ `2012`      &lt;dbl&gt; 50496.3, 349.1, 464.1, 115.0, 5156.3, 4375.0, 4062.7, 146.…\n$ `2011`      &lt;dbl&gt; 48318.2, 333.5, 468.5, 109.2, 5104.8, 4244.7, 3537.8, 132.…\n$ `2010`      &lt;dbl&gt; 46254.5, 272.6, 508.1, 78.4, 4928.8, 4089.3, 3291.1, 114.2…\n$ `2009`      &lt;dbl&gt; 40325.3, 209.3, 324.4, 99.3, 4344.5, 3712.2, 3642.6, 89.2,…\n$ `2008`      &lt;dbl&gt; 43935.5, 264.8, 272.6, 139.7, 4676.6, 3502.5, 3321.9, 93.3…\n$ `2007`      &lt;dbl&gt; 39789.5, 284.0, 248.3, 88.1, 4429.7, 3191.3, 3026.0, 136.5…\n$ `2006`      &lt;dbl&gt; 33098.0, 285.5, 120.6, 110.8, 3555.3, 2448.5, 2969.8, 102.…\n$ `2005`      &lt;dbl&gt; 28070.4, 274.6, 123.3, 77.1, 2790.5, 2084.6, 2364.4, 90.4,…\n$ `2004`      &lt;dbl&gt; 25457.1, 188.9, 138.6, 68.7, 2313.7, 1742.1, 2231.8, 87.6,…\n$ `2003`      &lt;dbl&gt; 18942.0, 157.0, 151.4, 45.3, 2059.7, 1241.4, 1767.5, 63.0,…\n$ `2002`      &lt;dbl&gt; 17741.7, 118.3, 144.3, 37.9, 1912.0, 907.6, 1801.4, 51.5, …\n$ `2001`      &lt;dbl&gt; 16226.5, 121.8, 97.8, 29.8, 1866.5, 735.4, 1707.5, 24.6, 3…\n$ `2000`      &lt;dbl&gt; 15816.9, 89.7, 100.5, 18.4, 1860.6, 746.8, 1607.9, 28.9, 3…\n$ category    &lt;chr&gt; \"Exports\", \"Exports\", \"Exports\", \"Exports\", \"Exports\", \"Ex…\n\n\n\n\n\n\nCode\nhead(combined_data)\n\n\n# A tibble: 6 × 26\n  data_series     `2023` `2022` `2021` `2020` `2019` `2018` `2017` `2016` `2015`\n  &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Asia            1.71e5 1.74e5 1.47e5 1.16e5 1.15e5 1.04e5 85093. 70462. 69881.\n2 Bangladesh      9.86e2 9.04e2 6.75e2 5.82e2 6.42e2 6.64e2   548.   483.   424.\n3 Brunei Darussa… 9.60e2 8.91e2 5.79e2 5.33e2 5.56e2 4.96e2   464.   420.   465.\n4 Cambodia        4.76e2 3.89e2 2.56e2 2.50e2 2.59e2 2.90e2   227.   196.   114.\n5 Hong Kong       2.17e4 2.13e4 1.81e4 1.51e4 1.13e4 1.05e4  8240.  7352.  7260 \n6 India           9.91e3 9.73e3 7.82e3 6.34e3 6.38e3 5.96e3  5326.  4422.  4236.\n# ℹ 16 more variables: `2014` &lt;dbl&gt;, `2013` &lt;dbl&gt;, `2012` &lt;dbl&gt;, `2011` &lt;dbl&gt;,\n#   `2010` &lt;dbl&gt;, `2009` &lt;dbl&gt;, `2008` &lt;dbl&gt;, `2007` &lt;dbl&gt;, `2006` &lt;dbl&gt;,\n#   `2005` &lt;dbl&gt;, `2004` &lt;dbl&gt;, `2003` &lt;dbl&gt;, `2002` &lt;dbl&gt;, `2001` &lt;dbl&gt;,\n#   `2000` &lt;dbl&gt;, category &lt;chr&gt;\n\n\n\n\n\n\nCode\nsummary(combined_data)\n\n\n data_series             2023               2022               2021         \n Length:136         Min.   :    93.1   Min.   :    83.5   Min.   :    53.6  \n Class :character   1st Qu.:   701.3   1st Qu.:   596.2   1st Qu.:   492.4  \n Mode  :character   Median :  2595.8   Median :  2330.3   Median :  1957.3  \n                    Mean   : 12524.7   Mean   : 12169.6   Mean   : 10353.0  \n                    3rd Qu.:  8674.3   3rd Qu.:  8664.1   3rd Qu.:  7272.9  \n                    Max.   :170787.7   Max.   :174243.3   Max.   :147125.9  \n      2020               2019               2018               2017        \n Min.   :    69.7   Min.   :    49.2   Min.   :    82.2   Min.   :   67.9  \n 1st Qu.:   388.6   1st Qu.:   408.8   1st Qu.:   378.1   1st Qu.:  311.0  \n Median :  1725.6   Median :  1685.2   Median :  1597.5   Median : 1364.0  \n Mean   :  8348.4   Mean   :  7773.0   Mean   :  7410.0   Mean   : 6392.0  \n 3rd Qu.:  6002.1   3rd Qu.:  6162.6   3rd Qu.:  5910.9   3rd Qu.: 5726.2  \n Max.   :115989.3   Max.   :114573.1   Max.   :104182.5   Max.   :85093.3  \n      2016            2015              2014              2013        \n Min.   :    0   Min.   :   34.3   Min.   :   40.3   Min.   :   48.9  \n 1st Qu.:  269   1st Qu.:  283.6   1st Qu.:  276.0   1st Qu.:  239.6  \n Median : 1188   Median : 1250.4   Median : 1148.3   Median : 1018.6  \n Mean   : 5509   Mean   : 5568.4   Mean   : 5174.7   Mean   : 4585.9  \n 3rd Qu.: 4716   3rd Qu.: 5071.8   3rd Qu.: 4882.9   3rd Qu.: 4602.4  \n Max.   :70462   Max.   :69881.1   Max.   :62802.7   Max.   :55949.9  \n      2012              2011              2010              2009        \n Min.   :   43.5   Min.   :   27.2   Min.   :    4.7   Min.   :    0.0  \n 1st Qu.:  202.4   1st Qu.:  193.8   1st Qu.:  146.8   1st Qu.:  127.5  \n Median :  809.0   Median :  746.0   Median :  599.6   Median :  514.7  \n Mean   : 4044.2   Mean   : 3608.9   Mean   : 3310.0   Mean   : 2888.4  \n 3rd Qu.: 3777.1   3rd Qu.: 3442.4   3rd Qu.: 2625.2   3rd Qu.: 2342.0  \n Max.   :50496.3   Max.   :48318.2   Max.   :46254.5   Max.   :40325.3  \n      2008              2007              2006              2005         \n Min.   :    4.5   Min.   :    1.8   Min.   :    0.0   Min.   :    0.00  \n 1st Qu.:  152.4   1st Qu.:  112.6   1st Qu.:  114.9   1st Qu.:   72.92  \n Median :  549.5   Median :  441.4   Median :  354.6   Median :  321.20  \n Mean   : 3020.2   Mean   : 2591.2   Mean   : 2250.5   Mean   : 1878.82  \n 3rd Qu.: 2357.7   3rd Qu.: 1896.8   3rd Qu.: 1785.8   3rd Qu.: 1518.40  \n Max.   :43935.5   Max.   :39789.5   Max.   :33098.0   Max.   :28070.40  \n      2004               2003               2002               2001         \n Min.   :    0.00   Min.   :    0.00   Min.   :    0.00   Min.   :    0.00  \n 1st Qu.:   55.27   1st Qu.:   34.12   1st Qu.:   31.05   1st Qu.:   22.55  \n Median :  257.65   Median :  195.60   Median :  127.45   Median :  107.30  \n Mean   : 1699.77   Mean   : 1350.57   Mean   : 1132.43   Mean   : 1019.79  \n 3rd Qu.: 1530.25   3rd Qu.: 1140.42   3rd Qu.:  890.12   3rd Qu.:  767.73  \n Max.   :25457.10   Max.   :18942.00   Max.   :17741.70   Max.   :16226.50  \n      2000            category        \n Min.   :    0.00   Length:136        \n 1st Qu.:   19.18   Class :character  \n Median :   98.50   Mode  :character  \n Mean   :  962.32                     \n 3rd Qu.:  679.02                     \n Max.   :15816.90                     \n\n\n\n\n\n\nCode\ncombined_data[duplicated(combined_data),]\n\n\n# A tibble: 0 × 26\n# ℹ 26 variables: data_series &lt;chr&gt;, 2023 &lt;dbl&gt;, 2022 &lt;dbl&gt;, 2021 &lt;dbl&gt;,\n#   2020 &lt;dbl&gt;, 2019 &lt;dbl&gt;, 2018 &lt;dbl&gt;, 2017 &lt;dbl&gt;, 2016 &lt;dbl&gt;, 2015 &lt;dbl&gt;,\n#   2014 &lt;dbl&gt;, 2013 &lt;dbl&gt;, 2012 &lt;dbl&gt;, 2011 &lt;dbl&gt;, 2010 &lt;dbl&gt;, 2009 &lt;dbl&gt;,\n#   2008 &lt;dbl&gt;, 2007 &lt;dbl&gt;, 2006 &lt;dbl&gt;, 2005 &lt;dbl&gt;, 2004 &lt;dbl&gt;, 2003 &lt;dbl&gt;,\n#   2002 &lt;dbl&gt;, 2001 &lt;dbl&gt;, 2000 &lt;dbl&gt;, category &lt;chr&gt;\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nEnsure that there are no duplicated columns, if not will have to investigate further.\n\n\n\n\n\n\n\nCode\ncolSums(is.na(combined_data))\n\n\ndata_series        2023        2022        2021        2020        2019 \n          0           0           0           0           0           0 \n       2018        2017        2016        2015        2014        2013 \n          0           0           0           0           0           0 \n       2012        2011        2010        2009        2008        2007 \n          0           0           0           0           0           0 \n       2006        2005        2004        2003        2002        2001 \n          0           0           0           0           0           0 \n       2000    category \n          0           0 \n\n\n\n\n\n\n\n\nNote\n\n\n\n\nEnsure that there are no NA values, if not will have to investigate further.\nPossibility to use the replace_na() function to replace missing values with 0 in specified columns.\n\n\n\n\n\n\n\nCode\nstr(combined_data)\n\n\ntibble [136 × 26] (S3: tbl_df/tbl/data.frame)\n $ data_series: chr [1:136] \"Asia\" \"Bangladesh\" \"Brunei Darussalam\" \"Cambodia\" ...\n $ 2023       : num [1:136] 170788 986 960 476 21686 ...\n $ 2022       : num [1:136] 174243 904 891 390 21307 ...\n $ 2021       : num [1:136] 147126 675 579 256 18126 ...\n $ 2020       : num [1:136] 115989 582 533 250 15063 ...\n $ 2019       : num [1:136] 114573 642 556 259 11273 ...\n $ 2018       : num [1:136] 104183 664 496 290 10466 ...\n $ 2017       : num [1:136] 85093 548 464 227 8240 ...\n $ 2016       : num [1:136] 70462 483 420 196 7352 ...\n $ 2015       : num [1:136] 69881 424 465 114 7260 ...\n $ 2014       : num [1:136] 62802.7 384.3 520.4 86.3 5841.4 ...\n $ 2013       : num [1:136] 55949.9 347.5 474.4 89.2 5184.6 ...\n $ 2012       : num [1:136] 50496 349 464 115 5156 ...\n $ 2011       : num [1:136] 48318 334 468 109 5105 ...\n $ 2010       : num [1:136] 46254.5 272.6 508.1 78.4 4928.8 ...\n $ 2009       : num [1:136] 40325.3 209.3 324.4 99.3 4344.5 ...\n $ 2008       : num [1:136] 43936 265 273 140 4677 ...\n $ 2007       : num [1:136] 39789.5 284 248.3 88.1 4429.7 ...\n $ 2006       : num [1:136] 33098 286 121 111 3555 ...\n $ 2005       : num [1:136] 28070.4 274.6 123.3 77.1 2790.5 ...\n $ 2004       : num [1:136] 25457.1 188.9 138.6 68.7 2313.7 ...\n $ 2003       : num [1:136] 18942 157 151.4 45.3 2059.7 ...\n $ 2002       : num [1:136] 17741.7 118.3 144.3 37.9 1912 ...\n $ 2001       : num [1:136] 16226.5 121.8 97.8 29.8 1866.5 ...\n $ 2000       : num [1:136] 15816.9 89.7 100.5 18.4 1860.6 ...\n $ category   : chr [1:136] \"Exports\" \"Exports\" \"Exports\" \"Exports\" ...\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nEnsure that all variables are correctly classified by data type; recast variable types if needed.\nVariables are correctly classified - where categorical variables are classified as character, while continuous variables are classified as double.\n\n\n\n\n\n\n\n\n\nA new “category” column will be added to classify each trading partner as a Country, Region, or Economic & Political Union. This helps organize the data, making it easier to analyze trade patterns across different entity types.\n\n\n\n\n\n\n\ntypes\ndata_series\n\n\n\n\nCountry\nBangladesh, Brunei Darussalam, Cambodia, Hong Kong, India, Indonesia, Israel, Japan, Kuwait, Mainland China, Malaysia, Myanmar, Pakistan, Philippines, Qatar, Republic Of Korea, Saudi Arabia, Sri Lanka, Taiwan Thailand,Turkiye United Arab Emirates, Vietnam, Belgium, Cyprus Denmark, Finland France Germany, Greece Ireland Italy, Luxembourg, Netherlands, Norway, Portugal, Russian Federation, Spain, Sweden, Switzerland, United Kingdom, United States Of America, Canada, Australia, Marshall Islands, New Zealand, Papua New Guinea, Bermuda, Brazil, British Virgin Islands, Cayman Islands, Chile, Mexico Panama, Peru, Egypt, Liberia, Mauritius, Nigeria South Africa\n\n\nRegion\nAsia, Europe, North America, Oceania, South And Central America And The Caribbean, Africa\n\n\nEconomic & Political Union\nASEAN, European Union (EU-27)\n\n\n\n\n\nCode\n# Load the categorization mapping from Excel\ncategorization_mapping &lt;- read_excel(\"data/categorizationmapping.xlsx\")\n\n# Ensure column names are consistent\ncolnames(categorization_mapping) &lt;- tolower(gsub(\" \", \"_\", colnames(categorization_mapping)))\n\n# Merge only the type into combined_data\ncombined_data &lt;- combined_data %&gt;% left_join(categorization_mapping, by = \"data_series\")\n\n\n\n\n\nTo calculate the total trade values for each trading partner, the dataset was grouped by “data_series”, and trade values from 2000 to 2023 were summed using summarise(), ensuring missing values were ignored with na.rm = TRUE. A “category” column was assigned the value “Total” to indicate aggregated trade figures. The computed totals were then appended to the original dataset using bind_rows().\n\n\nCode\n# Summing up the values for each trading partner across all years (2000-2023)\nsummed_data &lt;- combined_data %&gt;%\n    group_by(data_series, types) %&gt;%\n    summarise(across(matches(\"^\\\\d{4}$\"), sum, na.rm = TRUE), .groups = \"drop\") %&gt;%\n    mutate(category = \"Total\")\n\n# Add the total category to the combined dataset\nfinal_data &lt;- bind_rows(combined_data, summed_data)\n\n# Reorder columns to move category to the second position\nfinal_data &lt;- final_data %&gt;% select(data_series, category, types, everything())\n\n# Display a preview of the merged data\nprint(head(final_data))\n\n\n# A tibble: 6 × 27\n  data_series    category types `2023` `2022` `2021` `2020` `2019` `2018` `2017`\n  &lt;chr&gt;          &lt;chr&gt;    &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Asia           Exports  Regi… 1.71e5 1.74e5 1.47e5 1.16e5 1.15e5 1.04e5 85093.\n2 Bangladesh     Exports  Coun… 9.86e2 9.04e2 6.75e2 5.82e2 6.42e2 6.64e2   548.\n3 Brunei Daruss… Exports  Coun… 9.60e2 8.91e2 5.79e2 5.33e2 5.56e2 4.96e2   464.\n4 Cambodia       Exports  Coun… 4.76e2 3.89e2 2.56e2 2.50e2 2.59e2 2.90e2   227.\n5 Hong Kong      Exports  Coun… 2.17e4 2.13e4 1.81e4 1.51e4 1.13e4 1.05e4  8240.\n6 India          Exports  Coun… 9.91e3 9.73e3 7.82e3 6.34e3 6.38e3 5.96e3  5326.\n# ℹ 17 more variables: `2016` &lt;dbl&gt;, `2015` &lt;dbl&gt;, `2014` &lt;dbl&gt;, `2013` &lt;dbl&gt;,\n#   `2012` &lt;dbl&gt;, `2011` &lt;dbl&gt;, `2010` &lt;dbl&gt;, `2009` &lt;dbl&gt;, `2008` &lt;dbl&gt;,\n#   `2007` &lt;dbl&gt;, `2006` &lt;dbl&gt;, `2005` &lt;dbl&gt;, `2004` &lt;dbl&gt;, `2003` &lt;dbl&gt;,\n#   `2002` &lt;dbl&gt;, `2001` &lt;dbl&gt;, `2000` &lt;dbl&gt;\n\n\nCode\n# library(openxlsx)\n# \n# write.xlsx(final_data, \"final_data.xlsx\")\n\n\n\n\n\n\nKey makeover changes:\n1️⃣ Breaking down trade data by types - e.g.: countries/ regions/ economic & political union\n\nOriginal visualization grouped countries, regions, economic & political unions making trade relationships harder to interpret.\nRevised visualization will categorize data into different types, ensuring clarity.\n\n2️⃣ Pie chart for region-level aggregation:\n\nPie chart groups trade volumes by region –&gt;Provide a high-level trade distribution which allow users to quickly assess which regions contribute the most to total trade.\n\n\n\n\n\n\n\nNote\n\n\n\n\nEasier comparison –&gt; Highlights contributions from countries, regions, and unions seperately.\nPie chart only showcase the Major Trading Partners for Trade in Services 2023 - Total Trade Volume Distribution by Region\n\n\n\n\nGraph()Code()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(viridis)\nlibrary(ggrepel)\n\n# Filter the trade data for regions\nregion_trade_data &lt;- final_data %&gt;%\n  filter(types == \"Region\", category == \"Total\") %&gt;%\n  select(region = data_series, trade_value = `2023`)\n\n# Ensure correct ordering based on trade value\nregion_trade_data &lt;- region_trade_data %&gt;%\n  arrange(desc(trade_value))\n\n# Compute cumulative sum for positioning\nregion_trade_data &lt;- region_trade_data %&gt;%\n  mutate(label = paste0(region, \"\\n$\", format(trade_value, big.mark = \",\")),\n         pos = cumsum(trade_value) - (0.5 * trade_value))  \n\n# Create Static Pie Chart with Labels Outside\nstatic_pie_chart &lt;- ggplot(region_trade_data, aes(x = \"\", y = trade_value, fill = trade_value)) +\n  geom_bar(stat = \"identity\", width = 1, color = \"white\") +  \n  coord_polar(theta = \"y\") +\n  scale_fill_viridis(option = \"viridis\", direction = -1) +  \n  geom_text_repel(aes(y = pos, label = label), size = 3, color = \"black\", nudge_x = 1, box.padding = 0.5) +  \n  theme_void() +\n  labs(\n    title = \"Major Trading Partners for Trade in Services, 2023\",\n    subtitle = \"Total Trade Volume Distribution by Region\",\n    fill = \"Trade Value (SGD)\"\n  ) +\n  theme(\n    legend.position = \"right\",\n    plot.title = element_text(hjust = 0, face = \"bold\", size = 14),  # Centered, bold title\n    plot.subtitle = element_text(hjust = 0, size = 12)  # Centered subtitle\n  )\n\n# Show the static pie chart\nprint(static_pie_chart)\n\n\n\n\n\n\n\n\nKey makeover changes:\n1️⃣ Enhanced trade value representation with a geo-spatial context -\n\nOriginal uses static infographics with flag-based indicators, the revised visualization presents an intecative world map, allowing for dynamic exploration.\nColor gradient (from yellow to purple) effectively communciates the trade value inensity in SGD\n\n\nGraph()Code()\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\nlibrary(sf)\nlibrary(viridis)\nlibrary(plotly)\n\n# Load world map data\nworld &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\n\n# Filter the trade data (assuming it's already in your R environment as `final_data`)\ntrade_data &lt;- final_data %&gt;%\n  filter(types == \"Country\", category == \"Total\") %&gt;%\n  select(region = data_series, trade_value = `2023`)\n\n# Standardize region names\ntrade_data$region &lt;- tolower(trimws(trade_data$region))\nworld$name &lt;- tolower(trimws(world$name))\n\n# Correct mismatched country names\ntrade_data$region &lt;- recode(trade_data$region,\n                             \"mainland china\" = \"china\",\n                             \"republic of korea\" = \"south korea\",\n                             \"russian federation\" = \"russia\",\n                             \"turkiye\" = \"turkey\",\n                             \"british virgin islands\" = \"u.s. virgin is.\",\n                             \"cayman islands\" = \"cayman is.\",\n                             \"brunei darussalam\" = \"brunei\",\n                             \"marshall islands\" = \"marshall is.\")\n\n# Merge world map with trade data\nworld &lt;- world %&gt;%\n  left_join(trade_data, by = c(\"name\" = \"region\"))\n\n# Create a ggplot object with title and subtitle\nggplot_map &lt;- ggplot(data = world) +\n  geom_sf(aes(fill = trade_value, text = paste(\"Country:\", name, \"&lt;br&gt;Trade Value: $\", trade_value)), \n          color = \"black\", size = 0.2) +\n  scale_fill_viridis(option = \"viridis\", direction = -1, na.value = \"gray90\") +\n  theme_minimal() +\n  labs(\n    title = \"Major Trading Partners for Trade in Services, 2023\",\n    subtitle = \"Total Trade Value Distribution by Countries\",\n    fill = \"Trade Value (SGD)\"\n  ) +\n  theme(\n    plot.title = element_text(hjust = 0, face = \"bold\", size = 14), \n    plot.subtitle = element_text(hjust = 0, size = 12)  \n  )\n\n# Convert to interactive Plotly map and manually add subtitle\nplotly_map &lt;- ggplotly(ggplot_map, tooltip = \"text\") %&gt;%\n  layout(\n    annotations = list(\n      list(\n        text = \"&lt;b&gt;Total Trade Value Distribution by Countries&lt;/b&gt;\",\n        x = 0,  # Left align\n        y = 1.03,  # Position above plot\n        xref = \"paper\",\n        yref = \"paper\",\n        showarrow = FALSE,\n        font = list(size = 12)\n      )\n    )\n  )\n\n# Show interactive map\nplotly_map\n\n\n\n\n\n\n\n\n\nKey observations:\n\nTotal trade vol()\n\nTop 5 trade partners: United States, Japan, Mainland China, Australia, and United Kingdom\nTrends: Significant growth post-2010, with accelerated trade activities from 2018 onward, with Asia and North America dominating trade volume.\n\nExports()\n\nTop 5 export destinations: United States, Japan, Australia, Mainland China, and United Kingdom\n\nImports()\n\nTop 5 import destinations: United States, Mainland China, Japan, United Kingdom, and Hong Kong.\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nUnited States, Mainland China, Japan, and Australia consistently reman Singapore’s top trade partners in services\nExports and imports have surged post-2018, reinforcing Singapore’s role as a global trade hub\nDigitalization and financial sector growth are key factors shaping the country trade landscape\n\n\n\n\nTotal trade vol()Export()Import()Code()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(viridis)\n\n# Filter trade data for countries\ntrade_data &lt;- final_data %&gt;%\n  filter(types == \"Country\", category == \"Imports\") %&gt;%\n  select(country = data_series, `2000`:`2023`)  # Select years from 2000 to 2023\n\n# Reshape data from wide to long format\ntrade_data_long &lt;- trade_data %&gt;%\n  pivot_longer(cols = `2000`:`2023`, names_to = \"year\", values_to = \"trade_value\") %&gt;%\n  mutate(year = as.numeric(year))  # Convert year to numeric\n\n# Calculate total trade value per country and reorder factors (highest at the top)\ntrade_data_long &lt;- trade_data_long %&gt;%\n  group_by(country) %&gt;%\n  mutate(total_trade_value = sum(trade_value, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(country = fct_reorder(country, total_trade_value, .desc = TRUE))  \n\n# Create heatmap-style plot using geom_tile()\nhori_plot &lt;- ggplot(trade_data_long, aes(x = year, y = country, fill = trade_value)) +\n  geom_tile(color = \"white\") +  # Add white borders between tiles\n  scale_fill_viridis_c(option = \"magma\", na.value = \"gray90\") +  # Color gradient for intensity\n  labs(\n    title = \"Total Trade Value Trends (2000-202f3)\",\n    subtitle = \"Major Trading Partners for Trade in Services\",\n    x = \"Year\",\n    y = \"Country\",\n    fill = \"Trade Value (SGD)\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.y = element_text(size = 8),  # Adjust country label size\n    axis.text.x = element_text(angle = 45, hjust = 1),  # Rotate year labels\n    plot.title = element_text(hjust = 0, face = \"bold\", size = 14),  # Left-aligned title\n    plot.subtitle = element_text(hjust = 0, size = 12)  # Left-aligned subtitle\n  )\n\n# Show plot\nprint(hori_plot)\n\n\n\n\n\n\n\n\nIn the below code, we will explore the tidymodels approach in time series forecasting.\n\n\n\n\nCode\n# Load all necessary packages\nlibrary(tidyverse)\nlibrary(modeltime)\nlibrary(timetk)\nlibrary(lubridate)\nlibrary(rsample)\nlibrary(parsnip)\nlibrary(recipes)\nlibrary(workflows)\nlibrary(yardstick)\n\n# Convert wide format to long format\nlong_data &lt;- final_data %&gt;%\n    pivot_longer(cols = matches(\"^\\\\d{4}$\"), names_to = \"year\", values_to = \"value\") %&gt;%\n    mutate(year = as.integer(year)) \n\n# Convert year to Date format\nlong_data &lt;- long_data %&gt;%\n    mutate(date = make_date(year = year, month = 1, day = 1)) %&gt;%\n    select(data_series, category, types, date, value)\n\n# Filter for a specific country and only \"Exports\"\nselected_series &lt;- long_data %&gt;%\n    filter(data_series == \"United States Of America\", category == \"Exports\") %&gt;%\n    select(date, value) %&gt;%\n    arrange(date)\n\n# Remove duplicates if any\nselected_series &lt;- selected_series %&gt;%\n    distinct(date, .keep_all = TRUE)\n\n# Perform a time-based split (80% training, 20% testing)\nsplits &lt;- initial_time_split(selected_series, prop = 0.8)\n\ncat(\"The training dataset contains\", nrow(training(splits)), \"observations.\\n\")\n\n\nThe training dataset contains 19 observations.\n\n\nCode\ncat(\"The testing dataset consists of\", nrow(testing(splits)), \"observations.\\n\")\n\n\nThe testing dataset consists of 5 observations.\n\n\n\n\n\nIn the code below, we will fit four models: - Error-Trend-Season (ETS) model by using exp_smoothing() - Auto ARIMA model by using arima_reg() - Boosted Auto ARIMA by using arima_boost() - Prophet model by using prophet_reg()\n\n\nCode\nmodel_fit_ets &lt;- exp_smoothing() %&gt;%\n    set_engine(\"ets\") %&gt;%\n    fit(value ~ date, data = training(splits))\n\nmodel_fit_arima &lt;- arima_reg() %&gt;%\n    set_engine(\"auto_arima\") %&gt;%\n    fit(value ~ date, data = training(splits))\n\nmodel_fit_arima_boosted &lt;- arima_boost(\n  min_n = 2,\n  learn_rate = 0.015) %&gt;%\n  set_engine(\"auto_arima_xgboost\") %&gt;%\n  fit(value ~ date, data = training(splits))\n\nmodel_fit_prophet &lt;- prophet_reg() %&gt;%\n    set_engine(\"prophet\") %&gt;%\n    fit(value ~ date, data = training(splits))\n\n\n\n\n\nNext, we will use modeltime_table of modeltime package to add each of the models to a Modeltime Table.\n\n\nCode\nmodels_tbl &lt;- modeltime_table(\n    model_fit_ets,\n    model_fit_arima,\n    model_fit_arima_boosted,\n    model_fit_prophet\n)\n\nprint(models_tbl)\n\n\n# Modeltime Table\n# A tibble: 4 × 3\n  .model_id .model   .model_desc            \n      &lt;int&gt; &lt;list&gt;   &lt;chr&gt;                  \n1         1 &lt;fit[+]&gt; ETS(M,A,N)             \n2         2 &lt;fit[+]&gt; ARIMA(0,1,0) WITH DRIFT\n3         3 &lt;fit[+]&gt; ARIMA(0,1,0) WITH DRIFT\n4         4 &lt;fit[+]&gt; PROPHET                \n\n\n\n\n\nWe will then use the modeltime_calibrate() to add a new column called .calibrate_data into the newly created models_tbl tibble data table.\n\n\nCode\ncalibration_tbl &lt;- models_tbl %&gt;%\n    modeltime_calibrate(new_data = testing(splits))\n\nprint(calibration_tbl)\n\n\n# Modeltime Table\n# A tibble: 4 × 5\n  .model_id .model   .model_desc             .type .calibration_data\n      &lt;int&gt; &lt;list&gt;   &lt;chr&gt;                   &lt;chr&gt; &lt;list&gt;           \n1         1 &lt;fit[+]&gt; ETS(M,A,N)              Test  &lt;tibble [5 × 4]&gt; \n2         2 &lt;fit[+]&gt; ARIMA(0,1,0) WITH DRIFT Test  &lt;tibble [5 × 4]&gt; \n3         3 &lt;fit[+]&gt; ARIMA(0,1,0) WITH DRIFT Test  &lt;tibble [5 × 4]&gt; \n4         4 &lt;fit[+]&gt; PROPHET                 Test  &lt;tibble [5 × 4]&gt; \n\n\n\n\n\nWe will use two way to assess the accuracy of the models - by (1) means of accuracy metrics, (2) visualization\n\n\nmodeltime_accuracy() of modeltime package is used compute the accuracy metrics. Then, table_modeltime_accuracy() is used to present the accuracy metrics in tabular form.\n\n\nCode\ncalibration_tbl %&gt;%\n  modeltime_accuracy() %&gt;%\n  table_modeltime_accuracy(.interactive = FALSE)\n\n\n\n\n\n\n\n\nAccuracy Table\n\n\n.model_id\n.model_desc\n.type\nmae\nmape\nmase\nsmape\nrmse\nrsq\n\n\n\n\n1\nETS(M,A,N)\nTest\n16181.62\n31.88\n1.49\n40.03\n19413.03\n0.73\n\n\n2\nARIMA(0,1,0) WITH DRIFT\nTest\n13059.38\n24.56\n1.20\n30.18\n16738.06\n0.73\n\n\n3\nARIMA(0,1,0) WITH DRIFT\nTest\n13059.38\n24.56\n1.20\n30.18\n16738.06\n0.73\n\n\n4\nPROPHET\nTest\n16031.61\n31.46\n1.48\n39.53\n19329.48\n0.61\n\n\n\n\n\n\n\n\n\n\nWe can also use the interactive plotly visualization to assess the accuracy of the models.\n\n\nCode\ncalibration_tbl %&gt;%\n    modeltime_forecast(new_data = testing(splits), actual_data = selected_series) %&gt;%\n    plot_modeltime_forecast()\n\n\n\n\n\n\n\n\n\n\nNext, we refit the models to the full dataset using modeltime_refit() and forecast them forward.We can use modeltime_refit() to refit the forecasting models with the full data.\nThen, modeltime_forecast() is used to forecast to a selected future time period, in this example 10 years.\n\n\nCode\nrefit_tbl &lt;- calibration_tbl %&gt;%\n    modeltime_refit(data = selected_series)  # Now trained on full dataset\n\nforecast_tbl &lt;- refit_tbl %&gt;%\n    modeltime_forecast(\n        h = 10,  # Forecast for 10 years\n        actual_data = selected_series,\n        keep_data = TRUE  # Keep historical data for better visualization\n    )\nforecast_tbl %&gt;%\n    plot_modeltime_forecast(\n        .legend_max_width = 25, \n        .interactive = TRUE,\n        .plotly_slider = TRUE\n    )\n\n\n\n\n\n\n\n\n\n\n\nCode\nrefit_tbl %&gt;%\n    modeltime_accuracy() %&gt;%\n    table_modeltime_accuracy(.interactive = FALSE)\n\n\n\n\n\n\n\n\nAccuracy Table\n\n\n.model_id\n.model_desc\n.type\nmae\nmape\nmase\nsmape\nrmse\nrsq\n\n\n\n\n1\nETS(M,A,N)\nTest\n16181.62\n31.88\n1.49\n40.03\n19413.03\n0.73\n\n\n2\nUPDATE: ARIMA(0,1,0)(0,0,1)[5] WITH DRIFT\nTest\n13059.38\n24.56\n1.20\n30.18\n16738.06\n0.73\n\n\n3\nUPDATE: ARIMA(0,1,0)(0,0,1)[5] WITH DRIFT\nTest\n13059.38\n24.56\n1.20\n30.18\n16738.06\n0.73\n\n\n4\nPROPHET\nTest\n16031.61\n31.46\n1.48\n39.53\n19329.48\n0.61"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02.html#overview",
    "href": "Take-home_Ex/Take-home_Ex02.html#overview",
    "title": "Take-home Exercise 02",
    "section": "",
    "text": "Since Mr. Donald Trump assumed office as the President of the United States on January 20, 2025, global trade has been a highly scrutinized topic. Given Singapore’s role as a key global trade hub, understanding its trade dynamics is crucial in assessing the potential impact of shifting geopolitical and economic policies.\nIn this take-home exercise, we apply newly acquired data visualization and analytical techniques to explore Singapore’s Trade in Services - Singapore’s exports and imports of services between Singapore and the rest of the world. By leveraging statistical tools and visualization methods, this study aims to uncover key trends, service category distributions, and trade imbalances while providing deeper insights into how different service sectors contribute to Singapore’s economy.\n\n\n\nUsing data from the Department of Statistics Singapore, DOS on Trade In Services By Services Category, this analysis applies Exploratory Data (EDA) and data visualization techniques to:\n\nAssess three existing visualizations from this page, identifying each of these visualization pros and cons.\nRedesign and enhance these visualizations using ggplot2 and other R packages to improve data interpretation, visual appeal, and accessibility.\nConduct time-series analysis and forecasting to identify key trends in Singapore’s trade in services, evaluate growth patterns across service categories, and predict potential future trade movements."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02.html#getting-started",
    "href": "Take-home_Ex/Take-home_Ex02.html#getting-started",
    "title": "Take-home Exercise 02",
    "section": "",
    "text": "The following R packages will be loaded for this exercise using pacman::p_load():\n\nreadxl: Reads Excel files (.xls and .xlsx) into R\nwritexl: Writes data frames to Excel files (.xlsx).\ntidyverse: A collection of packages for data manipulation, visualization, and modeling.\ntimetk: Provides time series analysis and forecasting tools.\nforecast: Implements forecasting methods like ARIMA, ETS, and more.\nggplot2: A powerful package for data visualization using the grammar of graphics.\nplotly: Creates interactive plots, including 3D and web-based visualizations.\ntreemapify: Enables the creation of treemap visualizations in ggplot2.\npatchwork: Helps combine multiple ggplot2 plots into a single layout.\ndplyr: Provides fast and intuitive data manipulation functions.\nCGPfunctions: Offers additional plotting and visualization functions.\nrnaturalearth: Provides world map data for geographic visualizations.\nrnaturalearthdata: Supplies natural Earth vector data for spatial analysis.\nsf: Supports simple features for spatial data (GIS).\nviridis: Provides color palettes for better visualization accessibility.\nggrepel: Improves text labeling in ggplot2 by avoiding overlapping labels.\nggHoriPlot: Creates horizontal bar plots for ggplot2.\nggthemes: Adds additional themes and styles for ggplot2.\ntidymodels: A collection of packages for machine learning and modeling.\nmodeltime: A framework for time series forecasting using machine learning and statistical models.\n\n\n\nCode\npacman::p_load(readxl, openxlsx, data.table, tidyverse, timetk, plotly, forecast, ggplot2, CGPfunctions, rnaturalearth, rnaturalearthdata, sf, viridis, ggrepel, ggHoriPlot, ggthemes, tidymodels, timetk, modeltime)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02.html#data-wrangling-data-prepration",
    "href": "Take-home_Ex/Take-home_Ex02.html#data-wrangling-data-prepration",
    "title": "Take-home Exercise 02",
    "section": "",
    "text": "The code chunk below imports the Trade In Services By Services Category dataset, downloaded from Department of Statistics Singapore, DOS, using the read_excel() function from the readxl package.\n\n\nCode\ntrade_services &lt;- read_excel(\"data/Trade In Services By Services Category_base.xlsx\")\n\n\n\n\n\n\nglimpse(): provides a transposed overview of a dataset, showing variables and their types in a concise format.\nhead(): displays the first few rows of a dataset (default is 6 rows) to give a quick preview of the data.\nsummary(): generates a statistical summary of each variable, including measures like mean, median, and range for numeric data.\nduplicated():returns a logical vector indicating which elements or rows in a vector or data frame are duplicates.\ncolSums(is.na()): counts the number of missing values (NA) in each column of the data frame.\nstr(): use str() to display the column names, data types, and a preview of the data.\n\n\nglimpse()head()summary()duplicated()colSum(is.na())str())\n\n\n\n\nCode\nglimpse(trade_services)\n\n\nRows: 51\nColumns: 26\n$ `Data Series` &lt;chr&gt; \"Total Trade In Services\", \"Exports Of Services\", \"Manuf…\n$ `2024`        &lt;dbl&gt; 997749.8, 528568.3, 471.0, 10820.1, 172971.1, 145993.3, …\n$ `2023`        &lt;dbl&gt; 919117.0, 481009.2, 490.7, 10981.0, 149537.4, 124222.5, …\n$ `2022`        &lt;dbl&gt; 877252.5, 468190.5, 685.5, 10117.9, 189649.0, 167209.2, …\n$ `2021`        &lt;dbl&gt; 715093.5, 382492.4, 489.9, 8833.5, 144220.0, 130499.3, 1…\n$ `2020`        &lt;dbl&gt; 590035.1, 300004.6, 236.8, 8172.9, 93560.0, 79857.9, 137…\n$ `2019`        &lt;dbl&gt; 592824.3, 307215.9, 266.9, 9663.3, 94272.1, 76545.7, 177…\n$ `2018`        &lt;dbl&gt; 561271.0, 287141.4, 370.3, 9410.0, 88888.8, 71746.4, 171…\n$ `2017`        &lt;dbl&gt; 493353.8, 241568.0, 247.1, 7712.0, 69993.5, 54547.9, 154…\n$ `2016`        &lt;dbl&gt; 431109.3, 211835.8, 284.8, 8418.5, 59213.4, 45873.0, 133…\n$ `2015`        &lt;dbl&gt; 432922.3, 210622.7, 346.5, 9315.2, 64097.1, 50798.1, 132…\n$ `2014`        &lt;dbl&gt; 406020.8, 194843.2, 424.4, 9853.1, 63918.8, 50917.2, 130…\n$ `2013`        &lt;dbl&gt; 365055.0, 177719.3, 283.2, 10767.2, 57830.9, 45929.4, 11…\n$ `2012`        &lt;dbl&gt; 327866.3, 161769.2, 249.6, 9053.1, 55586.3, 42864.3, 127…\n$ `2011`        &lt;dbl&gt; 298227.8, 150013.0, 260.4, 9342.9, 53523.0, 41416.7, 121…\n$ `2010`        &lt;dbl&gt; 273929.7, 136872.3, 289.5, 8648.4, 52606.6, 41214.6, 113…\n$ `2009`        &lt;dbl&gt; 238962.6, 117832.0, 323.4, 9128.1, 43365.7, 33042.6, 103…\n$ `2008`        &lt;dbl&gt; 254282.0, 126155.0, 452.1, 8354.6, 51108.8, 38561.9, 125…\n$ `2007`        &lt;dbl&gt; 223936.4, 110796.6, 492.6, 6605.6, 43642.8, 31104.6, 125…\n$ `2006`        &lt;dbl&gt; 195966.0, 92674.8, 534.7, 5701.0, 35877.1, 24748.0, 1112…\n$ `2005`        &lt;dbl&gt; 167532.8, 75904.8, 315.0, 4797.7, 32435.1, 21286.1, 1114…\n$ `2004`        &lt;dbl&gt; 150911.1, 66795.8, 353.2, 3450.5, 28630.3, 18702.3, 9928…\n$ `2003`        &lt;dbl&gt; 122427.9, 52966.2, 303.1, 2883.1, 23343.2, 15022.2, 8321…\n$ `2002`        &lt;dbl&gt; 109710.5, 49936.3, 369.5, 3071.8, 21539.3, 12656.8, 8882…\n$ `2001`        &lt;dbl&gt; 102892.5, 46286.1, 195.0, 2099.5, 20497.9, 12079.2, 8418…\n$ `2000`        &lt;dbl&gt; 96452.4, 44854.8, 202.4, 1755.3, 20379.3, 11595.5, 8783.…\n\n\n\n\n\n\nCode\nhead(trade_services)\n\n\n# A tibble: 6 × 26\n  `Data Series`   `2024` `2023` `2022` `2021` `2020` `2019` `2018` `2017` `2016`\n  &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Total Trade In… 9.98e5 9.19e5 8.77e5 7.15e5 5.90e5 5.93e5 5.61e5 4.93e5 4.31e5\n2 Exports Of Ser… 5.29e5 4.81e5 4.68e5 3.82e5 3.00e5 3.07e5 2.87e5 2.42e5 2.12e5\n3 Manufacturing … 4.71e2 4.91e2 6.85e2 4.90e2 2.37e2 2.67e2 3.70e2 2.47e2 2.85e2\n4 Maintenance An… 1.08e4 1.10e4 1.01e4 8.83e3 8.17e3 9.66e3 9.41e3 7.71e3 8.42e3\n5 Transport       1.73e5 1.50e5 1.90e5 1.44e5 9.36e4 9.43e4 8.89e4 7.00e4 5.92e4\n6 Freight         1.46e5 1.24e5 1.67e5 1.30e5 7.99e4 7.65e4 7.17e4 5.45e4 4.59e4\n# ℹ 16 more variables: `2015` &lt;dbl&gt;, `2014` &lt;dbl&gt;, `2013` &lt;dbl&gt;, `2012` &lt;dbl&gt;,\n#   `2011` &lt;dbl&gt;, `2010` &lt;dbl&gt;, `2009` &lt;dbl&gt;, `2008` &lt;dbl&gt;, `2007` &lt;dbl&gt;,\n#   `2006` &lt;dbl&gt;, `2005` &lt;dbl&gt;, `2004` &lt;dbl&gt;, `2003` &lt;dbl&gt;, `2002` &lt;dbl&gt;,\n#   `2001` &lt;dbl&gt;, `2000` &lt;dbl&gt;\n\n\n\n\n\n\nCode\nsummary(trade_services)\n\n\n Data Series             2024               2023               2022         \n Length:51          Min.   :    57.8   Min.   :    56.2   Min.   :    59.6  \n Class :character   1st Qu.:  1855.3   1st Qu.:  1817.6   1st Qu.:  1603.0  \n Mode  :character   Median : 13495.7   Median : 12907.1   Median : 11700.4  \n                    Mean   : 71188.6   Mean   : 65608.8   Mean   : 63405.1  \n                    3rd Qu.: 45456.5   3rd Qu.: 43514.1   3rd Qu.: 37732.8  \n                    Max.   :997749.8   Max.   :919117.0   Max.   :877252.5  \n      2021               2020               2019               2018         \n Min.   :    62.4   Min.   :    57.5   Min.   :    52.1   Min.   :    66.8  \n 1st Qu.:  1350.0   1st Qu.:  1209.1   1st Qu.:  1134.8   1st Qu.:  1051.2  \n Median :  9180.3   Median :  9124.8   Median : 10250.8   Median :  9410.0  \n Mean   : 51559.2   Mean   : 42075.8   Mean   : 41792.4   Mean   : 39491.2  \n 3rd Qu.: 35308.2   3rd Qu.: 30748.8   3rd Qu.: 29536.0   3rd Qu.: 27016.0  \n Max.   :715093.5   Max.   :590035.1   Max.   :592824.3   Max.   :561271.0  \n      2017               2016               2015               2014         \n Min.   :    56.1   Min.   :    72.0   Min.   :    46.9   Min.   :    56.9  \n 1st Qu.:   957.6   1st Qu.:   811.6   1st Qu.:   809.8   1st Qu.:   739.1  \n Median :  7848.2   Median :  6458.8   Median :  6194.0   Median :  6021.0  \n Mean   : 34523.8   Mean   : 29987.0   Mean   : 30256.4   Mean   : 28284.6  \n 3rd Qu.: 23594.4   3rd Qu.: 22798.2   3rd Qu.: 23150.7   3rd Qu.: 22757.6  \n Max.   :493353.8   Max.   :431109.3   Max.   :432922.3   Max.   :406020.8  \n      2013               2012               2011               2010         \n Min.   :    78.8   Min.   :    84.6   Min.   :    62.5   Min.   :    64.0  \n 1st Qu.:   661.6   1st Qu.:   656.4   1st Qu.:   617.9   1st Qu.:   587.1  \n Median :  4647.2   Median :  4034.2   Median :  3397.2   Median :  3493.8  \n Mean   : 25332.7   Mean   : 22691.6   Mean   : 20633.9   Mean   : 19039.0  \n 3rd Qu.: 21875.2   3rd Qu.: 19465.0   3rd Qu.: 17328.2   3rd Qu.: 16731.5  \n Max.   :365055.0   Max.   :327866.3   Max.   :298227.8   Max.   :273929.7  \n      2009               2008               2007               2006         \n Min.   :    51.0   Min.   :    76.9   Min.   :   130.4   Min.   :    48.6  \n 1st Qu.:   579.8   1st Qu.:   536.3   1st Qu.:   441.2   1st Qu.:   413.8  \n Median :  3015.6   Median :  3238.9   Median :  2213.8   Median :  2080.4  \n Mean   : 16671.3   Mean   : 17870.3   Mean   : 15789.5   Mean   : 13848.0  \n 3rd Qu.: 13932.2   3rd Qu.: 14221.4   3rd Qu.: 13344.4   3rd Qu.: 11489.8  \n Max.   :238962.6   Max.   :254282.0   Max.   :223936.4   Max.   :195966.0  \n      2005               2004               2003               2002         \n Min.   :    24.7   Min.   :    17.3   Min.   :    17.2   Min.   :    14.3  \n 1st Qu.:   313.1   1st Qu.:   284.9   1st Qu.:   228.2   1st Qu.:   212.3  \n Median :  1524.5   Median :  1374.8   Median :  1108.7   Median :  1001.9  \n Mean   : 11792.0   Mean   : 10647.8   Mean   :  8587.8   Mean   :  7648.4  \n 3rd Qu.: 10741.8   3rd Qu.:  9462.4   3rd Qu.:  7513.7   3rd Qu.:  8339.2  \n Max.   :167532.8   Max.   :150911.1   Max.   :122427.9   Max.   :109710.5  \n      2001               2000        \n Min.   :    22.6   Min.   :   13.6  \n 1st Qu.:   188.7   1st Qu.:  170.6  \n Median :   778.0   Median :  794.0  \n Mean   :  7219.0   Mean   : 6794.9  \n 3rd Qu.:  8164.2   3rd Qu.: 7509.9  \n Max.   :102892.5   Max.   :96452.4  \n\n\n\n\n\n\nCode\ntrade_services[duplicated(trade_services),]\n\n\n# A tibble: 0 × 26\n# ℹ 26 variables: Data Series &lt;chr&gt;, 2024 &lt;dbl&gt;, 2023 &lt;dbl&gt;, 2022 &lt;dbl&gt;,\n#   2021 &lt;dbl&gt;, 2020 &lt;dbl&gt;, 2019 &lt;dbl&gt;, 2018 &lt;dbl&gt;, 2017 &lt;dbl&gt;, 2016 &lt;dbl&gt;,\n#   2015 &lt;dbl&gt;, 2014 &lt;dbl&gt;, 2013 &lt;dbl&gt;, 2012 &lt;dbl&gt;, 2011 &lt;dbl&gt;, 2010 &lt;dbl&gt;,\n#   2009 &lt;dbl&gt;, 2008 &lt;dbl&gt;, 2007 &lt;dbl&gt;, 2006 &lt;dbl&gt;, 2005 &lt;dbl&gt;, 2004 &lt;dbl&gt;,\n#   2003 &lt;dbl&gt;, 2002 &lt;dbl&gt;, 2001 &lt;dbl&gt;, 2000 &lt;dbl&gt;\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nEnsure that there are no duplicated columns, if not will have to investigate further.\n\n\n\n\n\n\n\nCode\ncolSums(is.na(trade_services))\n\n\nData Series        2024        2023        2022        2021        2020 \n          0           0           0           0           0           0 \n       2019        2018        2017        2016        2015        2014 \n          0           0           0           0           0           0 \n       2013        2012        2011        2010        2009        2008 \n          0           0           0           0           0           0 \n       2007        2006        2005        2004        2003        2002 \n          0           0           0           0           0           0 \n       2001        2000 \n          0           0 \n\n\n\n\n\n\n\n\nNote\n\n\n\n\nEnsure that there are no NA values, if not will have to investigate further.\nPossibility to use drop_na() function to drop rows where any specified column contains a missing value.\n\n\n\n\n\n\n\nCode\nstr(trade_services)\n\n\ntibble [51 × 26] (S3: tbl_df/tbl/data.frame)\n $ Data Series: chr [1:51] \"Total Trade In Services\" \"Exports Of Services\" \"Manufacturing Services On Physical Inputs Owned By Others\" \"Maintenance And Repair Services\" ...\n $ 2024       : num [1:51] 997750 528568 471 10820 172971 ...\n $ 2023       : num [1:51] 919117 481009 491 10981 149537 ...\n $ 2022       : num [1:51] 877253 468191 686 10118 189649 ...\n $ 2021       : num [1:51] 715094 382492 490 8834 144220 ...\n $ 2020       : num [1:51] 590035 300005 237 8173 93560 ...\n $ 2019       : num [1:51] 592824 307216 267 9663 94272 ...\n $ 2018       : num [1:51] 561271 287141 370 9410 88889 ...\n $ 2017       : num [1:51] 493354 241568 247 7712 69994 ...\n $ 2016       : num [1:51] 431109 211836 285 8418 59213 ...\n $ 2015       : num [1:51] 432922 210623 346 9315 64097 ...\n $ 2014       : num [1:51] 406021 194843 424 9853 63919 ...\n $ 2013       : num [1:51] 365055 177719 283 10767 57831 ...\n $ 2012       : num [1:51] 327866 161769 250 9053 55586 ...\n $ 2011       : num [1:51] 298228 150013 260 9343 53523 ...\n $ 2010       : num [1:51] 273930 136872 290 8648 52607 ...\n $ 2009       : num [1:51] 238963 117832 323 9128 43366 ...\n $ 2008       : num [1:51] 254282 126155 452 8355 51109 ...\n $ 2007       : num [1:51] 223936 110797 493 6606 43643 ...\n $ 2006       : num [1:51] 195966 92675 535 5701 35877 ...\n $ 2005       : num [1:51] 167533 75905 315 4798 32435 ...\n $ 2004       : num [1:51] 150911 66796 353 3450 28630 ...\n $ 2003       : num [1:51] 122428 52966 303 2883 23343 ...\n $ 2002       : num [1:51] 109711 49936 370 3072 21539 ...\n $ 2001       : num [1:51] 102893 46286 195 2100 20498 ...\n $ 2000       : num [1:51] 96452 44855 202 1755 20379 ...\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nEnsure that all variables are correctly classified by data type; recast variable types if needed.\nVariables are correctly classified - where categorical variables are classified as character, while continuous variables are classified as double.\n\n\n\n\n\n\nThe trade_services tibble contains 26 attributes, as shown above.\nThe following preprocessing checks were conducted as part of data preparation:\n\n\n\n\n\n\nPreprocessing Checks\n\n\n\n\nVerified that the correct data types were loaded in the trade_services dataset using glimpse() and str()\nEnsured there were no duplicate variable names using duplicated() in the dataset\nChecked for missing values using colSums(is.na())"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02.html#datavis-makeover-1",
    "href": "Take-home_Ex/Take-home_Ex02.html#datavis-makeover-1",
    "title": "Take-home Exercise 02",
    "section": "",
    "text": "The original visualizations analyzed in this analysis are sourced from here.\n\n\n\n\nIn the below section, we will evaluate the effectiveness of the above visualization by identifying its pros and cons, focusing on aspects such as (1) clarity, and (2) visual appeal:\n\nClarity: How well the data is presented and understood\n\n\n\n\n\n\n\n\n\nPros\nCons\nSuggested fixes\n\n\n\n\nClear categorization of exports and imports - The chart effectively differentiates between exports and imports\nYear-specific colors make trend analysis difficult – Each year is assigned a different color, making it hard to track trends across time.\nUse consistent colors for exports and imports across all years (Implemented by converting them into line charts).\n\n\nTotal trade values are highlighted – The total trade (Exports + Imports) is displayed for each year.\nFloating “Total” labels can be easily overlooked – They are placed above bars separately, which may lead to misinterpretation.\nIntroduce a line chart for total trade trends instead of floating labels.\n\n\nGrowth rate (CAGR) and trade balance data are included – Additional insights are provided.\nTrade balance is not well integrated – It is displayed separately at the bottom instead of being visually linked to the bars.\nRepresent trade balance directly as a bar chart (Green for surplus, Red for deficit)\n\n\n\n\nVisual appeal: How visually engaging and effective the design is\n\n\n\n\n\n\n\n\n\nPros\nCons\nSuggested fixes\n\n\n\n\nEngaging use of colors and icons – Makes the visualization appealing and eye-catching.\nOveruse of colors creates clutter – Different colors for each year make it visually overwhelming.\nReduce unnecessary color variations and simplify color coding.\n\n\n\n\n\n\n\n\n\n\n\nThe code chunk below imports the Trade In Services By Services Category dataset, downloaded from Department of Statistics Singapore, DOS, using the read_excel() function from the readxl package.\n\n\nCode\ntrade_services &lt;- read_excel(\"data/Trade In Services By Services Category_base.xlsx\")\n\n\n\n\n\n\nglimpse(): provides a transposed overview of a dataset, showing variables and their types in a concise format.\nhead(): displays the first few rows of a dataset (default is 6 rows) to give a quick preview of the data.\nsummary(): generates a statistical summary of each variable, including measures like mean, median, and range for numeric data.\nduplicated():returns a logical vector indicating which elements or rows in a vector or data frame are duplicates.\ncolSums(is.na()): counts the number of missing values (NA) in each column of the data frame.\nstr(): use str() to display the column names, data types, and a preview of the data.\n\n\nglimpse()head()summary()duplicated()colSum(is.na())str())\n\n\n\n\nCode\nglimpse(trade_services)\n\n\nRows: 51\nColumns: 26\n$ `Data Series` &lt;chr&gt; \"Total Trade In Services\", \"Exports Of Services\", \"Manuf…\n$ `2024`        &lt;dbl&gt; 997749.8, 528568.3, 471.0, 10820.1, 172971.1, 145993.3, …\n$ `2023`        &lt;dbl&gt; 919117.0, 481009.2, 490.7, 10981.0, 149537.4, 124222.5, …\n$ `2022`        &lt;dbl&gt; 877252.5, 468190.5, 685.5, 10117.9, 189649.0, 167209.2, …\n$ `2021`        &lt;dbl&gt; 715093.5, 382492.4, 489.9, 8833.5, 144220.0, 130499.3, 1…\n$ `2020`        &lt;dbl&gt; 590035.1, 300004.6, 236.8, 8172.9, 93560.0, 79857.9, 137…\n$ `2019`        &lt;dbl&gt; 592824.3, 307215.9, 266.9, 9663.3, 94272.1, 76545.7, 177…\n$ `2018`        &lt;dbl&gt; 561271.0, 287141.4, 370.3, 9410.0, 88888.8, 71746.4, 171…\n$ `2017`        &lt;dbl&gt; 493353.8, 241568.0, 247.1, 7712.0, 69993.5, 54547.9, 154…\n$ `2016`        &lt;dbl&gt; 431109.3, 211835.8, 284.8, 8418.5, 59213.4, 45873.0, 133…\n$ `2015`        &lt;dbl&gt; 432922.3, 210622.7, 346.5, 9315.2, 64097.1, 50798.1, 132…\n$ `2014`        &lt;dbl&gt; 406020.8, 194843.2, 424.4, 9853.1, 63918.8, 50917.2, 130…\n$ `2013`        &lt;dbl&gt; 365055.0, 177719.3, 283.2, 10767.2, 57830.9, 45929.4, 11…\n$ `2012`        &lt;dbl&gt; 327866.3, 161769.2, 249.6, 9053.1, 55586.3, 42864.3, 127…\n$ `2011`        &lt;dbl&gt; 298227.8, 150013.0, 260.4, 9342.9, 53523.0, 41416.7, 121…\n$ `2010`        &lt;dbl&gt; 273929.7, 136872.3, 289.5, 8648.4, 52606.6, 41214.6, 113…\n$ `2009`        &lt;dbl&gt; 238962.6, 117832.0, 323.4, 9128.1, 43365.7, 33042.6, 103…\n$ `2008`        &lt;dbl&gt; 254282.0, 126155.0, 452.1, 8354.6, 51108.8, 38561.9, 125…\n$ `2007`        &lt;dbl&gt; 223936.4, 110796.6, 492.6, 6605.6, 43642.8, 31104.6, 125…\n$ `2006`        &lt;dbl&gt; 195966.0, 92674.8, 534.7, 5701.0, 35877.1, 24748.0, 1112…\n$ `2005`        &lt;dbl&gt; 167532.8, 75904.8, 315.0, 4797.7, 32435.1, 21286.1, 1114…\n$ `2004`        &lt;dbl&gt; 150911.1, 66795.8, 353.2, 3450.5, 28630.3, 18702.3, 9928…\n$ `2003`        &lt;dbl&gt; 122427.9, 52966.2, 303.1, 2883.1, 23343.2, 15022.2, 8321…\n$ `2002`        &lt;dbl&gt; 109710.5, 49936.3, 369.5, 3071.8, 21539.3, 12656.8, 8882…\n$ `2001`        &lt;dbl&gt; 102892.5, 46286.1, 195.0, 2099.5, 20497.9, 12079.2, 8418…\n$ `2000`        &lt;dbl&gt; 96452.4, 44854.8, 202.4, 1755.3, 20379.3, 11595.5, 8783.…\n\n\n\n\n\n\nCode\nhead(trade_services)\n\n\n# A tibble: 6 × 26\n  `Data Series`   `2024` `2023` `2022` `2021` `2020` `2019` `2018` `2017` `2016`\n  &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Total Trade In… 9.98e5 9.19e5 8.77e5 7.15e5 5.90e5 5.93e5 5.61e5 4.93e5 4.31e5\n2 Exports Of Ser… 5.29e5 4.81e5 4.68e5 3.82e5 3.00e5 3.07e5 2.87e5 2.42e5 2.12e5\n3 Manufacturing … 4.71e2 4.91e2 6.85e2 4.90e2 2.37e2 2.67e2 3.70e2 2.47e2 2.85e2\n4 Maintenance An… 1.08e4 1.10e4 1.01e4 8.83e3 8.17e3 9.66e3 9.41e3 7.71e3 8.42e3\n5 Transport       1.73e5 1.50e5 1.90e5 1.44e5 9.36e4 9.43e4 8.89e4 7.00e4 5.92e4\n6 Freight         1.46e5 1.24e5 1.67e5 1.30e5 7.99e4 7.65e4 7.17e4 5.45e4 4.59e4\n# ℹ 16 more variables: `2015` &lt;dbl&gt;, `2014` &lt;dbl&gt;, `2013` &lt;dbl&gt;, `2012` &lt;dbl&gt;,\n#   `2011` &lt;dbl&gt;, `2010` &lt;dbl&gt;, `2009` &lt;dbl&gt;, `2008` &lt;dbl&gt;, `2007` &lt;dbl&gt;,\n#   `2006` &lt;dbl&gt;, `2005` &lt;dbl&gt;, `2004` &lt;dbl&gt;, `2003` &lt;dbl&gt;, `2002` &lt;dbl&gt;,\n#   `2001` &lt;dbl&gt;, `2000` &lt;dbl&gt;\n\n\n\n\n\n\nCode\nsummary(trade_services)\n\n\n Data Series             2024               2023               2022         \n Length:51          Min.   :    57.8   Min.   :    56.2   Min.   :    59.6  \n Class :character   1st Qu.:  1855.3   1st Qu.:  1817.6   1st Qu.:  1603.0  \n Mode  :character   Median : 13495.7   Median : 12907.1   Median : 11700.4  \n                    Mean   : 71188.6   Mean   : 65608.8   Mean   : 63405.1  \n                    3rd Qu.: 45456.5   3rd Qu.: 43514.1   3rd Qu.: 37732.8  \n                    Max.   :997749.8   Max.   :919117.0   Max.   :877252.5  \n      2021               2020               2019               2018         \n Min.   :    62.4   Min.   :    57.5   Min.   :    52.1   Min.   :    66.8  \n 1st Qu.:  1350.0   1st Qu.:  1209.1   1st Qu.:  1134.8   1st Qu.:  1051.2  \n Median :  9180.3   Median :  9124.8   Median : 10250.8   Median :  9410.0  \n Mean   : 51559.2   Mean   : 42075.8   Mean   : 41792.4   Mean   : 39491.2  \n 3rd Qu.: 35308.2   3rd Qu.: 30748.8   3rd Qu.: 29536.0   3rd Qu.: 27016.0  \n Max.   :715093.5   Max.   :590035.1   Max.   :592824.3   Max.   :561271.0  \n      2017               2016               2015               2014         \n Min.   :    56.1   Min.   :    72.0   Min.   :    46.9   Min.   :    56.9  \n 1st Qu.:   957.6   1st Qu.:   811.6   1st Qu.:   809.8   1st Qu.:   739.1  \n Median :  7848.2   Median :  6458.8   Median :  6194.0   Median :  6021.0  \n Mean   : 34523.8   Mean   : 29987.0   Mean   : 30256.4   Mean   : 28284.6  \n 3rd Qu.: 23594.4   3rd Qu.: 22798.2   3rd Qu.: 23150.7   3rd Qu.: 22757.6  \n Max.   :493353.8   Max.   :431109.3   Max.   :432922.3   Max.   :406020.8  \n      2013               2012               2011               2010         \n Min.   :    78.8   Min.   :    84.6   Min.   :    62.5   Min.   :    64.0  \n 1st Qu.:   661.6   1st Qu.:   656.4   1st Qu.:   617.9   1st Qu.:   587.1  \n Median :  4647.2   Median :  4034.2   Median :  3397.2   Median :  3493.8  \n Mean   : 25332.7   Mean   : 22691.6   Mean   : 20633.9   Mean   : 19039.0  \n 3rd Qu.: 21875.2   3rd Qu.: 19465.0   3rd Qu.: 17328.2   3rd Qu.: 16731.5  \n Max.   :365055.0   Max.   :327866.3   Max.   :298227.8   Max.   :273929.7  \n      2009               2008               2007               2006         \n Min.   :    51.0   Min.   :    76.9   Min.   :   130.4   Min.   :    48.6  \n 1st Qu.:   579.8   1st Qu.:   536.3   1st Qu.:   441.2   1st Qu.:   413.8  \n Median :  3015.6   Median :  3238.9   Median :  2213.8   Median :  2080.4  \n Mean   : 16671.3   Mean   : 17870.3   Mean   : 15789.5   Mean   : 13848.0  \n 3rd Qu.: 13932.2   3rd Qu.: 14221.4   3rd Qu.: 13344.4   3rd Qu.: 11489.8  \n Max.   :238962.6   Max.   :254282.0   Max.   :223936.4   Max.   :195966.0  \n      2005               2004               2003               2002         \n Min.   :    24.7   Min.   :    17.3   Min.   :    17.2   Min.   :    14.3  \n 1st Qu.:   313.1   1st Qu.:   284.9   1st Qu.:   228.2   1st Qu.:   212.3  \n Median :  1524.5   Median :  1374.8   Median :  1108.7   Median :  1001.9  \n Mean   : 11792.0   Mean   : 10647.8   Mean   :  8587.8   Mean   :  7648.4  \n 3rd Qu.: 10741.8   3rd Qu.:  9462.4   3rd Qu.:  7513.7   3rd Qu.:  8339.2  \n Max.   :167532.8   Max.   :150911.1   Max.   :122427.9   Max.   :109710.5  \n      2001               2000        \n Min.   :    22.6   Min.   :   13.6  \n 1st Qu.:   188.7   1st Qu.:  170.6  \n Median :   778.0   Median :  794.0  \n Mean   :  7219.0   Mean   : 6794.9  \n 3rd Qu.:  8164.2   3rd Qu.: 7509.9  \n Max.   :102892.5   Max.   :96452.4  \n\n\n\n\n\n\nCode\ntrade_services[duplicated(trade_services),]\n\n\n# A tibble: 0 × 26\n# ℹ 26 variables: Data Series &lt;chr&gt;, 2024 &lt;dbl&gt;, 2023 &lt;dbl&gt;, 2022 &lt;dbl&gt;,\n#   2021 &lt;dbl&gt;, 2020 &lt;dbl&gt;, 2019 &lt;dbl&gt;, 2018 &lt;dbl&gt;, 2017 &lt;dbl&gt;, 2016 &lt;dbl&gt;,\n#   2015 &lt;dbl&gt;, 2014 &lt;dbl&gt;, 2013 &lt;dbl&gt;, 2012 &lt;dbl&gt;, 2011 &lt;dbl&gt;, 2010 &lt;dbl&gt;,\n#   2009 &lt;dbl&gt;, 2008 &lt;dbl&gt;, 2007 &lt;dbl&gt;, 2006 &lt;dbl&gt;, 2005 &lt;dbl&gt;, 2004 &lt;dbl&gt;,\n#   2003 &lt;dbl&gt;, 2002 &lt;dbl&gt;, 2001 &lt;dbl&gt;, 2000 &lt;dbl&gt;\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nEnsure that there are no duplicated columns, if not will have to investigate further.\n\n\n\n\n\n\n\nCode\ncolSums(is.na(trade_services))\n\n\nData Series        2024        2023        2022        2021        2020 \n          0           0           0           0           0           0 \n       2019        2018        2017        2016        2015        2014 \n          0           0           0           0           0           0 \n       2013        2012        2011        2010        2009        2008 \n          0           0           0           0           0           0 \n       2007        2006        2005        2004        2003        2002 \n          0           0           0           0           0           0 \n       2001        2000 \n          0           0 \n\n\n\n\n\n\n\n\nNote\n\n\n\n\nEnsure that there are no NA values, if not will have to investigate further.\nPossibility to use drop_na() function to drop rows where any specified column contains a missing value.\n\n\n\n\n\n\n\nCode\nstr(trade_services)\n\n\ntibble [51 × 26] (S3: tbl_df/tbl/data.frame)\n $ Data Series: chr [1:51] \"Total Trade In Services\" \"Exports Of Services\" \"Manufacturing Services On Physical Inputs Owned By Others\" \"Maintenance And Repair Services\" ...\n $ 2024       : num [1:51] 997750 528568 471 10820 172971 ...\n $ 2023       : num [1:51] 919117 481009 491 10981 149537 ...\n $ 2022       : num [1:51] 877253 468191 686 10118 189649 ...\n $ 2021       : num [1:51] 715094 382492 490 8834 144220 ...\n $ 2020       : num [1:51] 590035 300005 237 8173 93560 ...\n $ 2019       : num [1:51] 592824 307216 267 9663 94272 ...\n $ 2018       : num [1:51] 561271 287141 370 9410 88889 ...\n $ 2017       : num [1:51] 493354 241568 247 7712 69994 ...\n $ 2016       : num [1:51] 431109 211836 285 8418 59213 ...\n $ 2015       : num [1:51] 432922 210623 346 9315 64097 ...\n $ 2014       : num [1:51] 406021 194843 424 9853 63919 ...\n $ 2013       : num [1:51] 365055 177719 283 10767 57831 ...\n $ 2012       : num [1:51] 327866 161769 250 9053 55586 ...\n $ 2011       : num [1:51] 298228 150013 260 9343 53523 ...\n $ 2010       : num [1:51] 273930 136872 290 8648 52607 ...\n $ 2009       : num [1:51] 238963 117832 323 9128 43366 ...\n $ 2008       : num [1:51] 254282 126155 452 8355 51109 ...\n $ 2007       : num [1:51] 223936 110797 493 6606 43643 ...\n $ 2006       : num [1:51] 195966 92675 535 5701 35877 ...\n $ 2005       : num [1:51] 167533 75905 315 4798 32435 ...\n $ 2004       : num [1:51] 150911 66796 353 3450 28630 ...\n $ 2003       : num [1:51] 122428 52966 303 2883 23343 ...\n $ 2002       : num [1:51] 109711 49936 370 3072 21539 ...\n $ 2001       : num [1:51] 102893 46286 195 2100 20498 ...\n $ 2000       : num [1:51] 96452 44855 202 1755 20379 ...\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nEnsure that all variables are correctly classified by data type; recast variable types if needed.\nVariables are correctly classified - where categorical variables are classified as character, while continuous variables are classified as double.\n\n\n\n\n\n\nThe trade_services tibble contains 26 attributes, as shown above.\nThe following preprocessing checks were conducted as part of data preparation:\n\n\n\n\n\n\nPreprocessing Checks\n\n\n\n\nVerified that the correct data types were loaded in the trade_services dataset using glimpse() and str()\nEnsured there were no duplicate variable names using duplicated() in the dataset\nChecked for missing values using colSums(is.na())\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\ntrade_services dataset will be used for both DataVis makeover 1 & 2\n\n\n\n\n\n\nAfter importing the trade_services dataset, we will filter for the three rows - Exports of Services, Imports of Services, and Total Trade in Services, which are essential for recreating the original visualization.\n\n\nCode\n# Select rows where 'Data Series' is either \"Exports Of Services\" or \"Imports Of Services\"\nexport_vs_import &lt;- trade_services %&gt;%\n  filter(`Data Series` %in% c(\"Exports Of Services\", \"Imports Of Services\", \"Total Trade In Services\"))\n\n# View the filtered data\nprint(export_vs_import)\n\n\n# A tibble: 3 × 26\n  `Data Series`   `2024` `2023` `2022` `2021` `2020` `2019` `2018` `2017` `2016`\n  &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Total Trade In… 9.98e5 9.19e5 8.77e5 7.15e5 5.90e5 5.93e5 5.61e5 4.93e5 4.31e5\n2 Exports Of Ser… 5.29e5 4.81e5 4.68e5 3.82e5 3.00e5 3.07e5 2.87e5 2.42e5 2.12e5\n3 Imports Of Ser… 4.69e5 4.38e5 4.09e5 3.33e5 2.90e5 2.86e5 2.74e5 2.52e5 2.19e5\n# ℹ 16 more variables: `2015` &lt;dbl&gt;, `2014` &lt;dbl&gt;, `2013` &lt;dbl&gt;, `2012` &lt;dbl&gt;,\n#   `2011` &lt;dbl&gt;, `2010` &lt;dbl&gt;, `2009` &lt;dbl&gt;, `2008` &lt;dbl&gt;, `2007` &lt;dbl&gt;,\n#   `2006` &lt;dbl&gt;, `2005` &lt;dbl&gt;, `2004` &lt;dbl&gt;, `2003` &lt;dbl&gt;, `2002` &lt;dbl&gt;,\n#   `2001` &lt;dbl&gt;, `2000` &lt;dbl&gt;\n\n\n\n\n\n\nKey makeover changes:\n1️⃣ Overall design improvements:\n\nReplaced bar chart with a line chart for exports and imports –&gt; Helps in tracking trends more effectively over time instead of color-coded bars per year.\nIntroduced a dashed line for total trade values –&gt; Previously, total trade was only displayed as floating labels above bars, which could be overlooked.\nChanged color usage –&gt; Original chart had different colors for each year, making trend analysis harder. Now, consistent colors are used:\n\nExports are in red,\nImports are in blue,\nTotal Trade is in black (dashed),\nTrade Surplus is in green, and\nTrade Deficit is in red.\n\n\n2️⃣ Trade Balance Integration:\n\nIntegration of key insights –&gt; Previously trade balance was displayed separately as a small section at the bottom. Now, I have integrated it as a bar chart within the main graph\n\ngreen bars represent trade surplus, and\nred bars will represent trade deficit.\n\n\n\n\n\n\n\n\nOverall\n\n\n\n\nThe new chart is cleaner, and provides a better narrative of how exports, imports, total trade, and trade balance evolve over time.\nThe new chart simplifies year-on-year trend analysis, making it easier to identify patterns in Singapore’s international trade.\n\n\n\n\nEnhance graph()Code()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Filter relevant rows (Exports, Imports, and Total Trade)\nexport_vs_import &lt;- trade_services %&gt;%\n  filter(`Data Series` %in% c(\"Exports Of Services\", \"Imports Of Services\", \"Total Trade In Services\"))\n\n# Convert dataset from wide to long format for ggplot\ndata_long &lt;- export_vs_import %&gt;%\n  pivot_longer(cols = -`Data Series`, names_to = \"Year\", values_to = \"Value\") %&gt;%\n  mutate(Year = as.numeric(Year)) %&gt;%\n  filter(Year &gt;= 2020 & Year &lt;= 2024)\n\n# Extract `Total Trade In Services` from dataset\ntotal_trade_data &lt;- data_long %&gt;%\n  filter(`Data Series` == \"Total Trade In Services\") %&gt;%\n  select(Year, Value) %&gt;%\n  rename(Total_Trade = Value)\n\n# Calculate Trade Balance (Exports - Imports)\ntrade_balance &lt;- data_long %&gt;%\n  spread(`Data Series`, Value) %&gt;%\n  mutate(Services_Trade_Balance = `Exports Of Services` - `Imports Of Services`) %&gt;%\n  select(Year, Services_Trade_Balance)\n\n# Merge Trade Balance & Total Trade into DataFrame\ndata_long &lt;- left_join(data_long, trade_balance, by = \"Year\")\ndata_long &lt;- left_join(data_long, total_trade_data, by = \"Year\")\n\n# Increase plotting window size\noptions(repr.plot.width=16, repr.plot.height=9)  \n\n# Create the plot\np &lt;- ggplot(data_long) +\n\n  # **Line Chart for Exports**\n  geom_line(data = subset(data_long, `Data Series` == \"Exports Of Services\"), \n            aes(x = Year, y = Value, color = \"Exports\"), linewidth = 1.5) +\n  \n  # **Data Labels for Exports**\n  geom_text(data = subset(data_long, `Data Series` == \"Exports Of Services\"), \n            aes(x = Year, y = Value, label = round(Value, 1)), \n            vjust = -1, size = 2.5, color = \"red\") +\n\n  # **Line Chart for Imports**\n  geom_line(data = subset(data_long, `Data Series` == \"Imports Of Services\"), \n            aes(x = Year, y = Value, color = \"Imports\"), linewidth = 1.5) +\n\n  # **Data Labels for Imports**\n  geom_text(data = subset(data_long, `Data Series` == \"Imports Of Services\"), \n            aes(x = Year, y = Value, label = round(Value, 1)), \n            vjust = 1.5, size = 2.5, color = \"blue\") +\n\n  # **Line Chart for Total Trade (Black, Dashed)**\n  geom_line(data = total_trade_data,\n            aes(x = Year, y = Total_Trade, color = \"Total Trade\"), linewidth = 1.5, linetype = \"dashed\") +\n\n  # **Labels for Total Trade**\n  geom_text(data = total_trade_data,\n            aes(x = Year, y = Total_Trade + 20000, label = round(Total_Trade, 1)), \n            vjust = -0.5, size = 2.5, color = \"black\") +\n\n  # **Bar Chart for Trade Balance (Surplus = Green, Deficit = Red)**\n  geom_bar(data = trade_balance, \n           aes(x = Year, y = Services_Trade_Balance, \n               fill = ifelse(Services_Trade_Balance &gt;= 0, \"Surplus\", \"Deficit\")), \n           stat = \"identity\", width = 0.5) +\n\n  # **Move Labels to the Top of Trade Balance Bars**\n  geom_text(data = trade_balance, \n            aes(x = Year, y = Services_Trade_Balance + 5000, \n                label = round(Services_Trade_Balance, 1)), \n            vjust = -0.5, size = 2.5, color = \"black\") +\n\n  # **Title & Labels**\n  labs(title = \"Trends in International Trade in Services (2020-2024)\",\n       subtitle = \"Exports, Imports & Total Trade as Lines, Trade Balance as Bars\",\n       x = \"Year\", y = \"S$ Billion\") +\n\n  # **Custom Theme**\n  theme_minimal(base_size = 10) +  \n\n  # **Color Customization**\n  scale_color_manual(name = \"Category\", values = c(\"Exports\" = \"red\", \"Imports\" = \"blue\", \"Total Trade\" = \"black\")) +\n  scale_fill_manual(name = \"Trade Balance\", values = c(\"Surplus\" = \"green\", \"Deficit\" = \"red\")) +\n\n  # **Formatting**\n  theme(\n        legend.position = \"bottom\",\n        panel.grid.major.y = element_line(color = \"gray\", linetype = \"dashed\"),\n        axis.text.x = element_text(face = \"bold\"))\n\n# Display the plot\nprint(p)\n\n\n\n\n\n\n\n\n\nI conducted time-series analysis to analyze trends in exports, imports, and total trade from 2000 to 2024. First, I visualized the historical data using ggplot2, ensuring clear differentiation of trends by assigning red for exports, blue for imports, and black for total trade. This provided insights into the overall growth patterns and fluctuations over time.\nKey observations:\n\nSteady growth: Singapore’s total trade in services has shown a strong upward trend, reflecting its role as a global trade hub.\nExports Lead Imports: While both have grown, exports consistently exceed imports, indicating a positive trade balance.\nImpact of Global Events: Periods of slower growth (e.g., 2008-2012) align with economic disruptions like the Global Financial Crisis.\nAccelerated Growth After 2018: Likely driven by Singapore’s push for digital services, financial innovation, and trade agreements.\n\n\nGraph()Code()\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(forecast)\nlibrary(plotly)\n\n# Convert data from wide to long format\nexport_vs_import_long &lt;- export_vs_import %&gt;%\n  pivot_longer(cols = -`Data Series`, names_to = \"Year\", values_to = \"Value\") %&gt;%\n  mutate(Year = as.numeric(Year)) %&gt;%\n  arrange(Year)  # Ensure years are sorted in ascending order\n\n# Create separate data frames for each category to ensure proper order\nexports_data &lt;- export_vs_import_long %&gt;% filter(`Data Series` == \"Exports Of Services\")\nimports_data &lt;- export_vs_import_long %&gt;% filter(`Data Series` == \"Imports Of Services\")\ntotal_trade_data &lt;- export_vs_import_long %&gt;% filter(`Data Series` == \"Total Trade In Services\")\n\n# Convert to time-series format (with correctly sorted values)\nexports_ts &lt;- ts(exports_data$Value, start = exports_data$Year[1], frequency = 1)\nimports_ts &lt;- ts(imports_data$Value, start = imports_data$Year[1], frequency = 1)\ntotal_trade_ts &lt;- ts(total_trade_data$Value, start = total_trade_data$Year[1], frequency = 1)\n\n# Create Data Frame for Plotting\ndf &lt;- data.frame(\n  Year = rep(exports_data$Year, 3),\n  Value = c(exports_data$Value, imports_data$Value, total_trade_data$Value),\n  Category = rep(c(\"Exports\", \"Imports\", \"Total Trade\"), each = nrow(exports_data))\n)\n\n# Create a ggplot with explicit group aesthetic\np &lt;- ggplot(df, aes(x = Year, y = Value, color = Category, group = Category, \n                     text = paste0(\"Year: \", Year, \"&lt;br&gt;Value: \", round(Value, 1)))) +\n  geom_line(linewidth = 1.2) +  # Use linewidth instead of size (ggplot2 3.4.0+)\n  scale_color_manual(values = c(\"Exports\" = \"red\", \"Imports\" = \"blue\", \"Total Trade\" = \"black\")) +\n  ggtitle(\"Export vs Import vs Total Trade Trends\") +\n  ylab(\"Value (in Millions)\") + xlab(\"Year\") +\n  theme_minimal()\n\n# Convert to interactive plot with tooltips\ninteractive_plot &lt;- ggplotly(p, tooltip = \"text\")\n\n# Display interactive plot\ninteractive_plot\n\n\n\n\n\n\n\n\n\n\nBased on the model accuracy metrics, ARIMA outperforms ETS in forecasting both exports and imports. ARIMA consistently shows lower RMSE, MAE, and MAPE values, indicating that its predictions are more precise and closer to actual observations. Thus, ARIMA will be the preferred model for time series forecasting.\n\nResult()Code()\n\n\n\n\n                    ME     RMSE       MAE        MPE      MAPE      MASE\nTraining set  2250.362 11275.74  8425.338 -0.1740338  7.973583 0.5737539\nTest set     66791.835 86554.64 77415.700 13.2622158 16.803450 5.2719028\n                   ACF1\nTraining set 0.05311589\nTest set             NA\n\n\n                    ME     RMSE       MAE       MPE      MAPE      MASE\nTraining set  3516.706 10885.93  7472.406  2.582523  5.207104 0.5088606\nTest set     54848.687 74121.02 67064.995 10.655012 14.727052 4.5670340\n                  ACF1\nTraining set -0.109228\nTest set            NA\n\n\n# A tibble: 4 × 5\n  Model Type      RMSE    MAE  MAPE\n  &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 ETS   Exports 86555. 77416.  16.8\n2 ETS   Imports 89216. 74235.  17.4\n3 ARIMA Exports 74121. 67065.  14.7\n4 ARIMA Imports 81957. 68397.  16.0\n\n\n\n\n\n\nCode\nlibrary(forecast)\nlibrary(tidyverse)\nlibrary(ggplot2)\n\n# Set forecast horizon (e.g., last 5 years)\nhorizon &lt;- 5\n\n# Ensure there are enough observations for training and test\nif (length(exports_ts) &lt;= horizon | length(imports_ts) &lt;= horizon) {\n  stop(\"Not enough data points for proper train-test split!\")\n}\n\n# Split data: Use last `horizon` years as the test set\ntrain_exports &lt;- head(exports_ts, length(exports_ts) - horizon)\ntest_exports &lt;- tail(exports_ts, horizon)\n\ntrain_imports &lt;- head(imports_ts, length(imports_ts) - horizon)\ntest_imports &lt;- tail(imports_ts, horizon)\n\n# Fit ETS models\nets_exports &lt;- ets(train_exports)\nets_imports &lt;- ets(train_imports)\n\n# Fit ARIMA models\narima_exports &lt;- auto.arima(train_exports)\narima_imports &lt;- auto.arima(train_imports)\n\n# 🔹 Generate Forecasts BEFORE extracting forecast values\nets_forecast_exports &lt;- forecast(ets_exports, h = horizon)\nets_forecast_imports &lt;- forecast(ets_imports, h = horizon)\n\narima_forecast_exports &lt;- forecast(arima_exports, h = horizon)\narima_forecast_imports &lt;- forecast(arima_imports, h = horizon)\n\n# 🔹 Extract the forecasted mean values\nets_forecast_exports_values &lt;- as.numeric(ets_forecast_exports$mean)\nets_forecast_imports_values &lt;- as.numeric(ets_forecast_imports$mean)\n\narima_forecast_exports_values &lt;- as.numeric(arima_forecast_exports$mean)\narima_forecast_imports_values &lt;- as.numeric(arima_forecast_imports$mean)\n\n# Ensure test sets are numeric\ntest_exports &lt;- as.numeric(test_exports)\ntest_imports &lt;- as.numeric(test_imports)\n\n# Ensure test and forecast lengths match\nif (length(test_exports) != length(ets_forecast_exports_values)) {\n  test_exports &lt;- head(test_exports, length(ets_forecast_exports_values))\n}\n\nif (length(test_imports) != length(ets_forecast_imports_values)) {\n  test_imports &lt;- head(test_imports, length(ets_forecast_imports_values))\n}\n\n# 🔹 Use accuracy() correctly with the model object\nets_accuracy_exports &lt;- forecast::accuracy(ets_forecast_exports, test_exports)\nets_accuracy_imports &lt;- forecast::accuracy(ets_forecast_imports, test_imports)\n\narima_accuracy_exports &lt;- forecast::accuracy(arima_forecast_exports, test_exports)\narima_accuracy_imports &lt;- forecast::accuracy(arima_forecast_imports, test_imports)\n\nprint(ets_accuracy_exports)\nprint(arima_accuracy_exports)\n\n# 🔹 Combine accuracy results into a structured dataframe\naccuracy_df &lt;- tibble(\n  Model = rep(c(\"ETS\", \"ARIMA\"), each = 2),\n  Type = rep(c(\"Exports\", \"Imports\"), times = 2),\n  RMSE = c(ets_accuracy_exports[\"Test set\", \"RMSE\"], \n           ets_accuracy_imports[\"Test set\", \"RMSE\"],\n           arima_accuracy_exports[\"Test set\", \"RMSE\"], \n           arima_accuracy_imports[\"Test set\", \"RMSE\"]),\n  MAE = c(ets_accuracy_exports[\"Test set\", \"MAE\"], \n          ets_accuracy_imports[\"Test set\", \"MAE\"],\n          arima_accuracy_exports[\"Test set\", \"MAE\"], \n          arima_accuracy_imports[\"Test set\", \"MAE\"]),\n  MAPE = c(ets_accuracy_exports[\"Test set\", \"MAPE\"], \n           ets_accuracy_imports[\"Test set\", \"MAPE\"],\n           arima_accuracy_exports[\"Test set\", \"MAPE\"], \n           arima_accuracy_imports[\"Test set\", \"MAPE\"])\n)\n\n# Print final accuracy dataframe\nprint(accuracy_df)\n\n\n\n\n\n\n\n\nNext, I applied ARIMA modeling (auto.arima()) using the forecast package to predict trade values for the next five years (2025-2029).\n\nARIMA Forecast for Exports()ARIMA Forecast for Imports()ARIMA Forecast for Total Trade()Combined Forecast()Code()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(forecast)\n\n# Apply ARIMA Model\narima_exports &lt;- auto.arima(exports_ts)\narima_imports &lt;- auto.arima(imports_ts)\narima_total_trade &lt;- auto.arima(total_trade_ts)\n\n# Forecast for the next 5 years\narima_forecast_exports &lt;- forecast(arima_exports, h = 5)\narima_forecast_imports &lt;- forecast(arima_imports, h = 5)\narima_forecast_total_trade &lt;- forecast(arima_total_trade, h = 5)\n\n# Plot ARIMA forecasts\nautoplot(arima_forecast_exports) + ggtitle(\"ARIMA Forecast for Exports\")\nautoplot(arima_forecast_imports) + ggtitle(\"ARIMA Forecast for Imports\")\nautoplot(arima_forecast_total_trade) + ggtitle(\"ARIMA Forecast for Total Trade\")\n\n# Convert forecasts to data frames for ggplot\ndf_exports &lt;- as.data.frame(arima_forecast_exports) %&gt;% mutate(Year = seq(max(exports_data$Year) + 1, by = 1, length.out = 5), Category = \"Exports\")\ndf_imports &lt;- as.data.frame(arima_forecast_imports) %&gt;% mutate(Year = seq(max(imports_data$Year) + 1, by = 1, length.out = 5), Category = \"Imports\")\ndf_total_trade &lt;- as.data.frame(arima_forecast_total_trade) %&gt;% mutate(Year = seq(max(total_trade_data$Year) + 1, by = 1, length.out = 5), Category = \"Total Trade\")\n\n# Combine all forecasts\ndf_forecast &lt;- bind_rows(df_exports, df_imports, df_total_trade)\n\n# Create the forecast plot\nggplot(df_forecast, aes(x = Year, y = `Point Forecast`, color = Category)) +\n  geom_line(linewidth = 1.5) +  # Use linewidth instead of size (ggplot2 3.4.0+)\n  scale_color_manual(values = c(\"Exports\" = \"red\", \"Imports\" = \"blue\", \"Total Trade\" = \"black\")) +\n  ggtitle(\"ARIMA Forecast for Exports, Imports, and Total Trade\") +\n  ylab(\"Forecasted Value (in Millions)\") + xlab(\"Year\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWe will also be testing the accuracy of the forecast model to evaluate its performance and reliability.\nStandard market practices for evaluating forecast accuracy rely on key metrics such as MAPE, RMSE, MAE, and ACF1.\n\nBased on these measures, the imports forecast is highly accurate (MAPE ~4.84%), while exports and total trade forecasts fall within the “good” range (MAPE ~6.10% and ~5.23%).\nThe absence of significant autocorrelation in residuals suggests the models effectively capture trends.\nThe high RMSE values, particularly for total trade, indicate potential reliability issues due to large fluctuations in data.\n\n\nForecast accuracy for Exports()Forecast accuracy for Imports()Forecast accuracy for Total Trade()Code()\n\n\n\n\n### Forecast Accuracy for Exports ###\n\n\n                      ME      RMSE       MAE        MPE      MAPE       MASE\nTraining set    5842.817  21775.54  13733.86   2.777663  6.099909  0.6402954\nTest set     -215550.380 217655.84 215550.38 -53.417606 53.417606 10.0493182\n                    ACF1\nTraining set -0.09397345\nTest set              NA\n\n\n\n\n\n\n\n### Forecast Accuracy for Imports ###\n\n\n                      ME      RMSE       MAE        MPE      MAPE       MASE\nTraining set    3887.432  15372.93  10045.78   1.659329  4.840996  0.5509206\nTest set     -182952.628 184188.44 182952.63 -49.695067 49.695067 10.0333023\n                   ACF1\nTraining set 0.03707208\nTest set             NA\n\n\n\n\n\n\n\n### Forecast Accuracy for Total Trade ###\n\n\n                      ME      RMSE       MAE        MPE      MAPE       MASE\nTraining set    9537.844  35750.43  23074.03   2.170273  5.229894  0.5884102\nTest set     -399957.217 403059.64 399957.22 -51.756773 51.756773 10.1992978\n                    ACF1\nTraining set -0.02527777\nTest set              NA\n\n\n\n\n\n\nCode\n# Compute and print accuracy\ncat(\"### Forecast Accuracy for Exports ###\\n\")\nprint(forecast::accuracy(arima_forecast_exports, test_exports))\n\n# Print forecast accuracy for imports\ncat(\"\\n### Forecast Accuracy for Imports ###\\n\")\nprint(forecast::accuracy(arima_forecast_imports, test_imports))\n\n# Print forecast accuracy for total trade\ncat(\"\\n### Forecast Accuracy for Total Trade ###\\n\")\nprint(forecast::accuracy(arima_forecast_total_trade, test_total_trade))"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02.html#datavis-makeover---overall-exports-and-import-of-services",
    "href": "Take-home_Ex/Take-home_Ex02.html#datavis-makeover---overall-exports-and-import-of-services",
    "title": "Take-home Exercise 02",
    "section": "",
    "text": "The original visualizations analyzed in this analysis are sourced from here.\n\n\n\n\nIn the below section, we will evaluate the effectiveness of the above visualization by identifying its pros and cons, focusing on aspects such as (1) clarity, and (2) visual appeal:\n\nClarity: How well the data is presented and understood\n\n\n\n\n\n\n\n\n\nPros\nCons\nSuggested fixes\n\n\n\n\nClear categorization of exports and imports - The chart effectively differentiates between exports and imports\nYear-specific colors make trend analysis difficult – Each year is assigned a different color, making it hard to track trends across time.\nUse consistent colors for exports and imports across all years (Implemented by converting them into line charts).\n\n\nTotal trade values are highlighted – The total trade (Exports + Imports) is displayed for each year.\nFloating “Total” labels can be easily overlooked – They are placed above bars separately, which may lead to misinterpretation.\nIntroduce a line chart for total trade trends instead of floating labels.\n\n\nGrowth rate (CAGR) and trade balance data are included – Additional insights are provided.\nTrade balance is not well integrated – It is displayed separately at the bottom instead of being visually linked to the bars.\nRepresent trade balance directly as a bar chart (Green for surplus, Red for deficit)\n\n\n\n\nVisual appeal: How visually engaging and effective the design is\n\n\n\n\n\n\n\n\n\nPros\nCons\nSuggested fixes\n\n\n\n\nEngaging use of colors and icons – Makes the visualization appealing and eye-catching.\nOveruse of colors creates clutter – Different colors for each year make it visually overwhelming.\nReduce unnecessary color variations and simplify color coding.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Select rows where 'Data Series' is either \"Exports Of Services\" or \"Imports Of Services\"\nexport_vs_import &lt;- trade_services %&gt;%\n  filter(`Data Series` %in% c(\"Exports Of Services\", \"Imports Of Services\", \"Total Trade In Services\"))\n\n# View the filtered data\nprint(export_vs_import)\n\n\n# A tibble: 3 × 26\n  `Data Series`   `2024` `2023` `2022` `2021` `2020` `2019` `2018` `2017` `2016`\n  &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Total Trade In… 9.98e5 9.19e5 8.77e5 7.15e5 5.90e5 5.93e5 5.61e5 4.93e5 4.31e5\n2 Exports Of Ser… 5.29e5 4.81e5 4.68e5 3.82e5 3.00e5 3.07e5 2.87e5 2.42e5 2.12e5\n3 Imports Of Ser… 4.69e5 4.38e5 4.09e5 3.33e5 2.90e5 2.86e5 2.74e5 2.52e5 2.19e5\n# ℹ 16 more variables: `2015` &lt;dbl&gt;, `2014` &lt;dbl&gt;, `2013` &lt;dbl&gt;, `2012` &lt;dbl&gt;,\n#   `2011` &lt;dbl&gt;, `2010` &lt;dbl&gt;, `2009` &lt;dbl&gt;, `2008` &lt;dbl&gt;, `2007` &lt;dbl&gt;,\n#   `2006` &lt;dbl&gt;, `2005` &lt;dbl&gt;, `2004` &lt;dbl&gt;, `2003` &lt;dbl&gt;, `2002` &lt;dbl&gt;,\n#   `2001` &lt;dbl&gt;, `2000` &lt;dbl&gt;\n\n\n\n\n\n\nEnhance graph()Code()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Filter relevant rows (Exports and Imports only)\nexport_vs_import &lt;- trade_services %&gt;%\n  filter(`Data Series` %in% c(\"Exports Of Services\", \"Imports Of Services\"))\n\n# Convert dataset from wide to long format for ggplot\ndata_long &lt;- export_vs_import %&gt;%\n  pivot_longer(cols = -`Data Series`, names_to = \"Year\", values_to = \"Value\") %&gt;%\n  mutate(Year = as.numeric(Year)) %&gt;%\n  filter(Year &gt;= 2020 & Year &lt;= 2024)  # Filter for years 2020-2024 only\n\n# Calculate Trade Balance (Exports - Imports) and Total Trade (Exports + Imports)\ntrade_summary &lt;- data_long %&gt;%\n  spread(`Data Series`, Value) %&gt;%\n  mutate(\n    Services_Trade_Balance = `Exports Of Services` - `Imports Of Services`,\n    Total_Trade = `Exports Of Services` + `Imports Of Services`\n  ) %&gt;%\n  select(Year, Services_Trade_Balance, Total_Trade)\n\n# Merge Trade Balance & Total Trade into DataFrame\ndata_long &lt;- left_join(data_long, trade_summary, by = \"Year\")\n\n# Increase plotting window size\noptions(repr.plot.width=16, repr.plot.height=9)  \n\n# Create the plot\np &lt;- ggplot(data_long) +\n\n  # **Line Chart for Exports**\n  geom_line(data = subset(data_long, `Data Series` == \"Exports Of Services\"), \n            aes(x = Year, y = Value, color = \"Exports\"), size = 1.5) +\n  \n  # **Data Labels for Exports (Normal Font)**\n  geom_text(data = subset(data_long, `Data Series` == \"Exports Of Services\"), \n            aes(x = Year, y = Value, label = round(Value, 1)), \n            vjust = -1, size = 2.5, color = \"red\", fontface = \"plain\") +\n\n  # **Line Chart for Imports**\n  geom_line(data = subset(data_long, `Data Series` == \"Imports Of Services\"), \n            aes(x = Year, y = Value, color = \"Imports\"), size = 1.5) +\n\n  # **Data Labels for Imports (Normal Font)**\n  geom_text(data = subset(data_long, `Data Series` == \"Imports Of Services\"), \n            aes(x = Year, y = Value, label = round(Value, 1)), \n            vjust = 1.5, size = 2.5, color = \"blue\", fontface = \"plain\") +\n\n  # **Line Chart for Total Trade (Black, Dashed)**\n  geom_line(data = distinct(data_long, Year, Total_Trade),\n            aes(x = Year, y = Total_Trade, color = \"Total Trade\"), size = 1.5, linetype = \"dashed\") +\n\n  # **Fixed Placement for Total Trade Labels (Higher Above the Line)**\n  geom_text(data = distinct(data_long, Year, Total_Trade),\n            aes(x = Year, y = Total_Trade + 20000,  # Moving label slightly higher\n                label = round(Total_Trade, 1)), \n            vjust = -0.5, size = 2.5, color = \"black\", fontface = \"plain\") +\n\n  # **Bar Chart for Trade Balance (Surplus = Green, Deficit = Red)**\n  geom_bar(data = trade_summary, \n           aes(x = Year, y = Services_Trade_Balance, \n               fill = ifelse(Services_Trade_Balance &gt;= 0, \"Surplus\", \"Deficit\")), \n           stat = \"identity\", width = 0.5) +\n\n  # **Move Labels to the Top of Trade Balance Bars (Normal Font)**\n  geom_text(data = trade_summary, \n            aes(x = Year, y = Services_Trade_Balance + 5000,  # Move label slightly above\n                label = round(Services_Trade_Balance, 1)), \n            vjust = -0.5, size = 2.5, color = \"black\", fontface = \"plain\") +\n\n  # **Title & Labels**\n  labs(title = \"Trends in International Trade in Services (2020-2024)\",\n       subtitle = \"Exports, Imports & Total Trade as Lines, Trade Balance as Bars\",\n       x = \"Year\", y = \"S$ Billion\") +\n\n  # **Custom Theme**\n  theme_minimal(base_size = 10) +  # Reduce base size for overall text\n\n  # **Color Customization**\n  scale_color_manual(name = \"Category\", values = c(\"Exports\" = \"red\", \"Imports\" = \"blue\", \"Total Trade\" = \"black\")) +\n  scale_fill_manual(name = \"Trade Balance\", values = c(\"Surplus\" = \"green\", \"Deficit\" = \"red\")) +\n\n  # **Formatting**\n  theme(\n        legend.position = \"bottom\",\n        panel.grid.major.y = element_line(color = \"gray\", linetype = \"dashed\"),\n        axis.text.x = element_text(face = \"bold\"))  # **Make x-axis labels bold**\n\n# Display the plot\nprint(p)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02.html#datavis-makeover",
    "href": "Take-home_Ex/Take-home_Ex02.html#datavis-makeover",
    "title": "Take-home Exercise 02",
    "section": "",
    "text": "The original visualizations analyzed in this analysis are sourced from here.\n\n\n\n\nIn the below section, we will evaluate the effectiveness of the above visualization by identifying its pros and cons, focusing on aspects such as (1) clarity, and (2) visual appeal:\n\nClarity: How well the data is presented and understood\n\n\n\n\n\n\n\n\n\nPros\nCons\nSuggested fixes\n\n\n\n\nClear categorization of exports and imports - The chart effectively differentiates between exports and imports\nYear-specific colors make trend analysis difficult – Each year is assigned a different color, making it hard to track trends across time.\nUse consistent colors for exports and imports across all years (Implemented by converting them into line charts).\n\n\nTotal trade values are highlighted – The total trade (Exports + Imports) is displayed for each year.\nFloating “Total” labels can be easily overlooked – They are placed above bars separately, which may lead to misinterpretation.\nIntroduce a line chart for total trade trends instead of floating labels.\n\n\nGrowth rate (CAGR) and trade balance data are included – Additional insights are provided.\nTrade balance is not well integrated – It is displayed separately at the bottom instead of being visually linked to the bars.\nRepresent trade balance directly as a bar chart (Green for surplus, Red for deficit)\n\n\n\n\nVisual appeal: How visually engaging and effective the design is\n\n\n\n\n\n\n\n\n\nPros\nCons\nSuggested fixes\n\n\n\n\nEngaging use of colors and icons – Makes the visualization appealing and eye-catching.\nOveruse of colors creates clutter – Different colors for each year make it visually overwhelming.\nReduce unnecessary color variations and simplify color coding.\n\n\n\n\n\n\n\n\n\nAfter importing the trade_services dataset, we will filter for the three rows - Exports of Services, Imports of Services, and Total Trade in Services, which are essential for recreating the original visualization.\n\n\nCode\n# Select rows where 'Data Series' is either \"Exports Of Services\" or \"Imports Of Services\"\nexport_vs_import &lt;- trade_services %&gt;%\n  filter(`Data Series` %in% c(\"Exports Of Services\", \"Imports Of Services\", \"Total Trade In Services\"))\n\n# View the filtered data\nprint(export_vs_import)\n\n\n# A tibble: 3 × 26\n  `Data Series`   `2024` `2023` `2022` `2021` `2020` `2019` `2018` `2017` `2016`\n  &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Total Trade In… 9.98e5 9.19e5 8.77e5 7.15e5 5.90e5 5.93e5 5.61e5 4.93e5 4.31e5\n2 Exports Of Ser… 5.29e5 4.81e5 4.68e5 3.82e5 3.00e5 3.07e5 2.87e5 2.42e5 2.12e5\n3 Imports Of Ser… 4.69e5 4.38e5 4.09e5 3.33e5 2.90e5 2.86e5 2.74e5 2.52e5 2.19e5\n# ℹ 16 more variables: `2015` &lt;dbl&gt;, `2014` &lt;dbl&gt;, `2013` &lt;dbl&gt;, `2012` &lt;dbl&gt;,\n#   `2011` &lt;dbl&gt;, `2010` &lt;dbl&gt;, `2009` &lt;dbl&gt;, `2008` &lt;dbl&gt;, `2007` &lt;dbl&gt;,\n#   `2006` &lt;dbl&gt;, `2005` &lt;dbl&gt;, `2004` &lt;dbl&gt;, `2003` &lt;dbl&gt;, `2002` &lt;dbl&gt;,\n#   `2001` &lt;dbl&gt;, `2000` &lt;dbl&gt;\n\n\n\n\n\nKey makeover changes:\n1️⃣ Overall design improvements:\n\nReplaced bar chart with a line chart for exports and imports –&gt; Helps in tracking trends more effectively over time instead of color-coded bars per year.\nIntroduced a dashed line for total trade values –&gt; Previously, total trade was only displayed as floating labels above bars, which could be overlooked.\nChanged color usage –&gt; Original chart had different colors for each year, making trend analysis harder. Now, consistent colors are used:\n\nExports are in red,\nImports are in blue,\nTotal Trade is in black (dashed),\nTrade Surplus is in green, and\nTrade Deficit is in red.\n\n\n2️⃣ Trade Balance Integration:\n\nIntegration of key insights –&gt; Previously trade balance was displayed separately as a small section at the bottom. Now, I have integrated it as a bar chart within the main graph\n\ngreen bars represent trade surplus, and\nred bars will represent trade deficit.\n\n\n\n\n\n\n\n\nOverall\n\n\n\n\nThe new chart is cleaner, and provides a better narrative of how exports, imports, total trade, and trade balance evolve over time.\nThe new chart simplifies year-on-year trend analysis, making it easier to identify patterns in Singapore’s international trade.\n\n\n\n\nEnhance graph()Code()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Filter relevant rows (Exports, Imports, and Total Trade)\nexport_vs_import &lt;- trade_services %&gt;%\n  filter(`Data Series` %in% c(\"Exports Of Services\", \"Imports Of Services\", \"Total Trade In Services\"))\n\n# Convert dataset from wide to long format for ggplot\ndata_long &lt;- export_vs_import %&gt;%\n  pivot_longer(cols = -`Data Series`, names_to = \"Year\", values_to = \"Value\") %&gt;%\n  mutate(Year = as.numeric(Year)) %&gt;%\n  filter(Year &gt;= 2020 & Year &lt;= 2024)\n\n# Extract `Total Trade In Services` from dataset\ntotal_trade_data &lt;- data_long %&gt;%\n  filter(`Data Series` == \"Total Trade In Services\") %&gt;%\n  select(Year, Value) %&gt;%\n  rename(Total_Trade = Value)\n\n# Calculate Trade Balance (Exports - Imports)\ntrade_balance &lt;- data_long %&gt;%\n  spread(`Data Series`, Value) %&gt;%\n  mutate(Services_Trade_Balance = `Exports Of Services` - `Imports Of Services`) %&gt;%\n  select(Year, Services_Trade_Balance)\n\n# Merge Trade Balance & Total Trade into DataFrame\ndata_long &lt;- left_join(data_long, trade_balance, by = \"Year\")\ndata_long &lt;- left_join(data_long, total_trade_data, by = \"Year\")\n\n# Increase plotting window size\noptions(repr.plot.width=16, repr.plot.height=9)  \n\n# Create the plot\np &lt;- ggplot(data_long) +\n\n  # **Line Chart for Exports**\n  geom_line(data = subset(data_long, `Data Series` == \"Exports Of Services\"), \n            aes(x = Year, y = Value, color = \"Exports\"), linewidth = 1.5) +\n  \n  # **Data Labels for Exports**\n  geom_text(data = subset(data_long, `Data Series` == \"Exports Of Services\"), \n            aes(x = Year, y = Value, label = round(Value, 1)), \n            vjust = -1, size = 2.5, color = \"red\") +\n\n  # **Line Chart for Imports**\n  geom_line(data = subset(data_long, `Data Series` == \"Imports Of Services\"), \n            aes(x = Year, y = Value, color = \"Imports\"), linewidth = 1.5) +\n\n  # **Data Labels for Imports**\n  geom_text(data = subset(data_long, `Data Series` == \"Imports Of Services\"), \n            aes(x = Year, y = Value, label = round(Value, 1)), \n            vjust = 1.5, size = 2.5, color = \"blue\") +\n\n  # **Line Chart for Total Trade (Black, Dashed)**\n  geom_line(data = total_trade_data,\n            aes(x = Year, y = Total_Trade, color = \"Total Trade\"), linewidth = 1.5, linetype = \"dashed\") +\n\n  # **Labels for Total Trade**\n  geom_text(data = total_trade_data,\n            aes(x = Year, y = Total_Trade + 20000, label = round(Total_Trade, 1)), \n            vjust = -0.5, size = 2.5, color = \"black\") +\n\n  # **Bar Chart for Trade Balance (Surplus = Green, Deficit = Red)**\n  geom_bar(data = trade_balance, \n           aes(x = Year, y = Services_Trade_Balance, \n               fill = ifelse(Services_Trade_Balance &gt;= 0, \"Surplus\", \"Deficit\")), \n           stat = \"identity\", width = 0.5) +\n\n  # **Move Labels to the Top of Trade Balance Bars**\n  geom_text(data = trade_balance, \n            aes(x = Year, y = Services_Trade_Balance + 5000, \n                label = round(Services_Trade_Balance, 1)), \n            vjust = -0.5, size = 2.5, color = \"black\") +\n\n  # **Title & Labels**\n  labs(title = \"Trends in International Trade in Services (2020-2024)\",\n       subtitle = \"Exports, Imports & Total Trade as Lines, Trade Balance as Bars\",\n       x = \"Year\", y = \"S$ Billion\") +\n\n  # **Custom Theme**\n  theme_minimal(base_size = 10) +  \n\n  # **Color Customization**\n  scale_color_manual(name = \"Category\", values = c(\"Exports\" = \"red\", \"Imports\" = \"blue\", \"Total Trade\" = \"black\")) +\n  scale_fill_manual(name = \"Trade Balance\", values = c(\"Surplus\" = \"green\", \"Deficit\" = \"red\")) +\n\n  # **Formatting**\n  theme(\n        legend.position = \"bottom\",\n        panel.grid.major.y = element_line(color = \"gray\", linetype = \"dashed\"),\n        axis.text.x = element_text(face = \"bold\"))\n\n# Display the plot\nprint(p)\n\n\n\n\n\n\n\n\n\nI conducted time-series analysis to analyze trends in exports, imports, and total trade from 2000 to 2024. First, I visualized the historical data using ggplot2, ensuring clear differentiation of trends by assigning red for exports, blue for imports, and black for total trade. This provided insights into the overall growth patterns and fluctuations over time.\nKey observations:\n\nSteady growth: Singapore’s total trade in services has shown a strong upward trend, reflecting its role as a global trade hub.\nExports Lead Imports: While both have grown, exports consistently exceed imports, indicating a positive trade balance.\nImpact of Global Events: Periods of slower growth (e.g., 2008-2012) align with economic disruptions like the Global Financial Crisis.\nAccelerated Growth After 2018: Likely driven by Singapore’s push for digital services, financial innovation, and trade agreements.\n\n\nGraph()Code()\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(forecast)\nlibrary(plotly)\n\n# Convert data from wide to long format\nexport_vs_import_long &lt;- export_vs_import %&gt;%\n  pivot_longer(cols = -`Data Series`, names_to = \"Year\", values_to = \"Value\") %&gt;%\n  mutate(Year = as.numeric(Year)) %&gt;%\n  arrange(Year)  # Ensure years are sorted in ascending order\n\n# Create separate data frames for each category to ensure proper order\nexports_data &lt;- export_vs_import_long %&gt;% filter(`Data Series` == \"Exports Of Services\")\nimports_data &lt;- export_vs_import_long %&gt;% filter(`Data Series` == \"Imports Of Services\")\ntotal_trade_data &lt;- export_vs_import_long %&gt;% filter(`Data Series` == \"Total Trade In Services\")\n\n# Convert to time-series format (with correctly sorted values)\nexports_ts &lt;- ts(exports_data$Value, start = exports_data$Year[1], frequency = 1)\nimports_ts &lt;- ts(imports_data$Value, start = imports_data$Year[1], frequency = 1)\ntotal_trade_ts &lt;- ts(total_trade_data$Value, start = total_trade_data$Year[1], frequency = 1)\n\n# Create Data Frame for Plotting\ndf &lt;- data.frame(\n  Year = rep(exports_data$Year, 3),\n  Value = c(exports_data$Value, imports_data$Value, total_trade_data$Value),\n  Category = rep(c(\"Exports\", \"Imports\", \"Total Trade\"), each = nrow(exports_data))\n)\n\n# Create a ggplot with explicit group aesthetic\np &lt;- ggplot(df, aes(x = Year, y = Value, color = Category, group = Category, \n                     text = paste0(\"Year: \", Year, \"&lt;br&gt;Value: \", round(Value, 1)))) +\n  geom_line(linewidth = 1.2) +  # Use linewidth instead of size (ggplot2 3.4.0+)\n  scale_color_manual(values = c(\"Exports\" = \"red\", \"Imports\" = \"blue\", \"Total Trade\" = \"black\")) +\n  ggtitle(\"Export vs Import vs Total Trade Trends\") +\n  ylab(\"Value (in Millions)\") + xlab(\"Year\") +\n  theme_minimal()\n\n# Convert to interactive plot with tooltips\ninteractive_plot &lt;- ggplotly(p, tooltip = \"text\")\n\n# Display interactive plot\ninteractive_plot\n\n\n\n\n\n\n\n\nNext, I applied ARIMA modeling (auto.arima()) using the forecast package to predict trade values for the next five years (2025-2029).\n\n\nCode\nlibrary(forecast)\n\n# Apply ARIMA Model\narima_exports &lt;- auto.arima(exports_ts)\narima_imports &lt;- auto.arima(imports_ts)\narima_total_trade &lt;- auto.arima(total_trade_ts)\n\n# Forecast for the next 5 years\narima_forecast_exports &lt;- forecast(arima_exports, h = 5)\narima_forecast_imports &lt;- forecast(arima_imports, h = 5)\narima_forecast_total_trade &lt;- forecast(arima_total_trade, h = 5)\n\n# Plot ARIMA forecasts\nautoplot(arima_forecast_exports) + ggtitle(\"ARIMA Forecast for Exports\")\n\n\n\n\n\n\n\n\n\nCode\nautoplot(arima_forecast_imports) + ggtitle(\"ARIMA Forecast for Imports\")\n\n\n\n\n\n\n\n\n\nCode\nautoplot(arima_forecast_total_trade) + ggtitle(\"ARIMA Forecast for Total Trade\")\n\n\n\n\n\n\n\n\n\nCode\n# Convert forecasts to data frames for ggplot\ndf_exports &lt;- as.data.frame(arima_forecast_exports) %&gt;% mutate(Year = seq(max(exports_data$Year) + 1, by = 1, length.out = 5), Category = \"Exports\")\ndf_imports &lt;- as.data.frame(arima_forecast_imports) %&gt;% mutate(Year = seq(max(imports_data$Year) + 1, by = 1, length.out = 5), Category = \"Imports\")\ndf_total_trade &lt;- as.data.frame(arima_forecast_total_trade) %&gt;% mutate(Year = seq(max(total_trade_data$Year) + 1, by = 1, length.out = 5), Category = \"Total Trade\")\n\n# Combine all forecasts\ndf_forecast &lt;- bind_rows(df_exports, df_imports, df_total_trade)\n\n# Create the forecast plot\nggplot(df_forecast, aes(x = Year, y = `Point Forecast`, color = Category)) +\n  geom_line(linewidth = 1.5) +  # Use linewidth instead of size (ggplot2 3.4.0+)\n  scale_color_manual(values = c(\"Exports\" = \"red\", \"Imports\" = \"blue\", \"Total Trade\" = \"black\")) +\n  ggtitle(\"ARIMA Forecast for Exports, Imports, and Total Trade\") +\n  ylab(\"Forecasted Value (in Millions)\") + xlab(\"Year\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# Evaluate forecast accuracy\naccuracy(arima_forecast_exports)\n\n\n                   ME     RMSE      MAE      MPE     MAPE      MASE        ACF1\nTraining set 5842.817 21775.54 13733.86 2.777663 6.099909 0.6402954 -0.09397345\n\n\nCode\naccuracy(arima_forecast_imports)\n\n\n                   ME     RMSE      MAE      MPE     MAPE      MASE       ACF1\nTraining set 3887.432 15372.93 10045.78 1.659329 4.840996 0.5509206 0.03707208\n\n\nCode\naccuracy(arima_forecast_total_trade)\n\n\n                   ME     RMSE      MAE      MPE     MAPE      MASE        ACF1\nTraining set 9537.844 35750.43 23074.03 2.170273 5.229894 0.5884102 -0.02527777"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02.html#datavis-makeover-2",
    "href": "Take-home_Ex/Take-home_Ex02.html#datavis-makeover-2",
    "title": "Take-home Exercise 02",
    "section": "",
    "text": "The original visualizations analyzed in this analysis are sourced from here.\n\n\n\n\n\nIn the below section, we will evaluate the effectiveness of the above visualization by identifying its pros and cons, focusing on aspects such as (1) clarity, and (2) visual appeal:\n\nClarity: How well the data is presented and understood\n\n\n\n\n\n\n\n\n\nPros\nCons\nSuggested fixes\n\n\n\n\nClearly labeled service categories with values and percentages – Each category is distinctly labeled, making it easy to understand the contributions of each service type.\nPie charts are not effective for comparing data slices – When multiple categories have close values, it is difficult to differentiate them accurately.\nReplace pie charts with bar charts or grouped bar charts - for clearer comparison of service categories.\n\n\nPercentage values enhance clarity – Helps users quickly understand each category’s contribution to total exports/ imports.\nPie charts are only useful when there are limited categories – Too many slices make it difficult to interpret, as smaller sections become unreadable.\nReplace with a bar chart or grouped bar chart to improve readability.\n\n\n\nThe pie chart format makes trend comparison difficult – Pie charts only represent a single year and do not show how exports have changed over time.\nAdd a trend chart (e.g., line graph or bar chart) – Including historical data can help identify patterns and trends over multiple years.\n\n\n\n\nVisual appeal: How visually engaging and effective the design is\n\n\n\n\n\n\n\n\n\nPros\nCons\nSuggested fixes\n\n\n\n\nGood use of icons to represent different services – This makes the chart more engaging and helps with intuitive understanding.\nPie charts distort perception – it’s hard to accurately compare slice sizes, especially when they are similar.\nUse a bar chart, grouped bar chart or treemaps, which allows for more proportional representation of each service category.\n\n\nWell-structured layout with categories spread around the chart - The service categories for both exports and imports maintain a similar placement\nPie chart format is too cluttered – The large number of slices makes it difficult to read smaller segments and their labels.\nUse a bar chart, grouped bar chart or treemaps for better spacing and spatial representation.\n\n\n\nToo many elements are packed into the chart, making it overwhelming.\nSimplify by removing non-essential elements - i.e: icons\n\n\n\nNo legend for color segmentation – The chart uses various colors for different service categories, but there is no clear legend explaining their grouping or significance.\nInclude a legend or categorize colors meaningfully – Assign gradient based color scale to visually emphasize trade volume intensity.\n\n\n\n\n\n\n\n\n\nLimitations of pie charts in data visualization\n\n\n\n\nDistorted perception: Pie chart makes comparisons difficult due to the reliance on angles and areas rather than a common baseline.\nDifficult to compare similar data slices: Can be misleading when there are many segments/ similar-sized portions making it hard to interpret differences accurately.\nSpace constraints: Pie chart can take up more space than necessary and can clutter dashboards or reports.\nPoor for trend analysis: Pie charts only show a single point in time and do not help in comparing trends over multiple years.\n\n\n\n\n\n\n\n\n\nWhy are the use of pie charts be frown upon in data visualization?\n\n\n\n\nRefer to this page to find out why the use of pie charts are discouraged.\n\n\n\n\n\n\n\n\n\nAfter importing the trade_services data set, we will filter for the key categories that contribute to the exports/ imports of services in Singapore, excluding subcategories. The 12 major categories are identified based on indentation, as shown below:\n\n\n\n\n\n\n\nData Series\nShortened label\n\n\n\n\nManufacturing Services On Physical Inputs Owned By Others\nManuf. Services\n\n\nMaintenance And Repair Services\nMaintenance & Repair\n\n\nTransport\nTransport\n\n\nTravel\nTravel\n\n\nInsurance\nInsurance\n\n\nGovernment Goods And Services\nGovt. Services\n\n\nConstruction\nConstruction\n\n\nFinancial\nFinancial Services\n\n\nTelecommunications, Computer & Information\nTelecom & IT\n\n\nCharges For The Use Of Intellectual Property\nIntellectual Property\n\n\nPersonal, Cultural And Recreational\nCultural & Recreational\n\n\nOther Business Services\nOther Biz Services\n\n\n\nWe first clean the “Data Series” column by removing leading and trailing spaces to ensure consistency. Next, we identify the positions of “Exports Of Services” and “Imports Of Services” in the dataset, extracting only the rows between these markers for exports and those following the imports marker for imports. We then filter both datasets to retain only the 12 major service categories as hghlighted above.\nFinally, we combine the cleaned exports and imports data into a single structured table, adding a “Trade Type” column to differentiate between imports/ exports.\n\n\nCode\n# Trim leading/trailing spaces from 'Data Series' column\ntrade_services$`Data Series` &lt;- trimws(trade_services$`Data Series`)\n\n# Identify row indices for \"Exports Of Services\" and \"Imports Of Services\"\nexport_start_idx &lt;- which(trade_services$`Data Series` == \"Exports Of Services\")\nimport_start_idx &lt;- which(trade_services$`Data Series` == \"Imports Of Services\")\n\n# Extract rows between \"Exports Of Services\" and \"Imports Of Services\"\nexports_df &lt;- trade_services[(export_start_idx + 1):(import_start_idx - 1), ]\nimports_df &lt;- trade_services[(import_start_idx + 1):nrow(trade_services), ]\n\n# Define the 12 major categories\nmajor_categories &lt;- c(\n  \"Manufacturing Services On Physical Inputs Owned By Others\",\n  \"Maintenance And Repair Services\",\n  \"Transport\",\n  \"Travel\",\n  \"Insurance\",\n  \"Government Goods And Services\",\n  \"Construction\",\n  \"Financial\",\n  \"Telecommunications, Computer & Information\",\n  \"Charges For The Use Of Intellectual Property\",\n  \"Personal, Cultural And Recreational\",\n  \"Other Business Services\"\n)\n\n# Filter only the major categories for exports and imports\nexports_major &lt;- exports_df %&gt;% filter(`Data Series` %in% major_categories)\nimports_major &lt;- imports_df %&gt;% filter(`Data Series` %in% major_categories)\n\n# Add a column to indicate whether it's exports or imports\nexports_major$`Trade Type` &lt;- \"Exports\"\nimports_major$`Trade Type` &lt;- \"Imports\"\n\n# Combine into a single dataframe\nfinal_trade_df &lt;- bind_rows(exports_major, imports_major)\n\n# Reorder columns: Move \"Trade Type\" to the second column\nfinal_trade_df &lt;- final_trade_df %&gt;% select(`Data Series`, `Trade Type`, everything())\n\n# View the final structured data\nprint(final_trade_df)\n\n\n# A tibble: 24 × 27\n   `Data Series`   `Trade Type` `2024` `2023` `2022` `2021` `2020` `2019` `2018`\n   &lt;chr&gt;           &lt;chr&gt;         &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 Manufacturing … Exports      4.71e2 4.91e2 6.85e2 4.90e2   237.   267.   370.\n 2 Maintenance An… Exports      1.08e4 1.10e4 1.01e4 8.83e3  8173.  9663.  9410 \n 3 Transport       Exports      1.73e5 1.50e5 1.90e5 1.44e5 93560  94272. 88889.\n 4 Travel          Exports      3.19e4 2.79e4 1.57e4 5.40e3  7527. 27755. 27367.\n 5 Insurance       Exports      1.26e4 1.11e4 1.15e4 9.13e3  8438   8817.  9485.\n 6 Government Goo… Exports      4.77e2 4.64e2 4.44e2 4.22e2   412.   419.   414.\n 7 Construction    Exports      2.10e3 2.04e3 1.62e3 1.20e3  1272.  1742.  1642.\n 8 Financial       Exports      7.16e4 6.53e4 5.61e4 5.31e4 49688. 45944. 42840.\n 9 Telecommunicat… Exports      4.11e4 3.93e4 3.42e4 3.19e4 26826. 20252. 20533.\n10 Charges For Th… Exports      2.63e4 2.42e4 1.79e4 1.64e4 12866. 12297. 12084.\n# ℹ 14 more rows\n# ℹ 18 more variables: `2017` &lt;dbl&gt;, `2016` &lt;dbl&gt;, `2015` &lt;dbl&gt;, `2014` &lt;dbl&gt;,\n#   `2013` &lt;dbl&gt;, `2012` &lt;dbl&gt;, `2011` &lt;dbl&gt;, `2010` &lt;dbl&gt;, `2009` &lt;dbl&gt;,\n#   `2008` &lt;dbl&gt;, `2007` &lt;dbl&gt;, `2006` &lt;dbl&gt;, `2005` &lt;dbl&gt;, `2004` &lt;dbl&gt;,\n#   `2003` &lt;dbl&gt;, `2002` &lt;dbl&gt;, `2001` &lt;dbl&gt;, `2000` &lt;dbl&gt;\n\n\n\n\n\nKey makeover changes:\n1️⃣ Single chart for imports and exports:\n\nCombined visualization –&gt; Instead of two separate pie charts, the new bar chart combines both, allowing for side-by-side comparison.\n\n2️⃣ Improved readability and comparability:\n\nGrouped bars (red for exports, blue for imports) –&gt; Offer a clearer visual distinction between trade types.\nSorted categories (largest to smallest) –&gt; Focus attention on key contributors, ensuring the most impactful sectors are easily identified.\n\n3️⃣ Simplified layout:\n\nRemoved unnecessary graphics (i.e.: icons, maps) –&gt;Decluttered the chart to emphasize trade data information.\n\n4️⃣ Interactive insights:\n\nHover tooltips –&gt; Display category names, trade values, and percentages shares, making the chart more dynamic and user-friendly.\n\n\n\n\n\n\n\nOverall\n\n\n\n\nBetter clarity, accuracy, and direct comparison in a single view.\nEliminates redundancy and improves clarity for faster data interpretation.\n\n\n\n\nEnhance graph()Code()\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Load required libraries\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(plotly)  # For interactive tooltips\n\n# Filter data for the year 2024\ntrade_2024 &lt;- final_trade_df %&gt;%\n  select(`Data Series`, `Trade Type`, `2024`) %&gt;%\n  rename(Value = `2024`)\n\n# Shorten long category labels for better readability\ntrade_2024$`Data Series` &lt;- gsub(\"Manufacturing Services On Physical Inputs Owned By Others\", \"Manuf. Services\", trade_2024$`Data Series`)\ntrade_2024$`Data Series` &lt;- gsub(\"Maintenance And Repair Services\", \"Maintenance & Repair\", trade_2024$`Data Series`)\ntrade_2024$`Data Series` &lt;- gsub(\"Government Goods And Services\", \"Govt. Services\", trade_2024$`Data Series`)\ntrade_2024$`Data Series` &lt;- gsub(\"Financial\", \"Financial Services\", trade_2024$`Data Series`)\ntrade_2024$`Data Series` &lt;- gsub(\"Telecommunications, Computer & Information\", \"Telecom & IT\", trade_2024$`Data Series`)\ntrade_2024$`Data Series` &lt;- gsub(\"Charges For The Use Of Intellectual Property\", \"Intellectual Property\", trade_2024$`Data Series`)\ntrade_2024$`Data Series` &lt;- gsub(\"Personal, Cultural And Recreational\", \"Cultural & Recreational\", trade_2024$`Data Series`)\ntrade_2024$`Data Series` &lt;- gsub(\"Other Business Services\", \"Other Biz Services\", trade_2024$`Data Series`)\n\n# Compute Total Exports & Imports for Percentage Calculation\ntotal_exports &lt;- sum(trade_2024$Value[trade_2024$`Trade Type` == \"Exports\"], na.rm = TRUE)\ntotal_imports &lt;- sum(trade_2024$Value[trade_2024$`Trade Type` == \"Imports\"], na.rm = TRUE)\n\n# Add Percentage Column\ntrade_2024 &lt;- trade_2024 %&gt;%\n  mutate(Percentage = ifelse(`Trade Type` == \"Exports\", \n                             Value / total_exports * 100, \n                             Value / total_imports * 100))\n\n# Sum values for sorting (descending order)\ncategory_order &lt;- trade_2024 %&gt;%\n  group_by(`Data Series`) %&gt;%\n  summarise(Total_Trade = sum(Value, na.rm = TRUE)) %&gt;%\n  arrange(desc(Total_Trade)) %&gt;%\n  pull(`Data Series`)\n\n# Convert `Data Series` to factor for correct sorting\ntrade_2024$`Data Series` &lt;- factor(trade_2024$`Data Series`, levels = category_order)\n\n# Define custom colors for Exports (Red) and Imports (Blue)\ncustom_colors &lt;- c(\"Exports\" = \"red\", \"Imports\" = \"blue\")\n\n# Create the ggplot object\ngg &lt;- ggplot(trade_2024, aes(x = `Data Series`, y = Value, fill = `Trade Type`,\n                             text = paste(\"Category:\", `Data Series`, \n                                          \"&lt;br&gt;Trade Type:\", `Trade Type`, \n                                          \"&lt;br&gt;Value: S$\", scales::comma(Value),\n                                          \"&lt;br&gt;Share:\", round(Percentage, 1), \"%\"))) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.8)) +  # Side-by-side bars\n  scale_fill_manual(values = custom_colors) +  # Apply custom colors\n  scale_y_continuous(labels = scales::comma) +  # Format y-axis labels\n  labs(title = \"Singapore Trade in Services - 2024\",\n       subtitle = \"Side-by-Side Comparison of Imports and Exports for 12 Service Categories\",\n       x = \"Service Category\",\n       y = \"Trade Value\",\n       fill = \"Trade Type\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),  # Rotate x-axis labels\n        legend.position = \"bottom\")  \n\n# Convert to interactive plot with tooltips\nggplotly(gg, tooltip = \"text\")\n\n\n\n\n\n\n\n\nKey makeover changes:\n1️⃣ Transition from Pie charts to Treemaps\n\nPie charts are replaced with treemaps –&gt; Treemaps uses area size to provide a better spatial comparison instead of circular slices in pie charts\n\n2️⃣ Added plotly inteactive features:\n\nEnabled interactive treempas using plotly –&gt; Allows for hovering over categories to explore trade value more dynamically.\n\n3️⃣ Gradient-based color scale for trade value:\n\nApplied green gradient (darker green = higher value, lighter green = lower value) –&gt; Visually emphasizes trade volume intensity instead of random colors in pie charts.\n\n\nEnhance graph()Code()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Load required libraries\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(treemap)\nlibrary(plotly)\n\n# Filter data for the year 2024\ntrade_2024 &lt;- final_trade_df %&gt;%\n  select(`Data Series`, `Trade Type`, `2024`) %&gt;%\n  rename(Value = `2024`)\n\n# Shorten long category labels for better readability\ntrade_2024 &lt;- trade_2024 %&gt;%\n  mutate(Short_Label = case_when(\n    `Data Series` == \"Manufacturing Services On Physical Inputs Owned By Others\" ~ \"Manuf. Services\",\n    `Data Series` == \"Maintenance And Repair Services\" ~ \"Maintenance & Repair\",\n    `Data Series` == \"Government Goods And Services\" ~ \"Govt. Services\",\n    `Data Series` == \"Financial\" ~ \"Financial Services\",\n    `Data Series` == \"Telecommunications, Computer & Information\" ~ \"Telecom & IT\",\n    `Data Series` == \"Charges For The Use Of Intellectual Property\" ~ \"Intellectual Property\",\n    `Data Series` == \"Personal, Cultural And Recreational\" ~ \"Cultural & Recreational\",\n    `Data Series` == \"Other Business Services\" ~ \"Other Biz Services\",\n    TRUE ~ `Data Series`\n  ))\n\n# Split the dataset into Exports and Imports\ntrade_exports &lt;- trade_2024 %&gt;% filter(`Trade Type` == \"Exports\")\ntrade_imports &lt;- trade_2024 %&gt;% filter(`Trade Type` == \"Imports\")\n\n# Create Treemap Data for Exports\ntreemap_exports &lt;- treemap(trade_exports,\n                           index = \"Short_Label\",\n                           vSize = \"Value\",\n                           type = \"index\",\n                           palette = \"Blues\",\n                           title = \"Exports Treemap - 2024\",\n                           draw = FALSE)\n\n# Create Treemap Data for Imports\ntreemap_imports &lt;- treemap(trade_imports,\n                           index = \"Short_Label\",\n                           vSize = \"Value\",\n                           type = \"index\",\n                           palette = \"Oranges\",\n                           title = \"Imports Treemap - 2024\",\n                           draw = FALSE)\n\n# Convert Exports Treemap Data to Plotly (Darker Green for Higher Values)\np1_interactive &lt;- plot_ly(\n  data = treemap_exports$tm,\n  labels = ~Short_Label,\n  parents = \"\",\n  values = ~vSize,\n  text = ~paste(\"Category:\", Short_Label, \"&lt;br&gt;Value:\", vSize),\n  type = \"treemap\",\n  textinfo = \"label+text\",\n  marker = list(\n    colorscale = list(c(0, 1), c(\"#d9f2d9\", \"#006400\")),  # Light to Dark Green\n    cmin = min(treemap_exports$tm$vSize),\n    cmax = max(treemap_exports$tm$vSize),\n    colorbar = list(title = \"Trade Value\")\n  )\n) %&gt;%\n  layout(title = \"Exports Treemap - 2024\")\n\n# Convert Imports Treemap Data to Plotly (Darker Green for Higher Values)\np2_interactive &lt;- plot_ly(\n  data = treemap_imports$tm,\n  labels = ~Short_Label,\n  parents = \"\",\n  values = ~vSize,\n  text = ~paste(\"Category:\", Short_Label, \"&lt;br&gt;Value:\", vSize),\n  type = \"treemap\",\n  textinfo = \"label+text\",\n  marker = list(\n    colorscale = list(c(0, 1), c(\"#d9f2d9\", \"#006400\")),  # Light to Dark Green\n    cmin = min(treemap_imports$tm$vSize),\n    cmax = max(treemap_imports$tm$vSize),\n    colorbar = list(title = \"Trade Value\")\n  )\n) %&gt;%\n  layout(title = \"Imports Treemap - 2024\")\n\np1_interactive\np2_interactive\n\n\n\n\n\n\n\n\n\n\n\nKey observations:\n\nRapid Growth: Singapore’s service exports have grown significantly, reinforcing its role as a global trade hub.\nLeading Sectors: Transport and Other Business Services dominate, showing the steepest growth.\nFinancial & Digital Expansion: Financial Services, Telecom & IT, and Intellectual Property have surged, driven by digitalization and economic policies.\nResilience & Acceleration: Despite slower growth around 2012 (likely due to economic shocks), post-2018 expansion highlights policy-driven trade growth.\n\n\n\n\n\n\n\nNote\n\n\n\n\nSingapore’s services exports continue to thrive, reflecting its strong economic positioning\n\n\n\n\nTime series()Code()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n#| fig-width: 8\n#| fig-height: 6\n\n# Load required libraries\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(CGPfunctions)\n\n# Ensure column names are trimmed properly\ncolnames(final_trade_df) &lt;- trimws(colnames(final_trade_df))\n\n# Shorten long category labels for better readability (Fixed repetitive label issue)\nfinal_trade_df$`Data Series` &lt;- gsub(\"Manufacturing Services On Physical Inputs Owned By Others\", \"Manuf. Services\", final_trade_df$`Data Series`)\nfinal_trade_df$`Data Series` &lt;- gsub(\"Maintenance And Repair Services\", \"Maintenance & Repair\", final_trade_df$`Data Series`)\nfinal_trade_df$`Data Series` &lt;- gsub(\"Government Goods And Services\", \"Govt. Services\", final_trade_df$`Data Series`)\nfinal_trade_df$`Data Series` &lt;- gsub(\"^Financial$\", \"Financial Services\", final_trade_df$`Data Series`)  \nfinal_trade_df$`Data Series` &lt;- gsub(\"Telecommunications, Computer & Information\", \"Telecom & IT\", final_trade_df$`Data Series`)\nfinal_trade_df$`Data Series` &lt;- gsub(\"Charges For The Use Of Intellectual Property\", \"Intellectual Property\", final_trade_df$`Data Series`)\nfinal_trade_df$`Data Series` &lt;- gsub(\"Personal, Cultural And Recreational\", \"Cultural & Recreational\", final_trade_df$`Data Series`)\nfinal_trade_df$`Data Series` &lt;- gsub(\"Other Business Services\", \"Other Biz Services\", final_trade_df$`Data Series`)\n\n# Filter for Exports only\nexports_df &lt;- final_trade_df %&gt;% filter(`Trade Type` == \"Exports\")\n\n# Select relevant columns: Data Series, 2000, 2012, 2024\nexports_slope &lt;- exports_df %&gt;%\n  select(`Data Series`, `2000`, `2012`, `2024`) %&gt;%\n  drop_na()\n\n# Keep only the top 10 services in 2024 for clarity\nexports_slope &lt;- exports_slope %&gt;% arrange(desc(`2024`)) %&gt;% head(10)\n\n# Convert data to long format for `newggslopegraph()`\nexports_long &lt;- exports_slope %&gt;%\n  pivot_longer(cols = c(`2000`, `2012`, `2024`), names_to = \"Year\", values_to = \"Value\")\n\n# Convert Year to character (Fix for newggslopegraph)\nexports_long$Year &lt;- as.character(exports_long$Year)\n\n# Increase figure height to improve label spacing\noptions(repr.plot.width = 14, repr.plot.height = 12)\n\n# Create slopegraph with **2000, 2012, and 2024**\nnewggslopegraph(dataframe = exports_long,\n                Times = Year,   \n                Measurement = Value,\n                Grouping = `Data Series`,\n                Title = \"Trade in Services (Exports) - Slopegraph (2000-2012-2024)\",\n                SubTitle = \"Top 10 Services by Export Value\",\n                LineThickness = 1.0,\n                DataTextSize = 2.5)\n\n\n\n\n\n\n\n\nKey observations:\n\nStrong Growth in Imports: Similar to exports, Singapore’s service imports have grown significantly, reflecting its global trade connectivity.\nTransport & Other Business Services Lead: These two sectors dominate imports, mirroring export trends, but at slightly lower values.\nRising Financial Services & Telecom & IT: These sectors have seen notable import growth, indicating increasing reliance on foreign expertise and digital services.\nHigher Import Dependency in Intellectual Property: Compared to exports, imports in Intellectual Property have increased faster, suggesting rising licensing and royalty payments.\n\n\n\n\n\n\n\nNote\n\n\n\n\nSingapore’s service imports align with its export-driven economy, with strong growth in digital services, finance, and transport.\n\n\n\n\nTime series()Code()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n#| fig-width: 8\n#| fig-height: 6\n\n# Load required libraries\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(CGPfunctions)\n\n# Ensure column names are trimmed properly\ncolnames(final_trade_df) &lt;- trimws(colnames(final_trade_df))\n\n# Shorten long category labels for better readability (Fixed repetitive label issue)\nfinal_trade_df$`Data Series` &lt;- gsub(\"Manufacturing Services On Physical Inputs Owned By Others\", \"Manuf. Services\", final_trade_df$`Data Series`)\nfinal_trade_df$`Data Series` &lt;- gsub(\"Maintenance And Repair Services\", \"Maintenance & Repair\", final_trade_df$`Data Series`)\nfinal_trade_df$`Data Series` &lt;- gsub(\"Government Goods And Services\", \"Govt. Services\", final_trade_df$`Data Series`)\nfinal_trade_df$`Data Series` &lt;- gsub(\"^Financial$\", \"Financial Services\", final_trade_df$`Data Series`)  \nfinal_trade_df$`Data Series` &lt;- gsub(\"Telecommunications, Computer & Information\", \"Telecom & IT\", final_trade_df$`Data Series`)\nfinal_trade_df$`Data Series` &lt;- gsub(\"Charges For The Use Of Intellectual Property\", \"Intellectual Property\", final_trade_df$`Data Series`)\nfinal_trade_df$`Data Series` &lt;- gsub(\"Personal, Cultural And Recreational\", \"Cultural & Recreational\", final_trade_df$`Data Series`)\nfinal_trade_df$`Data Series` &lt;- gsub(\"Other Business Services\", \"Other Biz Services\", final_trade_df$`Data Series`)\n\n# Filter for Imports only\nimports_df &lt;- final_trade_df %&gt;% filter(`Trade Type` == \"Imports\")\n\n# Select relevant columns: Data Series, 2000, 2012, 2024\nimports_slope &lt;- imports_df %&gt;%\n  select(`Data Series`, `2000`, `2012`, `2024`) %&gt;%\n  drop_na()\n\n# Keep only the top 10 services in 2024 for clarity\nimports_slope &lt;- imports_slope %&gt;% arrange(desc(`2024`)) %&gt;% head(10)\n\n# Convert data to long format for `newggslopegraph()`\nimports_long &lt;- imports_slope %&gt;%\n  pivot_longer(cols = c(`2000`, `2012`, `2024`), names_to = \"Year\", values_to = \"Value\")\n\n# Convert Year to character (Fix for newggslopegraph)\nimports_long$Year &lt;- as.character(imports_long$Year)\n\n# Increase figure height to improve label spacing\noptions(repr.plot.width = 14, repr.plot.height = 12)\n\n# Create slopegraph with **2000, 2012, and 2024**\nnewggslopegraph(dataframe = imports_long,\n                Times = Year,   \n                Measurement = Value,\n                Grouping = `Data Series`,\n                Title = \"Trade in Services (Imports) - Slopegraph (2000-2012-2024)\",\n                SubTitle = \"Top 10 Services by Import Value\",\n                LineThickness = 1.0,\n                DataTextSize = 2.5)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02.html#limitations-of-pie-charts-in-data-visualization",
    "href": "Take-home_Ex/Take-home_Ex02.html#limitations-of-pie-charts-in-data-visualization",
    "title": "Take-home Exercise 02",
    "section": "",
    "text": "Distored perception: Pie chart makes comparisons difficult due to the reliance on angles and areas rather than a common baseline.\nDifficult to compare similar data slices: Can be misleading when there are many segments/ similar-sized portions making it hard to interpret differences accurately.\nSpace constraints: Pie chart can take up more space than necessary and can clutter dashboards or reports.\nPoor for trend analysis: Pie charts only show a single point in time and do not help in comparing trends over multiple years."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02.html#why-are-the-use-of-pie-charts-be-frown-upon-in-data-visualization",
    "href": "Take-home_Ex/Take-home_Ex02.html#why-are-the-use-of-pie-charts-be-frown-upon-in-data-visualization",
    "title": "Take-home Exercise 02",
    "section": "",
    "text": "Refer to this page to find out why the use of pie charts are discouraged."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01.html#why-are-the-use-of-pie-charts-be-frown-upon-in-data-visualization",
    "href": "Take-home_Ex/Take-home_Ex01.html#why-are-the-use-of-pie-charts-be-frown-upon-in-data-visualization",
    "title": "Take-home Exercise 01",
    "section": "",
    "text": "Refer to this page to find out why the use of pie charts are discouraged."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html",
    "href": "In-class_Ex/In-class_Ex07.html",
    "title": "In-class Exercise 07",
    "section": "",
    "text": "For the purpose of this hands-on exercise, the following R packages will be used\n\n\nCode\npacman::p_load(tidyverse, tsibble, feasts, fable, seasonal)\n\n\n\nlubridate provides a collection to functions to parse and wrangle time and date data.\ntsibble, feasts, fable and fable.prophet are belong to tidyverts, a family of tidy tools for time series data handling, analysis and forecasting.\n\ntsibble provides a data infrastructure for tidy temporal data with wrangling tools. Adapting the tidy data principles, tsibble is a data- and model-oriented object.\nfeasts provides a collection of tools for the analysis of time series data. The package name is an acronym comprising of its key features: Feature Extraction And Statistics for Time Series."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#getting-started",
    "href": "In-class_Ex/In-class_Ex07.html#getting-started",
    "title": "In-class Exercise 07",
    "section": "",
    "text": "For the purpose of this hands-on exercise, the following R packages will be used\n\n\nCode\npacman::p_load(tidyverse, tsibble, feasts, fable, seasonal)\n\n\n\nlubridate provides a collection to functions to parse and wrangle time and date data.\ntsibble, feasts, fable and fable.prophet are belong to tidyverts, a family of tidy tools for time series data handling, analysis and forecasting.\n\ntsibble provides a data infrastructure for tidy temporal data with wrangling tools. Adapting the tidy data principles, tsibble is a data- and model-oriented object.\nfeasts provides a collection of tools for the analysis of time series data. The package name is an acronym comprising of its key features: Feature Extraction And Statistics for Time Series."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#importing-the-data",
    "href": "In-class_Ex/In-class_Ex07.html#importing-the-data",
    "title": "In-class Exercise 07",
    "section": "Importing the data",
    "text": "Importing the data\nWe use the read_csv() of readr package to import visitor_arrivals_by_air.csv file into R environment\n\n\nCode\nts_data &lt;- read_csv(\"data/visitor_arrivals_by_air.csv\")\n\n\nIn the code chunk below, dmy() of lubridate package is used to convert data type of Month-Year field from Character to Date.\n\n\nCode\nts_data$`Month-Year` &lt;- dmy(\n  ts_data$`Month-Year`)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#conventional-base-ts-object-versus-tibble-object",
    "href": "In-class_Ex/In-class_Ex07.html#conventional-base-ts-object-versus-tibble-object",
    "title": "In-class Exercise 07",
    "section": "Conventional base ts object versus tibble object",
    "text": "Conventional base ts object versus tibble object\ntibble object\n\n\nCode\nts_data\n\n\n# A tibble: 144 × 34\n   `Month-Year` `Republic of South Africa` Canada   USA Bangladesh Brunei China\n   &lt;date&gt;                            &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 2008-01-01                         3680   6972 31155       6786   3729 79599\n 2 2008-02-01                         1662   6056 27738       6314   3070 82074\n 3 2008-03-01                         3394   6220 31349       7502   4805 72546\n 4 2008-04-01                         3337   4764 26376       7333   3096 76112\n 5 2008-05-01                         2089   4460 26788       7988   3586 64808\n 6 2008-06-01                         2515   3888 29725       8301   5284 55238\n 7 2008-07-01                         2919   5313 33183       9004   4070 80747\n 8 2008-08-01                         2471   4519 27427       7913   4183 66625\n 9 2008-09-01                         2492   3421 21588       7549   3160 52649\n10 2008-10-01                         3023   4756 25112       7527   2983 54423\n# ℹ 134 more rows\n# ℹ 27 more variables: `Hong Kong SAR (China)` &lt;dbl&gt;, India &lt;dbl&gt;,\n#   Indonesia &lt;dbl&gt;, Japan &lt;dbl&gt;, `South Korea` &lt;dbl&gt;, Kuwait &lt;dbl&gt;,\n#   Malaysia &lt;dbl&gt;, Myanmar &lt;dbl&gt;, Pakistan &lt;dbl&gt;, Philippines &lt;dbl&gt;,\n#   `Saudi Arabia` &lt;dbl&gt;, `Sri Lanka` &lt;dbl&gt;, Taiwan &lt;dbl&gt;, Thailand &lt;dbl&gt;,\n#   `United Arab Emirates` &lt;dbl&gt;, Vietnam &lt;dbl&gt;, `Belgium & Luxembourg` &lt;dbl&gt;,\n#   Finland &lt;dbl&gt;, France &lt;dbl&gt;, Germany &lt;dbl&gt;, Italy &lt;dbl&gt;, …"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#conventional-base-ts-object-versus-tibble-object-1",
    "href": "In-class_Ex/In-class_Ex07.html#conventional-base-ts-object-versus-tibble-object-1",
    "title": "In-class Exercise 07",
    "section": "Conventional base ts object versus tibble object",
    "text": "Conventional base ts object versus tibble object\nts object - converting it into a tibble object\n\n\nCode\nts_data_ts &lt;- ts(ts_data)       \nhead(ts_data_ts)\n\n\n     Month-Year Republic of South Africa Canada   USA Bangladesh Brunei China\n[1,]      13879                     3680   6972 31155       6786   3729 79599\n[2,]      13910                     1662   6056 27738       6314   3070 82074\n[3,]      13939                     3394   6220 31349       7502   4805 72546\n[4,]      13970                     3337   4764 26376       7333   3096 76112\n[5,]      14000                     2089   4460 26788       7988   3586 64808\n[6,]      14031                     2515   3888 29725       8301   5284 55238\n     Hong Kong SAR (China) India Indonesia Japan South Korea Kuwait Malaysia\n[1,]                 17103 41639     62683 37673       27937    284    31352\n[2,]                 21089 37170     47834 35297       22633    241    35030\n[3,]                 23230 44815     64688 42575       22876    206    37629\n[4,]                 17688 49527     58074 26839       20634    193    37521\n[5,]                 19340 67754     57089 30814       22785    140    38044\n[6,]                 19152 57380     70118 31001       22575    354    40419\n     Myanmar Pakistan Philippines Saudi Arabia Sri Lanka Taiwan Thailand\n[1,]    5269     1395       18622          406      5289  13757    18370\n[2,]    4643     1027       21609          591      4767  13921    16400\n[3,]    6218     1635       28464          626      4988  11181    23387\n[4,]    7324     1232       30131          644      7639  11665    24469\n[5,]    5395     1306       30193          470      5125  11436    21935\n[6,]    5542     1996       25800          772      4791  10689    19900\n     United Arab Emirates Vietnam Belgium & Luxembourg Finland France Germany\n[1,]                 2652   10315                 1341    1179   6918   11982\n[2,]                 2230   13415                 1449    1207   7876   13256\n[3,]                 3353   14320                 1674    1071   8066   15185\n[4,]                 3245   15413                 1426     768   8312   11604\n[5,]                 2856   14424                 1243     690   7066    9853\n[6,]                 4292   21368                 1255     624   5926    9347\n     Italy Netherlands Spain Switzerland United Kingdom Australia New Zealand\n[1,]  2953        4938  1668        4450          41934     71260        7806\n[2,]  2704        4885  1568        4381          44029     45595        4729\n[3,]  2822        5015  2254        5015          49489     53191        6106\n[4,]  3018        4902  1503        5434          35771     56514        7560\n[5,]  2165        4397  1365        4427          24464     57808        9090\n[6,]  2022        4166  1446        3359          22473     63350        9681"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#converting-tibble-object-to-tsibble-object",
    "href": "In-class_Ex/In-class_Ex07.html#converting-tibble-object-to-tsibble-object",
    "title": "In-class Exercise 07",
    "section": "Converting tibble object to tsibble object",
    "text": "Converting tibble object to tsibble object\nThe following code snippet converts ts_data from a tibble to a tsibble using the as_tsibble() function from the tsibble package in R.\n\n\nCode\nts_tsibble &lt;- ts_data %&gt;%\n  mutate(Month = yearmonth(`Month-Year`)) %&gt;%\n  as_tsibble(index = `Month`)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#visualising-time-series-data",
    "href": "In-class_Ex/In-class_Ex07.html#visualising-time-series-data",
    "title": "In-class Exercise 07",
    "section": "Visualising Time-series Data",
    "text": "Visualising Time-series Data\n\n\nCode\nts_longer &lt;- ts_data %&gt;%\n  pivot_longer(cols = c(2:34),\n               names_to = \"Country\",\n               values_to = \"Arrivals\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#visualising-single-time-series-ggplot2-methods",
    "href": "In-class_Ex/In-class_Ex07.html#visualising-single-time-series-ggplot2-methods",
    "title": "In-class Exercise 07",
    "section": "Visualising single time-series: ggplot2 methods",
    "text": "Visualising single time-series: ggplot2 methods\n\n\nCode\nts_longer %&gt;%\n  filter(Country == \"Vietnam\") %&gt;%\n  ggplot(aes(x = `Month-Year`, \n             y = Arrivals))+\n  geom_line(size = 1)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#plotting-multiple-time-series-data-with-ggplot2-methods",
    "href": "In-class_Ex/In-class_Ex07.html#plotting-multiple-time-series-data-with-ggplot2-methods",
    "title": "In-class Exercise 07",
    "section": "Plotting multiple time-series data with ggplot2 methods",
    "text": "Plotting multiple time-series data with ggplot2 methods\n\n\nCode\nggplot(data = ts_longer, \n       aes(x = `Month-Year`, \n           y = Arrivals,\n           color = Country))+\n  geom_line(size = 0.5) +\n  theme(legend.position = \"bottom\", \n        legend.box.spacing = unit(0.5, \"cm\"))\n\n\n\n\n\n\n\n\n\nThe facet_wrap() function from the ggplot2 package is used to create small multiple line graphs, also known as a trellis plot."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#visual-analysis-of-time-series-data",
    "href": "In-class_Ex/In-class_Ex07.html#visual-analysis-of-time-series-data",
    "title": "In-class Exercise 07",
    "section": "Visual Analysis of Time-series Data",
    "text": "Visual Analysis of Time-series Data\nTo effectively visualize time-series data, we need to transform the data frame from wide to long format. This can be achieved using the pivot_longer() function from the tidyr package, as demonstrated in the code below.\n\n\nCode\ntsibble_longer &lt;- ts_tsibble %&gt;%\n  pivot_longer(cols = c(2:34),\n               names_to = \"Country\",\n               values_to = \"Arrivals\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#visual-analysis-of-seasonality-with-seasonal-plot",
    "href": "In-class_Ex/In-class_Ex07.html#visual-analysis-of-seasonality-with-seasonal-plot",
    "title": "In-class Exercise 07",
    "section": "Visual Analysis of Seasonality with Seasonal Plot",
    "text": "Visual Analysis of Seasonality with Seasonal Plot\nA seasonal plot is similar to a time plot but displays data based on individual seasons. It can be created using the gg_season() function from the feasts package.\n\n\nCode\ntsibble_longer %&gt;%\n  filter(Country == \"Italy\" |\n         Country == \"Vietnam\" |\n         Country == \"United Kingdom\" |\n         Country == \"Germany\") %&gt;% \n  gg_season(Arrivals)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#visual-analysis-of-seasonality-with-cycle-plot",
    "href": "In-class_Ex/In-class_Ex07.html#visual-analysis-of-seasonality-with-cycle-plot",
    "title": "In-class Exercise 07",
    "section": "Visual Analysis of Seasonality with Cycle Plot",
    "text": "Visual Analysis of Seasonality with Cycle Plot\nFigure below shows two time-series lines of visitor arrivals from Vietnam and Italy. Both lines reveal clear sign of seasonal patterns but not the trend.\n\n\nCode\ntsibble_longer %&gt;%\n  filter(Country == \"Vietnam\" |\n         Country == \"Italy\") %&gt;% \n  autoplot(Arrivals) + \n  facet_grid(Country ~ ., scales = \"free_y\")\n\n\n\n\n\n\n\n\n\nIn the code chunk below, cycle plots using gg_subseries() of feasts package are created. Notice that the cycle plots show not only seasonal patterns but also trend.\n\n\nCode\ntsibble_longer %&gt;%\n  filter(Country == \"Vietnam\" |\n         Country == \"Italy\") %&gt;% \n  gg_subseries(Arrivals)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#tsibble-object",
    "href": "In-class_Ex/In-class_Ex07.html#tsibble-object",
    "title": "In-class Exercise 07",
    "section": "tsibble object",
    "text": "tsibble object\nVisualizing the tsibble object:\n\n\nCode\nts_tsibble\n\n\n# A tsibble: 144 x 35 [1M]\n   `Month-Year` `Republic of South Africa` Canada   USA Bangladesh Brunei China\n   &lt;date&gt;                            &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 2008-01-01                         3680   6972 31155       6786   3729 79599\n 2 2008-02-01                         1662   6056 27738       6314   3070 82074\n 3 2008-03-01                         3394   6220 31349       7502   4805 72546\n 4 2008-04-01                         3337   4764 26376       7333   3096 76112\n 5 2008-05-01                         2089   4460 26788       7988   3586 64808\n 6 2008-06-01                         2515   3888 29725       8301   5284 55238\n 7 2008-07-01                         2919   5313 33183       9004   4070 80747\n 8 2008-08-01                         2471   4519 27427       7913   4183 66625\n 9 2008-09-01                         2492   3421 21588       7549   3160 52649\n10 2008-10-01                         3023   4756 25112       7527   2983 54423\n# ℹ 134 more rows\n# ℹ 28 more variables: `Hong Kong SAR (China)` &lt;dbl&gt;, India &lt;dbl&gt;,\n#   Indonesia &lt;dbl&gt;, Japan &lt;dbl&gt;, `South Korea` &lt;dbl&gt;, Kuwait &lt;dbl&gt;,\n#   Malaysia &lt;dbl&gt;, Myanmar &lt;dbl&gt;, Pakistan &lt;dbl&gt;, Philippines &lt;dbl&gt;,\n#   `Saudi Arabia` &lt;dbl&gt;, `Sri Lanka` &lt;dbl&gt;, Taiwan &lt;dbl&gt;, Thailand &lt;dbl&gt;,\n#   `United Arab Emirates` &lt;dbl&gt;, Vietnam &lt;dbl&gt;, `Belgium & Luxembourg` &lt;dbl&gt;,\n#   Finland &lt;dbl&gt;, France &lt;dbl&gt;, Germany &lt;dbl&gt;, Italy &lt;dbl&gt;, …"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#time-series-decomposition",
    "href": "In-class_Ex/In-class_Ex07.html#time-series-decomposition",
    "title": "In-class Exercise 07",
    "section": "Time series decomposition",
    "text": "Time series decomposition"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#single-time-series-decomposition",
    "href": "In-class_Ex/In-class_Ex07.html#single-time-series-decomposition",
    "title": "In-class Exercise 07",
    "section": "Single time series decomposition",
    "text": "Single time series decomposition\nIn the code chunk below, ACF() of feasts package is used to plot the ACF curve of visitor arrival from Vietnam.\n\n\nCode\ntsibble_longer %&gt;%\n  filter(`Country` == \"Vietnam\") %&gt;%\n  ACF(Arrivals) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\n\nIn the code chunk below, PACF() of feasts package is used to plot the Partial ACF curve of visitor arrival from Vietnam.\n\n\nCode\ntsibble_longer %&gt;%\n  filter(`Country` == \"Vietnam\") %&gt;%\n  PACF(Arrivals) %&gt;% \n  autoplot()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#multiple-time-series-decomposition",
    "href": "In-class_Ex/In-class_Ex07.html#multiple-time-series-decomposition",
    "title": "In-class Exercise 07",
    "section": "Multiple time-series decomposition",
    "text": "Multiple time-series decomposition\nCode chunk below is used to prepare a trellis plot of ACFs for visitor arrivals from Vietnam, Italy, United Kingdom and China.\n\n\nCode\ntsibble_longer %&gt;%\n  filter(`Country` == \"Vietnam\" |\n         `Country` == \"Italy\" |\n         `Country` == \"United Kingdom\" |\n         `Country` == \"China\") %&gt;%\n  ACF(Arrivals) %&gt;%\n  autoplot()\n\n\n\n\n\n\n\n\n\nOn the other hand, code chunk below is used to prepare a trellis plot of PACFs for visitor arrivals from Vietnam, Italy, United Kingdom and China.\n\n\nCode\ntsibble_longer %&gt;%\n  filter(`Country` == \"Vietnam\" |\n         `Country` == \"Italy\" |\n         `Country` == \"United Kingdom\" |\n         `Country` == \"China\") %&gt;%\n  PACF(Arrivals) %&gt;%\n  autoplot()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#composite-plot-of-time-series-decomposition",
    "href": "In-class_Ex/In-class_Ex07.html#composite-plot-of-time-series-decomposition",
    "title": "In-class Exercise 07",
    "section": "Composite plot of time series decomposition",
    "text": "Composite plot of time series decomposition\nAn interesting function in the feasts package for time series decomposition is gg_tsdisplay(). It creates a composite plot, displaying the original time series at the top, with the ACF (Autocorrelation Function) plot on the left and the seasonal plot on the right.\n\n\nCode\ntsibble_longer %&gt;%\n  filter(`Country` == \"Vietnam\") %&gt;%\n  gg_tsdisplay(Arrivals)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#visual-stl-diagnostics",
    "href": "In-class_Ex/In-class_Ex07.html#visual-stl-diagnostics",
    "title": "In-class Exercise 07",
    "section": "Visual STL Diagnostics",
    "text": "Visual STL Diagnostics\nSTL (Seasonal and Trend decomposition using Loess) is a robust method for time series decomposition, widely used in economic and environmental analyses. Developed by Cleveland et al. (1990), STL utilizes LOESS, a nonlinear regression technique, to break a time series into trend, seasonal, and remainder components. The algorithm operates in two loops: the inner loop alternates between seasonal and trend smoothing, while the outer loop minimizes outlier effects. Unlike classical decomposition methods such as SEATS and X11, STL can handle any seasonal pattern, allows user control over trend and seasonal smoothness, and is robust to outliers, ensuring that anomalies do not distort the overall trend or seasonality.\nIn the code chunk below, STL() of feasts package is used to decomposite visitor arrivals from Vietnam data.\n\n\nCode\ntsibble_longer %&gt;%\n  filter(`Country` == \"Vietnam\") %&gt;%\n  model(stl = STL(Arrivals)) %&gt;%\n  components() %&gt;%\n  autoplot()\n\n\n\n\n\n\n\n\n\nThe grey bars on the left of each panel indicate the relative scale of the components. Since each panel has a different scale, the bar sizes vary, even though they represent the same length. A larger grey bar in the bottom panel suggests that the remainder component has the smallest variation. If the bottom three panels were resized so their bars matched the data panel, all components would be on the same scale."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#classical-decomposition-with-feasts",
    "href": "In-class_Ex/In-class_Ex07.html#classical-decomposition-with-feasts",
    "title": "In-class Exercise 07",
    "section": "Classical Decomposition with feasts",
    "text": "Classical Decomposition with feasts\nThe classical_decomposition() function from the feasts package decomposes a time series into seasonal, trend, and irregular components using moving averages. It supports both additive and multiplicative seasonal components.\n\n\nCode\ntsibble_longer %&gt;%\n  filter(`Country` == \"Vietnam\") %&gt;%\n  model(\n    classical_decomposition(\n      Arrivals, type = \"additive\")) %&gt;%\n  components() %&gt;%\n  autoplot()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#time-series-data-sampling",
    "href": "In-class_Ex/In-class_Ex07.html#time-series-data-sampling",
    "title": "In-class Exercise 07",
    "section": "Time Series Data Sampling",
    "text": "Time Series Data Sampling\nIn forecasting, it is best practice to split the dataset into a training (estimate) sample and a hold-out sample. The training sample (typically 75-80% of the data) is used to estimate starting values and smoothing parameters, while the hold-out sample is used to evaluate forecasting performance on unseen data. This ensures that the model is assessed based on new observations, preventing overfitting.\nFirst, an extra column called Type indicating training or hold-out will be created by using mutate() of dplyr package. It will be extremely useful for subsequent data visualisation.\nIn this example we will use the last 12 months for hold-out and the rest for training.\nFirst, an extra column called Type indicating training or hold-out will be created by using mutate() of dplyr package. It will be extremely useful for subsequent data visualisation.\n\n\nCode\nvietnam_ts &lt;- tsibble_longer %&gt;%\n  filter(Country == \"Vietnam\") %&gt;% \n  mutate(Type = if_else(\n    `Month-Year` &gt;= \"2019-01-01\", \n    \"Hold-out\", \"Training\"))\n\n\nNext, a training data set is extracted from the original data set by using filter() of dplyr package.\n\n\nCode\nvietnam_train &lt;- vietnam_ts %&gt;%\n  filter(`Month-Year` &lt; \"2019-01-01\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02.html#datavis-makeover-3",
    "href": "Take-home_Ex/Take-home_Ex02.html#datavis-makeover-3",
    "title": "Take-home Exercise 02",
    "section": "",
    "text": "The original visualizations analyzed in this analysis are sourced from here.\n\n\n\n\n\nIn the below section, we will evaluate the effectiveness of the above visualization by identifying its pros and cons, focusing on aspects such as (1) clarity, and (2) visual appeal:\n\nClarity: How well the data is presented and understood\n\n\n\n\n\n\n\n\n\nPros\nCons\nSuggested fixes\n\n\n\n\nClear regional differentiation - The color-coded regions (North America, Europe, Asia, Oceania) make it easier to distinguish areas.\nColor scheme inconsistency - The colors used for regions in the world map do not match the second visualization, leading to confusion.\nEnsure consistent colors across both graphics (e.g., North America should be red in both images).\n\n\nEasy country identification - The use of country flags provides quick recognition of major trading partners on the world map.\nLack of legend for flags - It is unclear whether the flags represent top import/export partners or something else.\nInclude a small legend explaining the significance of the flags.\n\n\nWell-placed title - The main title is clear and ensures users immediately understand the topic.\nWorld map does not indicate rankings - It is difficult to determine the largest trading partners.\nAdd ranking indicators - e.g.: numbered markers, or a gradient effect on the map.\n\n\n\nNeed to scroll to see rankings - Users must scroll down to understand that U.S is the top trading partner, followed by EU, and China.\nOverlay rankings on the world map so users can see top trading partners at a glance.\n\n\n\nLack of clear distinction between Regions, Countries, and Economic & Political Unions - The visualization presents them together, making it difficult to see how rankings differ.\nIntroduce clear labels or grouping for Regions, Countries, and Economic & Political Unions in both the world map and bar chart.\n\n\n\n\nVisual appeal: How visually engaging and effective the design is\n\n\n\n\n\n\n\n\n\nPros\nCons\nSuggested fixes\n\n\n\n\nEngaging world map design - The dotted world map with vibrant colors creates a modern and appealing look.\nColor inconsistency between the two visuals - North America is red in the map but blue in the bar chart.\nUse a unified color scheme so regions in both visuals match.\n\n\n\n\n\n\n\n\n\n\n\nThe code chunk below imports the (1) Exports of Services by Major Trading Partner datasets, and (2) Imports of Services by Major Trading Partner datasets downloaded from the Department of Statistics Singapore (DOS), using the read_excel() function from the readxl package. These datasets contain trade value of exports and imports of services by major trading partner across various 2000 to 2023, which will be processed and analyzed in subsequent steps.\n\n\nCode\nexport_partners &lt;- read_excel(\"data/Exports Of Services By Major Trading Partner_base.xlsx\")\n\n\n\n\nCode\nimport_partners &lt;- read_excel(\"data/Imports Of Services By Major Trading Partner_base.xlsx\")\n\n\n\n\n\n\nglimpse(): provides a transposed overview of a dataset, showing variables and their types in a concise format.\nhead(): displays the first few rows of a dataset (default is 6 rows) to give a quick preview of the data.\nsummary(): generates a statistical summary of each variable, including measures like mean, median, and range for numeric data.\nduplicated():returns a logical vector indicating which elements or rows in a vector or data frame are duplicates.\ncolSums(is.na()): counts the number of missing values (NA) in each column of the data frame.\nstr(): use str() to display the column names, data types, and a preview of the data.\n\n\nglimpse()head()summary()duplicated()colSum(is.na())str())\n\n\n\n\nCode\nglimpse(export_partners)\n\n\nRows: 68\nColumns: 25\n$ `Data Series` &lt;chr&gt; \"Asia\", \"Bangladesh\", \"Brunei Darussalam\", \"Cambodia\", \"…\n$ `2023`        &lt;dbl&gt; 170787.7, 985.7, 960.1, 475.6, 21686.1, 9909.4, 8542.6, …\n$ `2022`        &lt;dbl&gt; 174243.3, 903.9, 891.2, 389.5, 21307.3, 9730.1, 8366.6, …\n$ `2021`        &lt;dbl&gt; 147125.9, 674.8, 579.0, 256.0, 18126.2, 7817.0, 6222.5, …\n$ `2020`        &lt;dbl&gt; 115989.3, 581.6, 533.4, 250.3, 15063.2, 6337.6, 5351.4, …\n$ `2019`        &lt;dbl&gt; 114573.1, 642.4, 556.4, 259.1, 11273.2, 6383.4, 6781.8, …\n$ `2018`        &lt;dbl&gt; 104182.5, 664.0, 495.9, 290.4, 10466.3, 5957.8, 5895.2, …\n$ `2017`        &lt;dbl&gt; 85093.3, 547.7, 463.9, 226.8, 8239.5, 5326.3, 5615.7, 25…\n$ `2016`        &lt;dbl&gt; 70461.8, 482.6, 420.5, 196.1, 7351.8, 4422.4, 5088.6, 23…\n$ `2015`        &lt;dbl&gt; 69881.1, 424.3, 465.4, 114.3, 7260.0, 4236.4, 5032.0, 17…\n$ `2014`        &lt;dbl&gt; 62802.7, 384.3, 520.4, 86.3, 5841.4, 4276.0, 4837.1, 159…\n$ `2013`        &lt;dbl&gt; 55949.9, 347.5, 474.4, 89.2, 5184.6, 4857.4, 4535.8, 155…\n$ `2012`        &lt;dbl&gt; 50496.3, 349.1, 464.1, 115.0, 5156.3, 4375.0, 4062.7, 14…\n$ `2011`        &lt;dbl&gt; 48318.2, 333.5, 468.5, 109.2, 5104.8, 4244.7, 3537.8, 13…\n$ `2010`        &lt;dbl&gt; 46254.5, 272.6, 508.1, 78.4, 4928.8, 4089.3, 3291.1, 114…\n$ `2009`        &lt;chr&gt; \"40325.300000000003\", \"209.3\", \"324.39999999999998\", \"99…\n$ `2008`        &lt;dbl&gt; 43935.5, 264.8, 272.6, 139.7, 4676.6, 3502.5, 3321.9, 93…\n$ `2007`        &lt;dbl&gt; 39789.5, 284.0, 248.3, 88.1, 4429.7, 3191.3, 3026.0, 136…\n$ `2006`        &lt;dbl&gt; 33098.0, 285.5, 120.6, 110.8, 3555.3, 2448.5, 2969.8, 10…\n$ `2005`        &lt;chr&gt; \"28070.400000000001\", \"274.60000000000002\", \"123.3\", \"77…\n$ `2004`        &lt;chr&gt; \"25457.1\", \"188.9\", \"138.6\", \"68.7\", \"2313.6999999999998…\n$ `2003`        &lt;chr&gt; \"18942\", \"157\", \"151.4\", \"45.3\", \"2059.6999999999998\", \"…\n$ `2002`        &lt;chr&gt; \"17741.7\", \"118.3\", \"144.30000000000001\", \"37.9\", \"1912\"…\n$ `2001`        &lt;chr&gt; \"16226.5\", \"121.8\", \"97.8\", \"29.8\", \"1866.5\", \"735.4\", \"…\n$ `2000`        &lt;chr&gt; \"15816.9\", \"89.7\", \"100.5\", \"18.399999999999999\", \"1860.…\n\n\n\n\n\n\nCode\nhead(export_partners)\n\n\n# A tibble: 6 × 25\n  `Data Series`   `2023` `2022` `2021` `2020` `2019` `2018` `2017` `2016` `2015`\n  &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Asia            1.71e5 1.74e5 1.47e5 1.16e5 1.15e5 1.04e5 85093. 70462. 69881.\n2 Bangladesh      9.86e2 9.04e2 6.75e2 5.82e2 6.42e2 6.64e2   548.   483.   424.\n3 Brunei Darussa… 9.60e2 8.91e2 5.79e2 5.33e2 5.56e2 4.96e2   464.   420.   465.\n4 Cambodia        4.76e2 3.89e2 2.56e2 2.50e2 2.59e2 2.90e2   227.   196.   114.\n5 Hong Kong       2.17e4 2.13e4 1.81e4 1.51e4 1.13e4 1.05e4  8240.  7352.  7260 \n6 India           9.91e3 9.73e3 7.82e3 6.34e3 6.38e3 5.96e3  5326.  4422.  4236.\n# ℹ 15 more variables: `2014` &lt;dbl&gt;, `2013` &lt;dbl&gt;, `2012` &lt;dbl&gt;, `2011` &lt;dbl&gt;,\n#   `2010` &lt;dbl&gt;, `2009` &lt;chr&gt;, `2008` &lt;dbl&gt;, `2007` &lt;dbl&gt;, `2006` &lt;dbl&gt;,\n#   `2005` &lt;chr&gt;, `2004` &lt;chr&gt;, `2003` &lt;chr&gt;, `2002` &lt;chr&gt;, `2001` &lt;chr&gt;,\n#   `2000` &lt;chr&gt;\n\n\n\n\n\n\nCode\nsummary(export_partners)\n\n\n Data Series             2023               2022               2021         \n Length:68          Min.   :   296.0   Min.   :   169.8   Min.   :   191.5  \n Class :character   1st Qu.:   789.3   1st Qu.:   685.9   1st Qu.:   560.1  \n Mode  :character   Median :  3133.4   Median :  3536.1   Median :  2923.7  \n                    Mean   : 13585.2   Mean   : 13476.6   Mean   : 11404.5  \n                    3rd Qu.:  9860.0   3rd Qu.:  9770.0   3rd Qu.:  8172.4  \n                    Max.   :170787.7   Max.   :174243.3   Max.   :147125.9  \n      2020               2019               2018               2017        \n Min.   :   104.1   Min.   :    93.3   Min.   :    85.0   Min.   :   67.9  \n 1st Qu.:   430.4   1st Qu.:   461.6   1st Qu.:   434.7   1st Qu.:  351.5  \n Median :  2219.9   Median :  1751.8   Median :  1659.9   Median : 1156.8  \n Mean   :  8714.4   Mean   :  8282.8   Mean   :  7774.1   Mean   : 6370.7  \n 3rd Qu.:  6733.2   3rd Qu.:  6483.0   3rd Qu.:  5910.9   3rd Qu.: 5398.6  \n Max.   :115989.3   Max.   :114573.1   Max.   :104182.5   Max.   :85093.3  \n      2016              2015              2014              2013        \n Min.   :   72.0   Min.   :   72.1   Min.   :   61.5   Min.   :   66.5  \n 1st Qu.:  295.7   1st Qu.:  370.0   1st Qu.:  311.5   1st Qu.:  307.6  \n Median : 1173.2   Median : 1549.8   Median : 1377.8   Median : 1177.2  \n Mean   : 5541.1   Mean   : 5569.5   Mean   : 5011.7   Mean   : 4548.1  \n 3rd Qu.: 4588.9   3rd Qu.: 5121.5   3rd Qu.: 4416.3   3rd Qu.: 4545.9  \n Max.   :70461.8   Max.   :69881.1   Max.   :62802.7   Max.   :55949.9  \n      2012              2011              2010             2009          \n Min.   :   77.3   Min.   :   33.1   Min.   :   17.5   Length:68         \n 1st Qu.:  279.5   1st Qu.:  233.1   1st Qu.:  262.3   Class :character  \n Median : 1008.1   Median : 1023.5   Median :  769.5   Mode  :character  \n Mean   : 4081.9   Mean   : 3751.8   Mean   : 3455.8                     \n 3rd Qu.: 3777.1   3rd Qu.: 3442.4   3rd Qu.: 3007.9                     \n Max.   :50496.3   Max.   :48318.2   Max.   :46254.5                     \n      2008              2007              2006             2005          \n Min.   :    5.4   Min.   :    1.8   Min.   :    1.6   Length:68         \n 1st Qu.:  204.1   1st Qu.:  179.8   1st Qu.:  146.8   Class :character  \n Median :  683.1   Median :  534.7   Median :  441.1   Mode  :character  \n Mean   : 3270.8   Mean   : 2834.3   Mean   : 2384.7                     \n 3rd Qu.: 2840.8   3rd Qu.: 2727.7   3rd Qu.: 2028.2                     \n Max.   :43935.5   Max.   :39789.5   Max.   :33098.0                     \n     2004               2003               2002               2001          \n Length:68          Length:68          Length:68          Length:68         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n     2000          \n Length:68         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n\n\n\n\n\nCode\nexport_partners[duplicated(export_partners),]\n\n\n# A tibble: 0 × 25\n# ℹ 25 variables: Data Series &lt;chr&gt;, 2023 &lt;dbl&gt;, 2022 &lt;dbl&gt;, 2021 &lt;dbl&gt;,\n#   2020 &lt;dbl&gt;, 2019 &lt;dbl&gt;, 2018 &lt;dbl&gt;, 2017 &lt;dbl&gt;, 2016 &lt;dbl&gt;, 2015 &lt;dbl&gt;,\n#   2014 &lt;dbl&gt;, 2013 &lt;dbl&gt;, 2012 &lt;dbl&gt;, 2011 &lt;dbl&gt;, 2010 &lt;dbl&gt;, 2009 &lt;chr&gt;,\n#   2008 &lt;dbl&gt;, 2007 &lt;dbl&gt;, 2006 &lt;dbl&gt;, 2005 &lt;chr&gt;, 2004 &lt;chr&gt;, 2003 &lt;chr&gt;,\n#   2002 &lt;chr&gt;, 2001 &lt;chr&gt;, 2000 &lt;chr&gt;\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nEnsure that there are no duplicated columns, if not will have to investigate further.\n\n\n\n\n\n\n\nCode\ncolSums(is.na(export_partners))\n\n\nData Series        2023        2022        2021        2020        2019 \n          0           0           0           0           0           0 \n       2018        2017        2016        2015        2014        2013 \n          0           0           0           0           0           0 \n       2012        2011        2010        2009        2008        2007 \n          0           0           0           0           0           0 \n       2006        2005        2004        2003        2002        2001 \n          0           0           0           0           0           0 \n       2000 \n          0 \n\n\n\n\n\n\n\n\nNote\n\n\n\n\nEnsure that there are no NA values, if not will have to investigate further.\nPossibility to use the replace_na() function to replace missing values with 0 in specified columns.\n\n\n\n\n\n\n\nCode\nstr(export_partners)\n\n\ntibble [68 × 25] (S3: tbl_df/tbl/data.frame)\n $ Data Series: chr [1:68] \"Asia\" \"Bangladesh\" \"Brunei Darussalam\" \"Cambodia\" ...\n $ 2023       : num [1:68] 170788 986 960 476 21686 ...\n $ 2022       : num [1:68] 174243 904 891 390 21307 ...\n $ 2021       : num [1:68] 147126 675 579 256 18126 ...\n $ 2020       : num [1:68] 115989 582 533 250 15063 ...\n $ 2019       : num [1:68] 114573 642 556 259 11273 ...\n $ 2018       : num [1:68] 104183 664 496 290 10466 ...\n $ 2017       : num [1:68] 85093 548 464 227 8240 ...\n $ 2016       : num [1:68] 70462 483 420 196 7352 ...\n $ 2015       : num [1:68] 69881 424 465 114 7260 ...\n $ 2014       : num [1:68] 62802.7 384.3 520.4 86.3 5841.4 ...\n $ 2013       : num [1:68] 55949.9 347.5 474.4 89.2 5184.6 ...\n $ 2012       : num [1:68] 50496 349 464 115 5156 ...\n $ 2011       : num [1:68] 48318 334 468 109 5105 ...\n $ 2010       : num [1:68] 46254.5 272.6 508.1 78.4 4928.8 ...\n $ 2009       : chr [1:68] \"40325.300000000003\" \"209.3\" \"324.39999999999998\" \"99.3\" ...\n $ 2008       : num [1:68] 43936 265 273 140 4677 ...\n $ 2007       : num [1:68] 39789.5 284 248.3 88.1 4429.7 ...\n $ 2006       : num [1:68] 33098 286 121 111 3555 ...\n $ 2005       : chr [1:68] \"28070.400000000001\" \"274.60000000000002\" \"123.3\" \"77.099999999999994\" ...\n $ 2004       : chr [1:68] \"25457.1\" \"188.9\" \"138.6\" \"68.7\" ...\n $ 2003       : chr [1:68] \"18942\" \"157\" \"151.4\" \"45.3\" ...\n $ 2002       : chr [1:68] \"17741.7\" \"118.3\" \"144.30000000000001\" \"37.9\" ...\n $ 2001       : chr [1:68] \"16226.5\" \"121.8\" \"97.8\" \"29.8\" ...\n $ 2000       : chr [1:68] \"15816.9\" \"89.7\" \"100.5\" \"18.399999999999999\" ...\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nEnsure that all variables are correctly classified by data type; recast variable types if needed.\nVariables are correctly classified - where categorical variables are classified as character, while continuous variables are classified as double.\n\n\n\n\n\n\n\n\n\n\nglimpse()head()summary()duplicated()colSum(is.na())str())\n\n\n\n\nCode\nglimpse(import_partners)\n\n\nRows: 68\nColumns: 25\n$ `Data Series` &lt;chr&gt; \"Asia\", \"Bangladesh\", \"Brunei Darussalam\", \"Cambodia\", \"…\n$ `2023`        &lt;dbl&gt; 139497.1, 468.8, 196.8, 121.2, 20255.7, 13532.6, 3517.7,…\n$ `2022`        &lt;dbl&gt; 136729.5, 599.1, 83.5, 133.6, 19762.7, 12913.9, 3632.4, …\n$ `2021`        &lt;dbl&gt; 115022.1, 441.8, 53.6, 150.8, 20830.3, 10462.6, 3358.9, …\n$ `2020`        &lt;dbl&gt; 93351.0, 392.9, 69.7, 159.6, 14410.3, 8780.3, 2949.9, 22…\n$ `2019`        &lt;dbl&gt; 83242.3, 380.9, 49.2, 217.9, 13093.5, 7813.9, 2794.3, 23…\n$ `2018`        &lt;dbl&gt; 78004.9, 318.7, 82.2, 116.5, 12234.9, 6612.3, 2494.2, 28…\n$ `2017`        &lt;dbl&gt; 65654.5, 251.9, 73.5, 112.7, 10874.2, 5461.9, 2245.5, 32…\n$ `2016`        &lt;chr&gt; \"55208.6\", \"233.9\", \"96.6\", \"92.6\", \"8845.4\", \"4549.6000…\n$ `2015`        &lt;dbl&gt; 52852.1, 192.7, 67.1, 82.1, 8664.1, 4035.6, 2170.2, 215.…\n$ `2014`        &lt;dbl&gt; 48565.4, 234.8, 53.1, 65.8, 7674.3, 3839.5, 1990.2, 148.…\n$ `2013`        &lt;dbl&gt; 40683.5, 176.8, 48.9, 63.5, 5096.7, 3555.7, 1920.0, 138.…\n$ `2012`        &lt;dbl&gt; 36390.3, 160.0, 58.3, 127.8, 4233.6, 3039.8, 1816.8, 113…\n$ `2011`        &lt;dbl&gt; 34902.8, 149.4, 41.3, 133.2, 3944.7, 2979.5, 1733.9, 74.…\n$ `2010`        &lt;dbl&gt; 31712.3, 141.0, 36.6, 110.7, 4279.6, 2700.1, 1780.2, 99.…\n$ `2009`        &lt;dbl&gt; 27360.1, 106.5, 40.2, 102.9, 3503.1, 2352.3, 1566.5, 62.…\n$ `2008`        &lt;dbl&gt; 28848.5, 82.4, 109.6, 112.4, 3623.9, 2345.5, 1673.7, 60.…\n$ `2007`        &lt;dbl&gt; 24679.0, 79.9, 101.2, 100.2, 3520.0, 1869.5, 1587.4, 47.…\n$ `2006`        &lt;chr&gt; \"22188.6\", \"105.7\", \"49.6\", \"205.6\", \"2938.8\", \"1512.1\",…\n$ `2005`        &lt;chr&gt; \"19161.400000000001\", \"91.2\", \"36.299999999999997\", \"69.…\n$ `2004`        &lt;chr&gt; \"18110.8\", \"82.2\", \"60.6\", \"58\", \"2222.6999999999998\", \"…\n$ `2003`        &lt;chr&gt; \"12607.1\", \"70.900000000000006\", \"49.3\", \"34.5\", \"1685.2…\n$ `2002`        &lt;chr&gt; \"10987.5\", \"77.5\", \"37.799999999999997\", \"30\", \"1585.9\",…\n$ `2001`        &lt;chr&gt; \"9702.7000000000007\", \"39.700000000000003\", \"29.4\", \"20.…\n$ `2000`        &lt;chr&gt; \"8639.7999999999993\", \"38.799999999999997\", \"32.6\", \"9\",…\n\n\n\n\n\n\nCode\nhead(import_partners)\n\n\n# A tibble: 6 × 25\n  `Data Series`   `2023` `2022` `2021` `2020` `2019` `2018` `2017` `2016` `2015`\n  &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1 Asia            1.39e5 1.37e5 1.15e5 9.34e4 8.32e4 7.80e4 6.57e4 55208… 5.29e4\n2 Bangladesh      4.69e2 5.99e2 4.42e2 3.93e2 3.81e2 3.19e2 2.52e2 233.9  1.93e2\n3 Brunei Darussa… 1.97e2 8.35e1 5.36e1 6.97e1 4.92e1 8.22e1 7.35e1 96.6   6.71e1\n4 Cambodia        1.21e2 1.34e2 1.51e2 1.60e2 2.18e2 1.16e2 1.13e2 92.6   8.21e1\n5 Hong Kong       2.03e4 1.98e4 2.08e4 1.44e4 1.31e4 1.22e4 1.09e4 8845.4 8.66e3\n6 India           1.35e4 1.29e4 1.05e4 8.78e3 7.81e3 6.61e3 5.46e3 4549.… 4.04e3\n# ℹ 15 more variables: `2014` &lt;dbl&gt;, `2013` &lt;dbl&gt;, `2012` &lt;dbl&gt;, `2011` &lt;dbl&gt;,\n#   `2010` &lt;dbl&gt;, `2009` &lt;dbl&gt;, `2008` &lt;dbl&gt;, `2007` &lt;dbl&gt;, `2006` &lt;chr&gt;,\n#   `2005` &lt;chr&gt;, `2004` &lt;chr&gt;, `2003` &lt;chr&gt;, `2002` &lt;chr&gt;, `2001` &lt;chr&gt;,\n#   `2000` &lt;chr&gt;\n\n\n\n\n\n\nCode\nsummary(import_partners)\n\n\n Data Series             2023               2022               2021         \n Length:68          Min.   :    93.1   Min.   :    83.5   Min.   :    53.6  \n Class :character   1st Qu.:   608.7   1st Qu.:   565.6   1st Qu.:   448.8  \n Mode  :character   Median :  2333.1   Median :  2153.3   Median :  1762.8  \n                    Mean   : 11464.3   Mean   : 10862.6   Mean   :  9301.5  \n                    3rd Qu.:  8176.1   3rd Qu.:  8600.9   3rd Qu.:  6637.0  \n                    Max.   :139497.1   Max.   :136729.5   Max.   :115022.1  \n      2020              2019              2018              2017        \n Min.   :   69.7   Min.   :   49.2   Min.   :   82.2   Min.   :   73.5  \n 1st Qu.:  344.2   1st Qu.:  309.6   1st Qu.:  310.2   1st Qu.:  274.6  \n Median : 1669.7   Median : 1556.3   Median : 1565.8   Median : 1411.4  \n Mean   : 7982.5   Mean   : 7263.2   Mean   : 7045.8   Mean   : 6413.4  \n 3rd Qu.: 5550.0   3rd Qu.: 5604.6   3rd Qu.: 5620.9   3rd Qu.: 6136.3  \n Max.   :93351.0   Max.   :83242.3   Max.   :78004.9   Max.   :65654.5  \n     2016                2015              2014              2013        \n Length:68          Min.   :   34.3   Min.   :   40.3   Min.   :   48.9  \n Class :character   1st Qu.:  217.2   1st Qu.:  250.5   1st Qu.:  192.3  \n Mode  :character   Median : 1142.8   Median : 1000.9   Median :  901.6  \n                    Mean   : 5567.2   Mean   : 5337.7   Mean   : 4623.7  \n                    3rd Qu.: 4324.4   3rd Qu.: 5590.8   3rd Qu.: 5151.0  \n                    Max.   :52852.1   Max.   :50558.2   Max.   :44479.7  \n      2012              2011              2010              2009         \n Min.   :   43.5   Min.   :   27.2   Min.   :    4.7   Min.   :    4.50  \n 1st Qu.:  160.7   1st Qu.:  141.7   1st Qu.:  109.5   1st Qu.:   98.22  \n Median :  628.3   Median :  606.6   Median :  438.5   Median :  380.20  \n Mean   : 4006.5   Mean   : 3466.0   Mean   : 3164.2   Mean   : 2726.97  \n 3rd Qu.: 3773.5   3rd Qu.: 3178.5   3rd Qu.: 2564.5   3rd Qu.: 2279.35  \n Max.   :36390.3   Max.   :34902.8   Max.   :31712.3   Max.   :27360.10  \n      2008               2007              2006               2005          \n Min.   :    4.50   Min.   :    8.90   Length:68          Length:68         \n 1st Qu.:   99.97   1st Qu.:   85.83   Class :character   Class :character  \n Median :  407.45   Median :  280.55   Mode  :character   Mode  :character  \n Mean   : 2769.65   Mean   : 2348.02                                        \n 3rd Qu.: 2253.03   3rd Qu.: 1720.08                                        \n Max.   :28848.50   Max.   :24679.00                                        \n     2004               2003               2002               2001          \n Length:68          Length:68          Length:68          Length:68         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n     2000          \n Length:68         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n\n\n\n\n\nCode\nimport_partners[duplicated(import_partners),]\n\n\n# A tibble: 0 × 25\n# ℹ 25 variables: Data Series &lt;chr&gt;, 2023 &lt;dbl&gt;, 2022 &lt;dbl&gt;, 2021 &lt;dbl&gt;,\n#   2020 &lt;dbl&gt;, 2019 &lt;dbl&gt;, 2018 &lt;dbl&gt;, 2017 &lt;dbl&gt;, 2016 &lt;chr&gt;, 2015 &lt;dbl&gt;,\n#   2014 &lt;dbl&gt;, 2013 &lt;dbl&gt;, 2012 &lt;dbl&gt;, 2011 &lt;dbl&gt;, 2010 &lt;dbl&gt;, 2009 &lt;dbl&gt;,\n#   2008 &lt;dbl&gt;, 2007 &lt;dbl&gt;, 2006 &lt;chr&gt;, 2005 &lt;chr&gt;, 2004 &lt;chr&gt;, 2003 &lt;chr&gt;,\n#   2002 &lt;chr&gt;, 2001 &lt;chr&gt;, 2000 &lt;chr&gt;\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nEnsure that there are no duplicated columns, if not will have to investigate further.\n\n\n\n\n\n\n\nCode\ncolSums(is.na(import_partners))\n\n\nData Series        2023        2022        2021        2020        2019 \n          0           0           0           0           0           0 \n       2018        2017        2016        2015        2014        2013 \n          0           0           0           0           0           0 \n       2012        2011        2010        2009        2008        2007 \n          0           0           0           0           0           0 \n       2006        2005        2004        2003        2002        2001 \n          0           0           0           0           0           0 \n       2000 \n          0 \n\n\n\n\n\n\n\n\nNote\n\n\n\n\nEnsure that there are no NA values, if not will have to investigate further.\nPossibility to use the replace_na() function to replace missing values with 0 in specified columns.\n\n\n\n\n\n\n\nCode\nstr(import_partners)\n\n\ntibble [68 × 25] (S3: tbl_df/tbl/data.frame)\n $ Data Series: chr [1:68] \"Asia\" \"Bangladesh\" \"Brunei Darussalam\" \"Cambodia\" ...\n $ 2023       : num [1:68] 139497 469 197 121 20256 ...\n $ 2022       : num [1:68] 136729.5 599.1 83.5 133.6 19762.7 ...\n $ 2021       : num [1:68] 115022.1 441.8 53.6 150.8 20830.3 ...\n $ 2020       : num [1:68] 93351 392.9 69.7 159.6 14410.3 ...\n $ 2019       : num [1:68] 83242.3 380.9 49.2 217.9 13093.5 ...\n $ 2018       : num [1:68] 78004.9 318.7 82.2 116.5 12234.9 ...\n $ 2017       : num [1:68] 65654.5 251.9 73.5 112.7 10874.2 ...\n $ 2016       : chr [1:68] \"55208.6\" \"233.9\" \"96.6\" \"92.6\" ...\n $ 2015       : num [1:68] 52852.1 192.7 67.1 82.1 8664.1 ...\n $ 2014       : num [1:68] 48565.4 234.8 53.1 65.8 7674.3 ...\n $ 2013       : num [1:68] 40683.5 176.8 48.9 63.5 5096.7 ...\n $ 2012       : num [1:68] 36390.3 160 58.3 127.8 4233.6 ...\n $ 2011       : num [1:68] 34902.8 149.4 41.3 133.2 3944.7 ...\n $ 2010       : num [1:68] 31712.3 141 36.6 110.7 4279.6 ...\n $ 2009       : num [1:68] 27360.1 106.5 40.2 102.9 3503.1 ...\n $ 2008       : num [1:68] 28848.5 82.4 109.6 112.4 3623.9 ...\n $ 2007       : num [1:68] 24679 79.9 101.2 100.2 3520 ...\n $ 2006       : chr [1:68] \"22188.6\" \"105.7\" \"49.6\" \"205.6\" ...\n $ 2005       : chr [1:68] \"19161.400000000001\" \"91.2\" \"36.299999999999997\" \"69.900000000000006\" ...\n $ 2004       : chr [1:68] \"18110.8\" \"82.2\" \"60.6\" \"58\" ...\n $ 2003       : chr [1:68] \"12607.1\" \"70.900000000000006\" \"49.3\" \"34.5\" ...\n $ 2002       : chr [1:68] \"10987.5\" \"77.5\" \"37.799999999999997\" \"30\" ...\n $ 2001       : chr [1:68] \"9702.7000000000007\" \"39.700000000000003\" \"29.4\" \"20.9\" ...\n $ 2000       : chr [1:68] \"8639.7999999999993\" \"38.799999999999997\" \"32.6\" \"9\" ...\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nEnsure that all variables are correctly classified by data type; recast variable types if needed.\nVariables are correctly classified - where categorical variables are classified as character, while continuous variables are classified as double.\n\n\n\n\n\n\n\n\n\nTo ensure consistency, column names were converted to lowercase, and spaces were replaced with underscores. Since numerical values were stored as different data types in the datasets, all year columns (2000-2023) were converted to numeric format to prevent data type mismatches. Additionally, any missing (NA) values or dashes (“-”) were replaced with 0 to ensure data integrity. A “category” column was added to distinguish between Exports and Imports. Finally, the two datasets were combined using bind_rows().\n\n\n\n\n\n\nImportant\n\n\n\n\nYear columns (2000-2023) in both Exports and Imports dataset were converted to numeric format to ensure consistency and prevent data type mismatches.\nThere are missing (NA) values and “-” characters in the dataset.\nReplace NA values and “-” with 0 using the mutate() and replace() functions.\n\n\n\n\n\nCode\n# Ensure column names are consistent\ncolnames(export_partners) &lt;- tolower(gsub(\" \", \"_\", colnames(export_partners)))\ncolnames(import_partners) &lt;- tolower(gsub(\" \", \"_\", colnames(import_partners)))\n\n# Convert all numeric year columns to numeric type safely by handling non-numeric values and replacing NA and \"-\" with 0\nexport_partners &lt;- export_partners %&gt;% mutate(across(matches(\"^\\\\d{4}$\"), ~suppressWarnings(as.numeric(.)))) %&gt;%\n    mutate(across(matches(\"^\\\\d{4}$\"), ~replace(., is.na(.) | . == \"-\", 0)))\nimport_partners &lt;- import_partners %&gt;% mutate(across(matches(\"^\\\\d{4}$\"), ~suppressWarnings(as.numeric(.)))) %&gt;%\n    mutate(across(matches(\"^\\\\d{4}$\"), ~replace(., is.na(.) | . == \"-\", 0)))\n\n# Add a column to indicate the category (Exports, Imports)\nexport_partners &lt;- export_partners %&gt;% mutate(category = \"Exports\")\nimport_partners &lt;- import_partners %&gt;% mutate(category = \"Imports\")\n\n# Combine both datasets\ncombined_data &lt;- bind_rows(export_partners, import_partners)\n\n\n\n\n\nThe combined_data tibble contains 26 attributes, as shown below.\nThe following preprocessing checks were conducted as part of data preparation:\n\n\n\n\n\n\nPreprocessing Checks\n\n\n\n\nVerified that the correct data types were loaded in the combined_data dataset using glimpse() and str()\nEnsured there were no duplicate variable names using duplicated() in the dataset\nChecked for missing values using colSums(is.na())\n\n\n\n\nglimpse()head()summary()duplicated()colSum(is.na())str())\n\n\n\n\nCode\nglimpse(combined_data)\n\n\nRows: 136\nColumns: 26\n$ data_series &lt;chr&gt; \"Asia\", \"Bangladesh\", \"Brunei Darussalam\", \"Cambodia\", \"Ho…\n$ `2023`      &lt;dbl&gt; 170787.7, 985.7, 960.1, 475.6, 21686.1, 9909.4, 8542.6, 81…\n$ `2022`      &lt;dbl&gt; 174243.3, 903.9, 891.2, 389.5, 21307.3, 9730.1, 8366.6, 58…\n$ `2021`      &lt;dbl&gt; 147125.9, 674.8, 579.0, 256.0, 18126.2, 7817.0, 6222.5, 50…\n$ `2020`      &lt;dbl&gt; 115989.3, 581.6, 533.4, 250.3, 15063.2, 6337.6, 5351.4, 35…\n$ `2019`      &lt;dbl&gt; 114573.1, 642.4, 556.4, 259.1, 11273.2, 6383.4, 6781.8, 25…\n$ `2018`      &lt;dbl&gt; 104182.5, 664.0, 495.9, 290.4, 10466.3, 5957.8, 5895.2, 24…\n$ `2017`      &lt;dbl&gt; 85093.3, 547.7, 463.9, 226.8, 8239.5, 5326.3, 5615.7, 254.…\n$ `2016`      &lt;dbl&gt; 70461.8, 482.6, 420.5, 196.1, 7351.8, 4422.4, 5088.6, 231.…\n$ `2015`      &lt;dbl&gt; 69881.1, 424.3, 465.4, 114.3, 7260.0, 4236.4, 5032.0, 175.…\n$ `2014`      &lt;dbl&gt; 62802.7, 384.3, 520.4, 86.3, 5841.4, 4276.0, 4837.1, 159.2…\n$ `2013`      &lt;dbl&gt; 55949.9, 347.5, 474.4, 89.2, 5184.6, 4857.4, 4535.8, 155.1…\n$ `2012`      &lt;dbl&gt; 50496.3, 349.1, 464.1, 115.0, 5156.3, 4375.0, 4062.7, 146.…\n$ `2011`      &lt;dbl&gt; 48318.2, 333.5, 468.5, 109.2, 5104.8, 4244.7, 3537.8, 132.…\n$ `2010`      &lt;dbl&gt; 46254.5, 272.6, 508.1, 78.4, 4928.8, 4089.3, 3291.1, 114.2…\n$ `2009`      &lt;dbl&gt; 40325.3, 209.3, 324.4, 99.3, 4344.5, 3712.2, 3642.6, 89.2,…\n$ `2008`      &lt;dbl&gt; 43935.5, 264.8, 272.6, 139.7, 4676.6, 3502.5, 3321.9, 93.3…\n$ `2007`      &lt;dbl&gt; 39789.5, 284.0, 248.3, 88.1, 4429.7, 3191.3, 3026.0, 136.5…\n$ `2006`      &lt;dbl&gt; 33098.0, 285.5, 120.6, 110.8, 3555.3, 2448.5, 2969.8, 102.…\n$ `2005`      &lt;dbl&gt; 28070.4, 274.6, 123.3, 77.1, 2790.5, 2084.6, 2364.4, 90.4,…\n$ `2004`      &lt;dbl&gt; 25457.1, 188.9, 138.6, 68.7, 2313.7, 1742.1, 2231.8, 87.6,…\n$ `2003`      &lt;dbl&gt; 18942.0, 157.0, 151.4, 45.3, 2059.7, 1241.4, 1767.5, 63.0,…\n$ `2002`      &lt;dbl&gt; 17741.7, 118.3, 144.3, 37.9, 1912.0, 907.6, 1801.4, 51.5, …\n$ `2001`      &lt;dbl&gt; 16226.5, 121.8, 97.8, 29.8, 1866.5, 735.4, 1707.5, 24.6, 3…\n$ `2000`      &lt;dbl&gt; 15816.9, 89.7, 100.5, 18.4, 1860.6, 746.8, 1607.9, 28.9, 3…\n$ category    &lt;chr&gt; \"Exports\", \"Exports\", \"Exports\", \"Exports\", \"Exports\", \"Ex…\n\n\n\n\n\n\nCode\nhead(combined_data)\n\n\n# A tibble: 6 × 26\n  data_series     `2023` `2022` `2021` `2020` `2019` `2018` `2017` `2016` `2015`\n  &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Asia            1.71e5 1.74e5 1.47e5 1.16e5 1.15e5 1.04e5 85093. 70462. 69881.\n2 Bangladesh      9.86e2 9.04e2 6.75e2 5.82e2 6.42e2 6.64e2   548.   483.   424.\n3 Brunei Darussa… 9.60e2 8.91e2 5.79e2 5.33e2 5.56e2 4.96e2   464.   420.   465.\n4 Cambodia        4.76e2 3.89e2 2.56e2 2.50e2 2.59e2 2.90e2   227.   196.   114.\n5 Hong Kong       2.17e4 2.13e4 1.81e4 1.51e4 1.13e4 1.05e4  8240.  7352.  7260 \n6 India           9.91e3 9.73e3 7.82e3 6.34e3 6.38e3 5.96e3  5326.  4422.  4236.\n# ℹ 16 more variables: `2014` &lt;dbl&gt;, `2013` &lt;dbl&gt;, `2012` &lt;dbl&gt;, `2011` &lt;dbl&gt;,\n#   `2010` &lt;dbl&gt;, `2009` &lt;dbl&gt;, `2008` &lt;dbl&gt;, `2007` &lt;dbl&gt;, `2006` &lt;dbl&gt;,\n#   `2005` &lt;dbl&gt;, `2004` &lt;dbl&gt;, `2003` &lt;dbl&gt;, `2002` &lt;dbl&gt;, `2001` &lt;dbl&gt;,\n#   `2000` &lt;dbl&gt;, category &lt;chr&gt;\n\n\n\n\n\n\nCode\nsummary(combined_data)\n\n\n data_series             2023               2022               2021         \n Length:136         Min.   :    93.1   Min.   :    83.5   Min.   :    53.6  \n Class :character   1st Qu.:   701.3   1st Qu.:   596.2   1st Qu.:   492.4  \n Mode  :character   Median :  2595.8   Median :  2330.3   Median :  1957.3  \n                    Mean   : 12524.7   Mean   : 12169.6   Mean   : 10353.0  \n                    3rd Qu.:  8674.3   3rd Qu.:  8664.1   3rd Qu.:  7272.9  \n                    Max.   :170787.7   Max.   :174243.3   Max.   :147125.9  \n      2020               2019               2018               2017        \n Min.   :    69.7   Min.   :    49.2   Min.   :    82.2   Min.   :   67.9  \n 1st Qu.:   388.6   1st Qu.:   408.8   1st Qu.:   378.1   1st Qu.:  311.0  \n Median :  1725.6   Median :  1685.2   Median :  1597.5   Median : 1364.0  \n Mean   :  8348.4   Mean   :  7773.0   Mean   :  7410.0   Mean   : 6392.0  \n 3rd Qu.:  6002.1   3rd Qu.:  6162.6   3rd Qu.:  5910.9   3rd Qu.: 5726.2  \n Max.   :115989.3   Max.   :114573.1   Max.   :104182.5   Max.   :85093.3  \n      2016            2015              2014              2013        \n Min.   :    0   Min.   :   34.3   Min.   :   40.3   Min.   :   48.9  \n 1st Qu.:  269   1st Qu.:  283.6   1st Qu.:  276.0   1st Qu.:  239.6  \n Median : 1188   Median : 1250.4   Median : 1148.3   Median : 1018.6  \n Mean   : 5509   Mean   : 5568.4   Mean   : 5174.7   Mean   : 4585.9  \n 3rd Qu.: 4716   3rd Qu.: 5071.8   3rd Qu.: 4882.9   3rd Qu.: 4602.4  \n Max.   :70462   Max.   :69881.1   Max.   :62802.7   Max.   :55949.9  \n      2012              2011              2010              2009        \n Min.   :   43.5   Min.   :   27.2   Min.   :    4.7   Min.   :    0.0  \n 1st Qu.:  202.4   1st Qu.:  193.8   1st Qu.:  146.8   1st Qu.:  127.5  \n Median :  809.0   Median :  746.0   Median :  599.6   Median :  514.7  \n Mean   : 4044.2   Mean   : 3608.9   Mean   : 3310.0   Mean   : 2888.4  \n 3rd Qu.: 3777.1   3rd Qu.: 3442.4   3rd Qu.: 2625.2   3rd Qu.: 2342.0  \n Max.   :50496.3   Max.   :48318.2   Max.   :46254.5   Max.   :40325.3  \n      2008              2007              2006              2005         \n Min.   :    4.5   Min.   :    1.8   Min.   :    0.0   Min.   :    0.00  \n 1st Qu.:  152.4   1st Qu.:  112.6   1st Qu.:  114.9   1st Qu.:   72.92  \n Median :  549.5   Median :  441.4   Median :  354.6   Median :  321.20  \n Mean   : 3020.2   Mean   : 2591.2   Mean   : 2250.5   Mean   : 1878.82  \n 3rd Qu.: 2357.7   3rd Qu.: 1896.8   3rd Qu.: 1785.8   3rd Qu.: 1518.40  \n Max.   :43935.5   Max.   :39789.5   Max.   :33098.0   Max.   :28070.40  \n      2004               2003               2002               2001         \n Min.   :    0.00   Min.   :    0.00   Min.   :    0.00   Min.   :    0.00  \n 1st Qu.:   55.27   1st Qu.:   34.12   1st Qu.:   31.05   1st Qu.:   22.55  \n Median :  257.65   Median :  195.60   Median :  127.45   Median :  107.30  \n Mean   : 1699.77   Mean   : 1350.57   Mean   : 1132.43   Mean   : 1019.79  \n 3rd Qu.: 1530.25   3rd Qu.: 1140.42   3rd Qu.:  890.12   3rd Qu.:  767.73  \n Max.   :25457.10   Max.   :18942.00   Max.   :17741.70   Max.   :16226.50  \n      2000            category        \n Min.   :    0.00   Length:136        \n 1st Qu.:   19.18   Class :character  \n Median :   98.50   Mode  :character  \n Mean   :  962.32                     \n 3rd Qu.:  679.02                     \n Max.   :15816.90                     \n\n\n\n\n\n\nCode\ncombined_data[duplicated(combined_data),]\n\n\n# A tibble: 0 × 26\n# ℹ 26 variables: data_series &lt;chr&gt;, 2023 &lt;dbl&gt;, 2022 &lt;dbl&gt;, 2021 &lt;dbl&gt;,\n#   2020 &lt;dbl&gt;, 2019 &lt;dbl&gt;, 2018 &lt;dbl&gt;, 2017 &lt;dbl&gt;, 2016 &lt;dbl&gt;, 2015 &lt;dbl&gt;,\n#   2014 &lt;dbl&gt;, 2013 &lt;dbl&gt;, 2012 &lt;dbl&gt;, 2011 &lt;dbl&gt;, 2010 &lt;dbl&gt;, 2009 &lt;dbl&gt;,\n#   2008 &lt;dbl&gt;, 2007 &lt;dbl&gt;, 2006 &lt;dbl&gt;, 2005 &lt;dbl&gt;, 2004 &lt;dbl&gt;, 2003 &lt;dbl&gt;,\n#   2002 &lt;dbl&gt;, 2001 &lt;dbl&gt;, 2000 &lt;dbl&gt;, category &lt;chr&gt;\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nEnsure that there are no duplicated columns, if not will have to investigate further.\n\n\n\n\n\n\n\nCode\ncolSums(is.na(combined_data))\n\n\ndata_series        2023        2022        2021        2020        2019 \n          0           0           0           0           0           0 \n       2018        2017        2016        2015        2014        2013 \n          0           0           0           0           0           0 \n       2012        2011        2010        2009        2008        2007 \n          0           0           0           0           0           0 \n       2006        2005        2004        2003        2002        2001 \n          0           0           0           0           0           0 \n       2000    category \n          0           0 \n\n\n\n\n\n\n\n\nNote\n\n\n\n\nEnsure that there are no NA values, if not will have to investigate further.\nPossibility to use the replace_na() function to replace missing values with 0 in specified columns.\n\n\n\n\n\n\n\nCode\nstr(combined_data)\n\n\ntibble [136 × 26] (S3: tbl_df/tbl/data.frame)\n $ data_series: chr [1:136] \"Asia\" \"Bangladesh\" \"Brunei Darussalam\" \"Cambodia\" ...\n $ 2023       : num [1:136] 170788 986 960 476 21686 ...\n $ 2022       : num [1:136] 174243 904 891 390 21307 ...\n $ 2021       : num [1:136] 147126 675 579 256 18126 ...\n $ 2020       : num [1:136] 115989 582 533 250 15063 ...\n $ 2019       : num [1:136] 114573 642 556 259 11273 ...\n $ 2018       : num [1:136] 104183 664 496 290 10466 ...\n $ 2017       : num [1:136] 85093 548 464 227 8240 ...\n $ 2016       : num [1:136] 70462 483 420 196 7352 ...\n $ 2015       : num [1:136] 69881 424 465 114 7260 ...\n $ 2014       : num [1:136] 62802.7 384.3 520.4 86.3 5841.4 ...\n $ 2013       : num [1:136] 55949.9 347.5 474.4 89.2 5184.6 ...\n $ 2012       : num [1:136] 50496 349 464 115 5156 ...\n $ 2011       : num [1:136] 48318 334 468 109 5105 ...\n $ 2010       : num [1:136] 46254.5 272.6 508.1 78.4 4928.8 ...\n $ 2009       : num [1:136] 40325.3 209.3 324.4 99.3 4344.5 ...\n $ 2008       : num [1:136] 43936 265 273 140 4677 ...\n $ 2007       : num [1:136] 39789.5 284 248.3 88.1 4429.7 ...\n $ 2006       : num [1:136] 33098 286 121 111 3555 ...\n $ 2005       : num [1:136] 28070.4 274.6 123.3 77.1 2790.5 ...\n $ 2004       : num [1:136] 25457.1 188.9 138.6 68.7 2313.7 ...\n $ 2003       : num [1:136] 18942 157 151.4 45.3 2059.7 ...\n $ 2002       : num [1:136] 17741.7 118.3 144.3 37.9 1912 ...\n $ 2001       : num [1:136] 16226.5 121.8 97.8 29.8 1866.5 ...\n $ 2000       : num [1:136] 15816.9 89.7 100.5 18.4 1860.6 ...\n $ category   : chr [1:136] \"Exports\" \"Exports\" \"Exports\" \"Exports\" ...\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nEnsure that all variables are correctly classified by data type; recast variable types if needed.\nVariables are correctly classified - where categorical variables are classified as character, while continuous variables are classified as double.\n\n\n\n\n\n\n\n\n\nA new “category” column will be added to classify each trading partner as a Country, Region, or Economic & Political Union. This helps organize the data, making it easier to analyze trade patterns across different entity types.\n\n\n\n\n\n\n\ntypes\ndata_series\n\n\n\n\nCountry\nBangladesh, Brunei Darussalam, Cambodia, Hong Kong, India, Indonesia, Israel, Japan, Kuwait, Mainland China, Malaysia, Myanmar, Pakistan, Philippines, Qatar, Republic Of Korea, Saudi Arabia, Sri Lanka, Taiwan Thailand,Turkiye United Arab Emirates, Vietnam, Belgium, Cyprus Denmark, Finland France Germany, Greece Ireland Italy, Luxembourg, Netherlands, Norway, Portugal, Russian Federation, Spain, Sweden, Switzerland, United Kingdom, United States Of America, Canada, Australia, Marshall Islands, New Zealand, Papua New Guinea, Bermuda, Brazil, British Virgin Islands, Cayman Islands, Chile, Mexico Panama, Peru, Egypt, Liberia, Mauritius, Nigeria South Africa\n\n\nRegion\nAsia, Europe, North America, Oceania, South And Central America And The Caribbean, Africa\n\n\nEconomic & Political Union\nASEAN, European Union (EU-27)\n\n\n\n\n\nCode\n# Load the categorization mapping from Excel\ncategorization_mapping &lt;- read_excel(\"data/categorizationmapping.xlsx\")\n\n# Ensure column names are consistent\ncolnames(categorization_mapping) &lt;- tolower(gsub(\" \", \"_\", colnames(categorization_mapping)))\n\n# Merge only the type into combined_data\ncombined_data &lt;- combined_data %&gt;% left_join(categorization_mapping, by = \"data_series\")\n\n\n\n\n\nTo calculate the total trade values for each trading partner, the dataset was grouped by “data_series”, and trade values from 2000 to 2023 were summed using summarise(), ensuring missing values were ignored with na.rm = TRUE. A “category” column was assigned the value “Total” to indicate aggregated trade figures. The computed totals were then appended to the original dataset using bind_rows().\n\n\nCode\n# Summing up the values for each trading partner across all years (2000-2023)\nsummed_data &lt;- combined_data %&gt;%\n    group_by(data_series, types) %&gt;%\n    summarise(across(matches(\"^\\\\d{4}$\"), sum, na.rm = TRUE), .groups = \"drop\") %&gt;%\n    mutate(category = \"Total\")\n\n# Add the total category to the combined dataset\nfinal_data &lt;- bind_rows(combined_data, summed_data)\n\n# Reorder columns to move category to the second position\nfinal_data &lt;- final_data %&gt;% select(data_series, category, types, everything())\n\n# Display a preview of the merged data\nprint(head(final_data))\n\n\n# A tibble: 6 × 27\n  data_series    category types `2023` `2022` `2021` `2020` `2019` `2018` `2017`\n  &lt;chr&gt;          &lt;chr&gt;    &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Asia           Exports  Regi… 1.71e5 1.74e5 1.47e5 1.16e5 1.15e5 1.04e5 85093.\n2 Bangladesh     Exports  Coun… 9.86e2 9.04e2 6.75e2 5.82e2 6.42e2 6.64e2   548.\n3 Brunei Daruss… Exports  Coun… 9.60e2 8.91e2 5.79e2 5.33e2 5.56e2 4.96e2   464.\n4 Cambodia       Exports  Coun… 4.76e2 3.89e2 2.56e2 2.50e2 2.59e2 2.90e2   227.\n5 Hong Kong      Exports  Coun… 2.17e4 2.13e4 1.81e4 1.51e4 1.13e4 1.05e4  8240.\n6 India          Exports  Coun… 9.91e3 9.73e3 7.82e3 6.34e3 6.38e3 5.96e3  5326.\n# ℹ 17 more variables: `2016` &lt;dbl&gt;, `2015` &lt;dbl&gt;, `2014` &lt;dbl&gt;, `2013` &lt;dbl&gt;,\n#   `2012` &lt;dbl&gt;, `2011` &lt;dbl&gt;, `2010` &lt;dbl&gt;, `2009` &lt;dbl&gt;, `2008` &lt;dbl&gt;,\n#   `2007` &lt;dbl&gt;, `2006` &lt;dbl&gt;, `2005` &lt;dbl&gt;, `2004` &lt;dbl&gt;, `2003` &lt;dbl&gt;,\n#   `2002` &lt;dbl&gt;, `2001` &lt;dbl&gt;, `2000` &lt;dbl&gt;\n\n\nCode\n# library(openxlsx)\n# \n# write.xlsx(final_data, \"final_data.xlsx\")\n\n\n\n\n\n\nKey makeover changes:\n1️⃣ Breaking down trade data by types - e.g.: countries/ regions/ economic & political union\n\nOriginal visualization grouped countries, regions, economic & political unions making trade relationships harder to interpret.\nRevised visualization will categorize data into different types, ensuring clarity.\n\n2️⃣ Pie chart for region-level aggregation:\n\nPie chart groups trade volumes by region –&gt;Provide a high-level trade distribution which allow users to quickly assess which regions contribute the most to total trade.\n\n\n\n\n\n\n\nNote\n\n\n\n\nEasier comparison –&gt; Highlights contributions from countries, regions, and unions seperately.\nPie chart only showcase the Major Trading Partners for Trade in Services 2023 - Total Trade Volume Distribution by Region\n\n\n\n\nGraph()Code()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(viridis)\nlibrary(ggrepel)\n\n# Filter the trade data for regions\nregion_trade_data &lt;- final_data %&gt;%\n  filter(types == \"Region\", category == \"Total\") %&gt;%\n  select(region = data_series, trade_value = `2023`)\n\n# Ensure correct ordering based on trade value\nregion_trade_data &lt;- region_trade_data %&gt;%\n  arrange(desc(trade_value))\n\n# Compute cumulative sum for positioning\nregion_trade_data &lt;- region_trade_data %&gt;%\n  mutate(label = paste0(region, \"\\n$\", format(trade_value, big.mark = \",\")),\n         pos = cumsum(trade_value) - (0.5 * trade_value))  \n\n# Create Static Pie Chart with Labels Outside\nstatic_pie_chart &lt;- ggplot(region_trade_data, aes(x = \"\", y = trade_value, fill = trade_value)) +\n  geom_bar(stat = \"identity\", width = 1, color = \"white\") +  \n  coord_polar(theta = \"y\") +\n  scale_fill_viridis(option = \"viridis\", direction = -1) +  \n  geom_text_repel(aes(y = pos, label = label), size = 3, color = \"black\", nudge_x = 1, box.padding = 0.5) +  \n  theme_void() +\n  labs(\n    title = \"Major Trading Partners for Trade in Services, 2023\",\n    subtitle = \"Total Trade Volume Distribution by Region\",\n    fill = \"Trade Value (SGD)\"\n  ) +\n  theme(\n    legend.position = \"right\",\n    plot.title = element_text(hjust = 0, face = \"bold\", size = 14),  # Centered, bold title\n    plot.subtitle = element_text(hjust = 0, size = 12)  # Centered subtitle\n  )\n\n# Show the static pie chart\nprint(static_pie_chart)\n\n\n\n\n\n\n\n\nKey makeover changes:\n1️⃣ Enhanced trade value representation with a geo-spatial context -\n\nOriginal uses static infographics with flag-based indicators, the revised visualization presents an intecative world map, allowing for dynamic exploration.\nColor gradient (from yellow to purple) effectively communciates the trade value inensity in SGD\n\n\nGraph()Code()\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\nlibrary(sf)\nlibrary(viridis)\nlibrary(plotly)\n\n# Load world map data\nworld &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\n\n# Filter the trade data (assuming it's already in your R environment as `final_data`)\ntrade_data &lt;- final_data %&gt;%\n  filter(types == \"Country\", category == \"Total\") %&gt;%\n  select(region = data_series, trade_value = `2023`)\n\n# Standardize region names\ntrade_data$region &lt;- tolower(trimws(trade_data$region))\nworld$name &lt;- tolower(trimws(world$name))\n\n# Correct mismatched country names\ntrade_data$region &lt;- recode(trade_data$region,\n                             \"mainland china\" = \"china\",\n                             \"republic of korea\" = \"south korea\",\n                             \"russian federation\" = \"russia\",\n                             \"turkiye\" = \"turkey\",\n                             \"british virgin islands\" = \"u.s. virgin is.\",\n                             \"cayman islands\" = \"cayman is.\",\n                             \"brunei darussalam\" = \"brunei\",\n                             \"marshall islands\" = \"marshall is.\")\n\n# Merge world map with trade data\nworld &lt;- world %&gt;%\n  left_join(trade_data, by = c(\"name\" = \"region\"))\n\n# Create a ggplot object with title and subtitle\nggplot_map &lt;- ggplot(data = world) +\n  geom_sf(aes(fill = trade_value, text = paste(\"Country:\", name, \"&lt;br&gt;Trade Value: $\", trade_value)), \n          color = \"black\", size = 0.2) +\n  scale_fill_viridis(option = \"viridis\", direction = -1, na.value = \"gray90\") +\n  theme_minimal() +\n  labs(\n    title = \"Major Trading Partners for Trade in Services, 2023\",\n    subtitle = \"Total Trade Value Distribution by Countries\",\n    fill = \"Trade Value (SGD)\"\n  ) +\n  theme(\n    plot.title = element_text(hjust = 0, face = \"bold\", size = 14), \n    plot.subtitle = element_text(hjust = 0, size = 12)  \n  )\n\n# Convert to interactive Plotly map and manually add subtitle\nplotly_map &lt;- ggplotly(ggplot_map, tooltip = \"text\") %&gt;%\n  layout(\n    annotations = list(\n      list(\n        text = \"&lt;b&gt;Total Trade Value Distribution by Countries&lt;/b&gt;\",\n        x = 0,  # Left align\n        y = 1.03,  # Position above plot\n        xref = \"paper\",\n        yref = \"paper\",\n        showarrow = FALSE,\n        font = list(size = 12)\n      )\n    )\n  )\n\n# Show interactive map\nplotly_map\n\n\n\n\n\n\n\n\n\nKey observations:\n\nTotal trade vol()\n\nTop 5 trade partners: United States, Japan, Mainland China, Australia, and United Kingdom\nTrends: Significant growth post-2010, with accelerated trade activities from 2018 onward, with Asia and North America dominating trade volume.\n\nExports()\n\nTop 5 export destinations: United States, Japan, Australia, Mainland China, and United Kingdom\n\nImports()\n\nTop 5 import destinations: United States, Mainland China, Japan, United Kingdom, and Hong Kong.\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nUnited States, Mainland China, Japan, and Australia consistently reman Singapore’s top trade partners in services\nExports and imports have surged post-2018, reinforcing Singapore’s role as a global trade hub\nDigitalization and financial sector growth are key factors shaping the country trade landscape\n\n\n\n\nTotal trade vol()Export()Import()Code()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(viridis)\n\n# Filter trade data for countries\ntrade_data &lt;- final_data %&gt;%\n  filter(types == \"Country\", category == \"Imports\") %&gt;%\n  select(country = data_series, `2000`:`2023`)  # Select years from 2000 to 2023\n\n# Reshape data from wide to long format\ntrade_data_long &lt;- trade_data %&gt;%\n  pivot_longer(cols = `2000`:`2023`, names_to = \"year\", values_to = \"trade_value\") %&gt;%\n  mutate(year = as.numeric(year))  # Convert year to numeric\n\n# Calculate total trade value per country and reorder factors (highest at the top)\ntrade_data_long &lt;- trade_data_long %&gt;%\n  group_by(country) %&gt;%\n  mutate(total_trade_value = sum(trade_value, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(country = fct_reorder(country, total_trade_value, .desc = TRUE))  \n\n# Create heatmap-style plot using geom_tile()\nhori_plot &lt;- ggplot(trade_data_long, aes(x = year, y = country, fill = trade_value)) +\n  geom_tile(color = \"white\") +  # Add white borders between tiles\n  scale_fill_viridis_c(option = \"magma\", na.value = \"gray90\") +  # Color gradient for intensity\n  labs(\n    title = \"Total Trade Value Trends (2000-202f3)\",\n    subtitle = \"Major Trading Partners for Trade in Services\",\n    x = \"Year\",\n    y = \"Country\",\n    fill = \"Trade Value (SGD)\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.y = element_text(size = 8),  # Adjust country label size\n    axis.text.x = element_text(angle = 45, hjust = 1),  # Rotate year labels\n    plot.title = element_text(hjust = 0, face = \"bold\", size = 14),  # Left-aligned title\n    plot.subtitle = element_text(hjust = 0, size = 12)  # Left-aligned subtitle\n  )\n\n# Show plot\nprint(hori_plot)\n\n\n\n\n\n\n\n\nIn the below code, we will explore the tidymodels approach in time series forecasting.\n\n\n\n\nCode\n# Load all necessary packages\nlibrary(tidyverse)\nlibrary(modeltime)\nlibrary(timetk)\nlibrary(lubridate)\nlibrary(rsample)\nlibrary(parsnip)\nlibrary(recipes)\nlibrary(workflows)\nlibrary(yardstick)\n\n# Convert wide format to long format\nlong_data &lt;- final_data %&gt;%\n    pivot_longer(cols = matches(\"^\\\\d{4}$\"), names_to = \"year\", values_to = \"value\") %&gt;%\n    mutate(year = as.integer(year)) \n\n# Convert year to Date format\nlong_data &lt;- long_data %&gt;%\n    mutate(date = make_date(year = year, month = 1, day = 1)) %&gt;%\n    select(data_series, category, types, date, value)\n\n# Filter for a specific country and only \"Exports\"\nselected_series &lt;- long_data %&gt;%\n    filter(data_series == \"United States Of America\", category == \"Exports\") %&gt;%\n    select(date, value) %&gt;%\n    arrange(date)\n\n# Remove duplicates if any\nselected_series &lt;- selected_series %&gt;%\n    distinct(date, .keep_all = TRUE)\n\n# Perform a time-based split (80% training, 20% testing)\nsplits &lt;- initial_time_split(selected_series, prop = 0.8)\n\ncat(\"The training dataset contains\", nrow(training(splits)), \"observations.\\n\")\n\n\nThe training dataset contains 19 observations.\n\n\nCode\ncat(\"The testing dataset consists of\", nrow(testing(splits)), \"observations.\\n\")\n\n\nThe testing dataset consists of 5 observations.\n\n\n\n\n\nIn the code below, we will fit four models: - Error-Trend-Season (ETS) model by using exp_smoothing() - Auto ARIMA model by using arima_reg() - Boosted Auto ARIMA by using arima_boost() - Prophet model by using prophet_reg()\n\n\nCode\nmodel_fit_ets &lt;- exp_smoothing() %&gt;%\n    set_engine(\"ets\") %&gt;%\n    fit(value ~ date, data = training(splits))\n\nmodel_fit_arima &lt;- arima_reg() %&gt;%\n    set_engine(\"auto_arima\") %&gt;%\n    fit(value ~ date, data = training(splits))\n\nmodel_fit_arima_boosted &lt;- arima_boost(\n  min_n = 2,\n  learn_rate = 0.015) %&gt;%\n  set_engine(\"auto_arima_xgboost\") %&gt;%\n  fit(value ~ date, data = training(splits))\n\nmodel_fit_prophet &lt;- prophet_reg() %&gt;%\n    set_engine(\"prophet\") %&gt;%\n    fit(value ~ date, data = training(splits))\n\n\n\n\n\nNext, we will use modeltime_table of modeltime package to add each of the models to a Modeltime Table.\n\n\nCode\nmodels_tbl &lt;- modeltime_table(\n    model_fit_ets,\n    model_fit_arima,\n    model_fit_arima_boosted,\n    model_fit_prophet\n)\n\nprint(models_tbl)\n\n\n# Modeltime Table\n# A tibble: 4 × 3\n  .model_id .model   .model_desc            \n      &lt;int&gt; &lt;list&gt;   &lt;chr&gt;                  \n1         1 &lt;fit[+]&gt; ETS(M,A,N)             \n2         2 &lt;fit[+]&gt; ARIMA(0,1,0) WITH DRIFT\n3         3 &lt;fit[+]&gt; ARIMA(0,1,0) WITH DRIFT\n4         4 &lt;fit[+]&gt; PROPHET                \n\n\n\n\n\nWe will then use the modeltime_calibrate() to add a new column called .calibrate_data into the newly created models_tbl tibble data table.\n\n\nCode\ncalibration_tbl &lt;- models_tbl %&gt;%\n    modeltime_calibrate(new_data = testing(splits))\n\nprint(calibration_tbl)\n\n\n# Modeltime Table\n# A tibble: 4 × 5\n  .model_id .model   .model_desc             .type .calibration_data\n      &lt;int&gt; &lt;list&gt;   &lt;chr&gt;                   &lt;chr&gt; &lt;list&gt;           \n1         1 &lt;fit[+]&gt; ETS(M,A,N)              Test  &lt;tibble [5 × 4]&gt; \n2         2 &lt;fit[+]&gt; ARIMA(0,1,0) WITH DRIFT Test  &lt;tibble [5 × 4]&gt; \n3         3 &lt;fit[+]&gt; ARIMA(0,1,0) WITH DRIFT Test  &lt;tibble [5 × 4]&gt; \n4         4 &lt;fit[+]&gt; PROPHET                 Test  &lt;tibble [5 × 4]&gt; \n\n\n\n\n\nWe will use two way to assess the accuracy of the models - by (1) means of accuracy metrics, (2) visualization\n\n\nmodeltime_accuracy() of modeltime package is used compute the accuracy metrics. Then, table_modeltime_accuracy() is used to present the accuracy metrics in tabular form.\n\n\nCode\ncalibration_tbl %&gt;%\n  modeltime_accuracy() %&gt;%\n  table_modeltime_accuracy(.interactive = FALSE)\n\n\n\n\n\n\n\n\nAccuracy Table\n\n\n.model_id\n.model_desc\n.type\nmae\nmape\nmase\nsmape\nrmse\nrsq\n\n\n\n\n1\nETS(M,A,N)\nTest\n16181.62\n31.88\n1.49\n40.03\n19413.03\n0.73\n\n\n2\nARIMA(0,1,0) WITH DRIFT\nTest\n13059.38\n24.56\n1.20\n30.18\n16738.06\n0.73\n\n\n3\nARIMA(0,1,0) WITH DRIFT\nTest\n13059.38\n24.56\n1.20\n30.18\n16738.06\n0.73\n\n\n4\nPROPHET\nTest\n16031.61\n31.46\n1.48\n39.53\n19329.48\n0.61\n\n\n\n\n\n\n\n\n\n\nWe can also use the interactive plotly visualization to assess the accuracy of the models.\n\n\nCode\ncalibration_tbl %&gt;%\n    modeltime_forecast(new_data = testing(splits), actual_data = selected_series) %&gt;%\n    plot_modeltime_forecast()\n\n\n\n\n\n\n\n\n\n\nNext, we refit the models to the full dataset using modeltime_refit() and forecast them forward.We can use modeltime_refit() to refit the forecasting models with the full data.\nThen, modeltime_forecast() is used to forecast to a selected future time period, in this example 10 years.\n\n\nCode\nrefit_tbl &lt;- calibration_tbl %&gt;%\n    modeltime_refit(data = selected_series)  # Now trained on full dataset\n\nforecast_tbl &lt;- refit_tbl %&gt;%\n    modeltime_forecast(\n        h = 10,  # Forecast for 10 years\n        actual_data = selected_series,\n        keep_data = TRUE  # Keep historical data for better visualization\n    )\nforecast_tbl %&gt;%\n    plot_modeltime_forecast(\n        .legend_max_width = 25, \n        .interactive = TRUE,\n        .plotly_slider = TRUE\n    )\n\n\n\n\n\n\n\n\n\n\n\nCode\nrefit_tbl %&gt;%\n    modeltime_accuracy() %&gt;%\n    table_modeltime_accuracy(.interactive = FALSE)\n\n\n\n\n\n\n\n\nAccuracy Table\n\n\n.model_id\n.model_desc\n.type\nmae\nmape\nmase\nsmape\nrmse\nrsq\n\n\n\n\n1\nETS(M,A,N)\nTest\n16181.62\n31.88\n1.49\n40.03\n19413.03\n0.73\n\n\n2\nUPDATE: ARIMA(0,1,0)(0,0,1)[5] WITH DRIFT\nTest\n13059.38\n24.56\n1.20\n30.18\n16738.06\n0.73\n\n\n3\nUPDATE: ARIMA(0,1,0)(0,0,1)[5] WITH DRIFT\nTest\n13059.38\n24.56\n1.20\n30.18\n16738.06\n0.73\n\n\n4\nPROPHET\nTest\n16031.61\n31.46\n1.48\n39.53\n19329.48\n0.61"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#exploratory-data-analysis-eda-time-series-data",
    "href": "In-class_Ex/In-class_Ex07.html#exploratory-data-analysis-eda-time-series-data",
    "title": "In-class Exercise 07",
    "section": "Exploratory Data Analysis (EDA): Time Series Data",
    "text": "Exploratory Data Analysis (EDA): Time Series Data\nBefore fitting forecasting models, it is a good practice to analysis the time series data by using EDA methods.\n\n\nCode\nvietnam_train %&gt;%\n  model(stl = STL(Arrivals)) %&gt;%\n  components() %&gt;%\n  autoplot()"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#fitting-forecasting-models",
    "href": "In-class_Ex/In-class_Ex07.html#fitting-forecasting-models",
    "title": "In-class Exercise 07",
    "section": "Fitting forecasting models",
    "text": "Fitting forecasting models"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#fitting-exponential-smoothing-state-space-ets-models-fable-methods",
    "href": "In-class_Ex/In-class_Ex07.html#fitting-exponential-smoothing-state-space-ets-models-fable-methods",
    "title": "In-class Exercise 07",
    "section": "Fitting Exponential Smoothing State Space (ETS) Models: fable methods",
    "text": "Fitting Exponential Smoothing State Space (ETS) Models: fable methods\nIn fable, Exponential Smoothing State Space Models are supported by ETS(). The combinations are specified through the formula:\n\n\nCode\nETS(y ~ error(\"A\") \n    + trend(\"N\") \n    + season(\"N\"))\n\n\n&lt;ETS model definition&gt;"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#fitting-a-simple-exponential-smoothing-ses",
    "href": "In-class_Ex/In-class_Ex07.html#fitting-a-simple-exponential-smoothing-ses",
    "title": "In-class Exercise 07",
    "section": "Fitting a simple exponential smoothing (SES)",
    "text": "Fitting a simple exponential smoothing (SES)\n\n\nCode\nfit_ses &lt;- vietnam_train %&gt;%\n  model(ETS(Arrivals ~ error(\"A\") \n            + trend(\"N\") \n            + season(\"N\")))\nfit_ses\n\n\n# A mable: 1 x 2\n# Key:     Country [1]\n  Country `ETS(Arrivals ~ error(\"A\") + trend(\"N\") + season(\"N\"))`\n  &lt;chr&gt;                                                   &lt;model&gt;\n1 Vietnam                                            &lt;ETS(A,N,N)&gt;\n\n\nNotice that model() of fable package is used to estimate the ETS model on a particular dataset, and returns a mable (model table) object.\nA mable contains a row for each time series (uniquely identified by the key variables), and a column for each model specification. A model is contained within the cells of each model column."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#examine-model-assumptions",
    "href": "In-class_Ex/In-class_Ex07.html#examine-model-assumptions",
    "title": "In-class Exercise 07",
    "section": "Examine Model Assumptions",
    "text": "Examine Model Assumptions\nNext, gg_tsresiduals() of feasts package is used to check the model assumptions with residuals plots.\n\n\nCode\ngg_tsresiduals(fit_ses)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#the-model-details",
    "href": "In-class_Ex/In-class_Ex07.html#the-model-details",
    "title": "In-class Exercise 07",
    "section": "The model details",
    "text": "The model details\nreport() of fabletools is be used to reveal the model details.\n\n\nCode\nfit_ses %&gt;%\n  report()\n\n\nSeries: Arrivals \nModel: ETS(A,N,N) \n  Smoothing parameters:\n    alpha = 0.9998995 \n\n  Initial states:\n     l[0]\n 10312.99\n\n  sigma^2:  27939164\n\n     AIC     AICc      BIC \n2911.726 2911.913 2920.374"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#fitting-ets-methods-with-trend-holts-linear",
    "href": "In-class_Ex/In-class_Ex07.html#fitting-ets-methods-with-trend-holts-linear",
    "title": "In-class Exercise 07",
    "section": "Fitting ETS Methods with Trend: Holt’s Linear",
    "text": "Fitting ETS Methods with Trend: Holt’s Linear"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#trend-methods",
    "href": "In-class_Ex/In-class_Ex07.html#trend-methods",
    "title": "In-class Exercise 07",
    "section": "Trend methods",
    "text": "Trend methods\n\n\nCode\nvietnam_H &lt;- vietnam_train %&gt;%\n  model(`Holt's method` = \n          ETS(Arrivals ~ error(\"A\") +\n                trend(\"A\") + \n                season(\"N\")))\nvietnam_H %&gt;% report()\n\n\nSeries: Arrivals \nModel: ETS(A,A,N) \n  Smoothing parameters:\n    alpha = 0.9998995 \n    beta  = 0.0001004625 \n\n  Initial states:\n     l[0]     b[0]\n 13673.29 525.8859\n\n  sigma^2:  28584805\n\n     AIC     AICc      BIC \n2916.695 2917.171 2931.109"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#damped-trend-methods",
    "href": "In-class_Ex/In-class_Ex07.html#damped-trend-methods",
    "title": "In-class Exercise 07",
    "section": "Damped Trend methods",
    "text": "Damped Trend methods\n\n\nCode\nvietnam_HAd &lt;- vietnam_train %&gt;%\n  model(`Holt's method` = \n          ETS(Arrivals ~ error(\"A\") +\n                trend(\"Ad\") + \n                season(\"N\")))\nvietnam_HAd %&gt;% report()\n\n\nSeries: Arrivals \nModel: ETS(A,Ad,N) \n  Smoothing parameters:\n    alpha = 0.9998999 \n    beta  = 0.0001098602 \n    phi   = 0.9784562 \n\n  Initial states:\n     l[0]   b[0]\n 13257.28 523.54\n\n  sigma^2:  28641536\n\n     AIC     AICc      BIC \n2917.921 2918.593 2935.218"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#checking-for-results",
    "href": "In-class_Ex/In-class_Ex07.html#checking-for-results",
    "title": "In-class Exercise 07",
    "section": "Checking for results",
    "text": "Checking for results\nCheck the model assumptions with residuals plots.\n\n\nCode\ngg_tsresiduals(vietnam_H)\n\n\n\n\n\n\n\n\n\n\n\nCode\ngg_tsresiduals(vietnam_HAd)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#fitting-ets-methods-with-season-holt-winters",
    "href": "In-class_Ex/In-class_Ex07.html#fitting-ets-methods-with-season-holt-winters",
    "title": "In-class Exercise 07",
    "section": "Fitting ETS Methods with Season: Holt-Winters",
    "text": "Fitting ETS Methods with Season: Holt-Winters\n\n\nCode\nVietnam_WH &lt;- vietnam_train %&gt;%\n  model(\n    Additive = ETS(Arrivals ~ error(\"A\") \n                   + trend(\"A\") \n                   + season(\"A\")),\n    Multiplicative = ETS(Arrivals ~ error(\"M\") \n                         + trend(\"A\") \n                         + season(\"M\"))\n    )\n\nVietnam_WH %&gt;% report()\n\n\n# A tibble: 2 × 10\n  Country .model          sigma2 log_lik   AIC  AICc   BIC    MSE   AMSE     MAE\n  &lt;chr&gt;   &lt;chr&gt;            &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 Vietnam Additive       5.33e+6  -1336. 2706. 2711. 2755. 4.68e6 8.56e6 1.72e+3\n2 Vietnam Multiplicative 4.55e-3  -1300. 2635. 2640. 2684. 3.05e6 3.42e6 5.20e-2"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#fitting-multiple-ets-models",
    "href": "In-class_Ex/In-class_Ex07.html#fitting-multiple-ets-models",
    "title": "In-class Exercise 07",
    "section": "Fitting multiple ETS Models",
    "text": "Fitting multiple ETS Models\n\n\nCode\nfit_ETS &lt;- vietnam_train %&gt;%\n  model(`SES` = ETS(Arrivals ~ error(\"A\") + \n                      trend(\"N\") + \n                      season(\"N\")),\n        `Holt`= ETS(Arrivals ~ error(\"A\") +\n                      trend(\"A\") +\n                      season(\"N\")),\n        `damped Holt` = \n          ETS(Arrivals ~ error(\"A\") +\n                trend(\"Ad\") + \n                season(\"N\")),\n        `WH_A` = ETS(\n          Arrivals ~ error(\"A\") + \n            trend(\"A\") + \n            season(\"A\")),\n        `WH_M` = ETS(Arrivals ~ error(\"M\") \n                         + trend(\"A\") \n                         + season(\"M\"))\n  )"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#the-model-coefficient",
    "href": "In-class_Ex/In-class_Ex07.html#the-model-coefficient",
    "title": "In-class Exercise 07",
    "section": "The model coefficient",
    "text": "The model coefficient\ntidy() of fabletools is be used to extract model coefficients from a mable.\n\n\nCode\nfit_ETS %&gt;%\n  tidy()\n\n\n# A tibble: 45 × 4\n   Country .model      term      estimate\n   &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;        &lt;dbl&gt;\n 1 Vietnam SES         alpha     1.00    \n 2 Vietnam SES         l[0]  10313.      \n 3 Vietnam Holt        alpha     1.00    \n 4 Vietnam Holt        beta      0.000100\n 5 Vietnam Holt        l[0]  13673.      \n 6 Vietnam Holt        b[0]    526.      \n 7 Vietnam damped Holt alpha     1.00    \n 8 Vietnam damped Holt beta      0.000110\n 9 Vietnam damped Holt phi       0.978   \n10 Vietnam damped Holt l[0]  13257.      \n# ℹ 35 more rows"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#step-4-model-comparison",
    "href": "In-class_Ex/In-class_Ex07.html#step-4-model-comparison",
    "title": "In-class Exercise 07",
    "section": "Step 4: Model Comparison",
    "text": "Step 4: Model Comparison\nglance() of fabletool\n\n\nCode\nfit_ETS %&gt;% \n  report()\n\n\n# A tibble: 5 × 10\n  Country .model       sigma2 log_lik   AIC  AICc   BIC       MSE   AMSE     MAE\n  &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 Vietnam SES         2.79e+7  -1453. 2912. 2912. 2920. 27515844. 5.99e7 3.91e+3\n2 Vietnam Holt        2.86e+7  -1453. 2917. 2917. 2931. 27718599. 6.03e7 3.92e+3\n3 Vietnam damped Holt 2.86e+7  -1453. 2918. 2919. 2935. 27556629. 5.97e7 3.92e+3\n4 Vietnam WH_A        5.33e+6  -1336. 2706. 2711. 2755.  4684271. 8.56e6 1.72e+3\n5 Vietnam WH_M        4.55e-3  -1300. 2635. 2640. 2684.  3046059. 3.42e6 5.20e-2"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#step-5-forecasting-future-values",
    "href": "In-class_Ex/In-class_Ex07.html#step-5-forecasting-future-values",
    "title": "In-class Exercise 07",
    "section": "Step 5: Forecasting future values",
    "text": "Step 5: Forecasting future values\nTo forecast the future values, forecast() of fable will be used. Notice that the forecast period is 12 months.\n\n\nCode\nfit_ETS %&gt;%\n  forecast(h = \"12 months\") %&gt;%\n  autoplot(vietnam_ts, \n           level = NULL)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#fitting-ets-automatically",
    "href": "In-class_Ex/In-class_Ex07.html#fitting-ets-automatically",
    "title": "In-class Exercise 07",
    "section": "Fitting ETS Automatically",
    "text": "Fitting ETS Automatically\n\n\nCode\nfit_autoETS &lt;- vietnam_train %&gt;%\n  model(ETS(Arrivals))\nfit_autoETS %&gt;% report()\n\n\nSeries: Arrivals \nModel: ETS(M,A,M) \n  Smoothing parameters:\n    alpha = 0.1613503 \n    beta  = 0.0001021811 \n    gamma = 0.0001030996 \n\n  Initial states:\n     l[0]     b[0]      s[0]     s[-1]     s[-2]     s[-3]    s[-4]    s[-5]\n 15001.12 212.3552 0.9167302 0.8311728 0.8739287 0.8690543 1.104668 1.485584\n    s[-6]     s[-7]    s[-8]     s[-9]    s[-10]    s[-11]\n 1.311207 0.9917759 1.014187 0.8973028 0.8816768 0.8227129\n\n  sigma^2:  0.0046\n\n     AIC     AICc      BIC \n2634.751 2640.119 2683.759"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#fitting-fitting-ets-automatically",
    "href": "In-class_Ex/In-class_Ex07.html#fitting-fitting-ets-automatically",
    "title": "In-class Exercise 07",
    "section": "Fitting Fitting ETS Automatically",
    "text": "Fitting Fitting ETS Automatically\nNext, we will check the model assumptions with residuals plots by using gg_tsresiduals() of feasts package\n\n\nCode\ngg_tsresiduals(fit_autoETS)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#forecast-the-future-values",
    "href": "In-class_Ex/In-class_Ex07.html#forecast-the-future-values",
    "title": "In-class Exercise 07",
    "section": "Forecast the future values",
    "text": "Forecast the future values\nIn the code chunk below, forecast() of fable package is used to forecast the future values. Then, autoplot() of feasts package is used to see the training data along with the forecast values.\n\n\nCode\nfit_autoETS %&gt;%\n  forecast(h = \"12 months\") %&gt;%\n  autoplot(vietnam_train)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#visualising-autoets-model-with-ggplot2",
    "href": "In-class_Ex/In-class_Ex07.html#visualising-autoets-model-with-ggplot2",
    "title": "In-class Exercise 07",
    "section": "Visualising AutoETS model with ggplot2",
    "text": "Visualising AutoETS model with ggplot2\nThere are time that we are interested to visualise relationship between training data and fit data and forecasted values versus the hold-out data."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#visualising-autoets-model-with-ggplot2-1",
    "href": "In-class_Ex/In-class_Ex07.html#visualising-autoets-model-with-ggplot2-1",
    "title": "In-class Exercise 07",
    "section": "Visualising AutoETS model with ggplot2",
    "text": "Visualising AutoETS model with ggplot2\nCode chunk below is used to create the data visualisation in previous slide.\n\n\nCode\nfc_autoETS &lt;- fit_autoETS %&gt;%\n  forecast(h = \"12 months\")\n\nvietnam_ts %&gt;%\n  ggplot(aes(x=`Month`, \n             y=Arrivals)) +\n  autolayer(fc_autoETS, \n            alpha = 0.6) +\n  geom_line(aes(\n    color = Type), \n    alpha = 0.8) + \n  geom_line(aes(\n    y = .mean, \n    colour = \"Forecast\"), \n    data = fc_autoETS) +\n  geom_line(aes(\n    y = .fitted, \n    colour = \"Fitted\"), \n    data = augment(fit_autoETS))"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#autoregressive-integrated-moving-averagearima-methods-for-time-series-forecasting-fable-tidyverts-methods",
    "href": "In-class_Ex/In-class_Ex07.html#autoregressive-integrated-moving-averagearima-methods-for-time-series-forecasting-fable-tidyverts-methods",
    "title": "In-class Exercise 07",
    "section": "AutoRegressive Integrated Moving Average(ARIMA) Methods for Time Series Forecasting: fable (tidyverts) methods",
    "text": "AutoRegressive Integrated Moving Average(ARIMA) Methods for Time Series Forecasting: fable (tidyverts) methods"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#visualising-autocorrelations-feasts-methods",
    "href": "In-class_Ex/In-class_Ex07.html#visualising-autocorrelations-feasts-methods",
    "title": "In-class Exercise 07",
    "section": "Visualising Autocorrelations: feasts methods",
    "text": "Visualising Autocorrelations: feasts methods\nfeasts package provides a very handy function for visualising ACF and PACF of a time series called gg_tsdiaply().\n\n\nCode\nvietnam_train %&gt;%\n  gg_tsdisplay(plot_type='partial')"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#visualising-autocorrelations-feasts-methods-1",
    "href": "In-class_Ex/In-class_Ex07.html#visualising-autocorrelations-feasts-methods-1",
    "title": "In-class Exercise 07",
    "section": "Visualising Autocorrelations: feasts methods",
    "text": "Visualising Autocorrelations: feasts methods\n\n\nCode\ntsibble_longer %&gt;%\n  filter(`Country` == \"Vietnam\") %&gt;%\n  ACF(Arrivals) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\n\n\n\nCode\ntsibble_longer %&gt;%\n  filter(`Country` == \"United Kingdom\") %&gt;%\n  ACF(Arrivals) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\n\nBy comparing both ACF plots, it is clear that visitor arrivals from United Kingdom were very seasonal and relatively weaker trend as compare to visitor arrivals from Vietnam."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#differencing-fable-methods",
    "href": "In-class_Ex/In-class_Ex07.html#differencing-fable-methods",
    "title": "In-class Exercise 07",
    "section": "Differencing: fable methods",
    "text": "Differencing: fable methods"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#trend-differencing",
    "href": "In-class_Ex/In-class_Ex07.html#trend-differencing",
    "title": "In-class Exercise 07",
    "section": "Trend differencing",
    "text": "Trend differencing\n\n\nCode\ntsibble_longer %&gt;%\n  filter(Country == \"Vietnam\") %&gt;%\n  gg_tsdisplay(difference(\n    Arrivals,\n    lag = 1), \n    plot_type='partial')"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#seasonal-differencing",
    "href": "In-class_Ex/In-class_Ex07.html#seasonal-differencing",
    "title": "In-class Exercise 07",
    "section": "Seasonal differencing",
    "text": "Seasonal differencing\n\n\nCode\ntsibble_longer %&gt;%\n  filter(Country == \"Vietnam\") %&gt;%\n  gg_tsdisplay(difference(\n    Arrivals,\n    difference = 12), \n    plot_type='partial')\n\n\n\n\n\n\n\n\n\nThe PACF is suggestive of an AR(1) model; so an initial candidate model is an ARIMA(1,1,0). The ACF suggests an MA(1) model; so an alternative candidate is an ARIMA(0,1,1)."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#fitting-arima-models-manually-fable-methods",
    "href": "In-class_Ex/In-class_Ex07.html#fitting-arima-models-manually-fable-methods",
    "title": "In-class Exercise 07",
    "section": "Fitting ARIMA models manually: fable methods",
    "text": "Fitting ARIMA models manually: fable methods\n\n\nCode\nfit_arima &lt;- vietnam_train %&gt;%\n  model(\n    arima200 = ARIMA(Arrivals ~ pdq(2,0,0)),\n    sarima210 = ARIMA(Arrivals ~ pdq(2,0,0) + \n                        PDQ(2,1,0))\n    )\nreport(fit_arima)\n\n\n# A tibble: 2 × 9\n  Country .model      sigma2 log_lik   AIC  AICc   BIC ar_roots   ma_roots \n  &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt;     &lt;list&gt;   \n1 Vietnam arima200  4173906.  -1085. 2181. 2182. 2198. &lt;cpl [26]&gt; &lt;cpl [0]&gt;\n2 Vietnam sarima210 4173906.  -1085. 2181. 2182. 2198. &lt;cpl [26]&gt; &lt;cpl [0]&gt;"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#fitting-arima-models-automatically-fable-methods",
    "href": "In-class_Ex/In-class_Ex07.html#fitting-arima-models-automatically-fable-methods",
    "title": "In-class Exercise 07",
    "section": "Fitting ARIMA models automatically: fable methods",
    "text": "Fitting ARIMA models automatically: fable methods\n\n\nCode\nfit_autoARIMA &lt;- vietnam_train %&gt;%\n  model(ARIMA(Arrivals))\nreport(fit_autoARIMA)\n\n\nSeries: Arrivals \nModel: ARIMA(2,0,0)(2,1,0)[12] w/ drift \n\nCoefficients:\n         ar1     ar2     sar1     sar2   constant\n      0.4748  0.1892  -0.5723  -0.1578  1443.2068\ns.e.  0.0924  0.0903   0.0989   0.1032   188.9468\n\nsigma^2 estimated as 4173906:  log likelihood=-1084.6\nAIC=2181.19   AICc=2181.94   BIC=2197.92"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#model-comparison",
    "href": "In-class_Ex/In-class_Ex07.html#model-comparison",
    "title": "In-class Exercise 07",
    "section": "Model Comparison",
    "text": "Model Comparison\n\n\nCode\nbind_rows(\n    fit_autoARIMA %&gt;% accuracy(),\n    fit_autoETS %&gt;% accuracy(),\n    fit_autoARIMA %&gt;% \n      forecast(h = 12) %&gt;% \n      accuracy(vietnam_ts),\n    fit_autoETS %&gt;% \n      forecast(h = 12) %&gt;% \n      accuracy(vietnam_ts)) %&gt;%\n  select(-ME, -MPE, -ACF1)\n\n\n# A tibble: 4 × 8\n  Country .model          .type     RMSE   MAE  MAPE  MASE RMSSE\n  &lt;chr&gt;   &lt;chr&gt;           &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Vietnam ARIMA(Arrivals) Training 1907. 1458.  5.37 0.491 0.517\n2 Vietnam ETS(Arrivals)   Training 1745. 1386.  5.29 0.467 0.473\n3 Vietnam ARIMA(Arrivals) Test     2647. 2136.  5.17 0.720 0.717\n4 Vietnam ETS(Arrivals)   Test     3163. 2636.  6.64 0.889 0.857"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#forecast-multiple-time-series",
    "href": "In-class_Ex/In-class_Ex07.html#forecast-multiple-time-series",
    "title": "In-class Exercise 07",
    "section": "Forecast Multiple Time Series",
    "text": "Forecast Multiple Time Series\nIn this section, we will perform time series forecasting on multiple time series at one goal. For the purpose of the hand-on exercise, visitor arrivals from five selected ASEAN countries will be used.\nFirst, filter() is used to extract the selected countries’ data.\n\n\nCode\nASEAN &lt;- tsibble_longer %&gt;%\n  filter(Country == \"Vietnam\" |\n         Country == \"Malaysia\" |\n         Country == \"Indonesia\" |\n         Country == \"Thailand\" |\n         Country == \"Philippines\")\n\n\nNext, mutate() is used to create a new field called Type and populates their respective values. Lastly, filter() is used to extract the training data set and save it as a tsibble object called ASEAN_train.\n\n\nCode\nASEAN_train &lt;- ASEAN %&gt;%\n  mutate(Type = if_else(\n    `Month-Year` &gt;= \"2019-01-01\", \n    \"Hold-out\", \"Training\")) %&gt;%\n  filter(Type == \"Training\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#fitting-mulltiple-time-series",
    "href": "In-class_Ex/In-class_Ex07.html#fitting-mulltiple-time-series",
    "title": "In-class Exercise 07",
    "section": "Fitting Mulltiple Time Series",
    "text": "Fitting Mulltiple Time Series\nIn the code chunk below auto ETS and ARIMA models are fitted by using model().\n\n\nCode\nASEAN_fit &lt;- ASEAN_train %&gt;%\n  model(\n    ets = ETS(Arrivals),\n    arima = ARIMA(Arrivals)\n  )"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#examining-models",
    "href": "In-class_Ex/In-class_Ex07.html#examining-models",
    "title": "In-class Exercise 07",
    "section": "Examining Models",
    "text": "Examining Models\nThe glance() of fabletools provides a one-row summary of each model, and commonly includes descriptions of the model’s fit such as the residual variance and information criteria.\n\n\nCode\nASEAN_fit %&gt;%\n  glance()\n\n\n# A tibble: 10 × 12\n   Country     .model  sigma2 log_lik   AIC  AICc   BIC      MSE    AMSE     MAE\n   &lt;chr&gt;       &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 Indonesia   ets    1.02e-2  -1561. 3156. 3161. 3205.   1.74e8  1.80e8  0.0732\n 2 Indonesia   arima  1.48e+8  -1290. 2589. 2590. 2603.  NA      NA      NA     \n 3 Malaysia    ets    4.67e-3  -1430. 2894. 2899. 2943.   2.04e7  2.00e7  0.0506\n 4 Malaysia    arima  2.62e+7  -1185. 2378. 2379. 2390.  NA      NA      NA     \n 5 Philippines ets    3.56e-3  -1343. 2722. 2728. 2774.   5.28e6  7.58e6  0.0461\n 6 Philippines arima  8.04e+6  -1122. 2260. 2262. 2283.  NA      NA      NA     \n 7 Thailand    ets    6.63e-3  -1343. 2722. 2728. 2774.   5.40e6  6.33e6  0.0584\n 8 Thailand    arima  8.51e+6  -1127. 2269. 2270. 2288.  NA      NA      NA     \n 9 Vietnam     ets    4.55e-3  -1300. 2635. 2640. 2684.   3.05e6  3.42e6  0.0520\n10 Vietnam     arima  4.17e+6  -1085. 2181. 2182. 2198.  NA      NA      NA     \n# ℹ 2 more variables: ar_roots &lt;list&gt;, ma_roots &lt;list&gt;\n\n\nBe wary though, as information criteria (AIC, AICc, BIC) are only comparable between the same model class and only if those models share the same response (after transformations and differencing)."
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#extracting-fitted-and-residual-values",
    "href": "In-class_Ex/In-class_Ex07.html#extracting-fitted-and-residual-values",
    "title": "In-class Exercise 07",
    "section": "Extracting fitted and residual values",
    "text": "Extracting fitted and residual values\nThe fitted values and residuals from a model can obtained using fitted() and residuals() respectively. Additionally, the augment() function may be more convenient, which provides the original data along with both fitted values and their residuals.\n\n\nCode\nASEAN_fit %&gt;%\n  augment()\n\n\n# A tsibble: 1,320 x 7 [1M]\n# Key:       Country, .model [10]\n   Country   .model     Month Arrivals .fitted  .resid  .innov\n   &lt;chr&gt;     &lt;chr&gt;      &lt;mth&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 Indonesia ets     2008 Jan    62683  56534.   6149.  0.109 \n 2 Indonesia ets     2008 Feb    47834  46417.   1417.  0.0305\n 3 Indonesia ets     2008 Mar    64688  62660.   2028.  0.0324\n 4 Indonesia ets     2008 Apr    58074  61045.  -2971. -0.0487\n 5 Indonesia ets     2008 May    57089  62280.  -5191. -0.0833\n 6 Indonesia ets     2008 Jun    70118  75791.  -5673. -0.0749\n 7 Indonesia ets     2008 Jul    73805  78691.  -4886. -0.0621\n 8 Indonesia ets     2008 Aug    58015  61910.  -3895. -0.0629\n 9 Indonesia ets    2008 Sept    63730  74518. -10788. -0.145 \n10 Indonesia ets     2008 Oct    71206  67869.   3337.  0.0492\n# ℹ 1,310 more rows"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#comparing-fit-models",
    "href": "In-class_Ex/In-class_Ex07.html#comparing-fit-models",
    "title": "In-class Exercise 07",
    "section": "Comparing Fit Models",
    "text": "Comparing Fit Models\nIn the code chunk below, accuracy() is used to compare the performances of the models.\n\n\nCode\nASEAN_fit %&gt;%\n  accuracy() %&gt;%\n  arrange(Country)\n\n\n# A tibble: 10 × 11\n   Country   .model .type      ME   RMSE   MAE    MPE  MAPE  MASE RMSSE     ACF1\n   &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 Indonesia ets    Trai… -1.81e3 13187. 9665. -1.83   7.57 0.556 0.619 -0.236  \n 2 Indonesia arima  Trai… -9.54e1 11351. 8382. -0.136  6.38 0.482 0.533 -0.00802\n 3 Malaysia  ets    Trai… -6.78e2  4515. 3538. -1.25   5.15 0.529 0.527 -0.288  \n 4 Malaysia  arima  Trai… -2.33e1  4801. 3684. -0.109  5.20 0.551 0.561 -0.00933\n 5 Philippi… ets    Trai… -2.35e0  2298. 1897. -0.334  4.64 0.464 0.408  0.0400 \n 6 Philippi… arima  Trai…  9.53e0  2624. 1934. -0.269  4.60 0.473 0.466  0.00717\n 7 Thailand  ets    Trai…  1.97e1  2323. 1773. -0.511  5.89 0.489 0.485 -0.0812 \n 8 Thailand  arima  Trai…  5.88e1  2710. 1932. -0.562  6.16 0.532 0.565 -0.0112 \n 9 Vietnam   ets    Trai… -3.52e1  1745. 1386. -0.728  5.29 0.467 0.473  0.279  \n10 Vietnam   arima  Trai…  1.95e0  1907. 1458. -0.671  5.37 0.491 0.517  0.0136"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#forecast-future-values",
    "href": "In-class_Ex/In-class_Ex07.html#forecast-future-values",
    "title": "In-class Exercise 07",
    "section": "Forecast Future Values",
    "text": "Forecast Future Values\nForecasts from these models can be produced directly as our specified models do not require any additional data.\n\n\nCode\nASEAN_fc &lt;- ASEAN_fit %&gt;%\n  forecast(h = \"12 months\")"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex07.html#visualising-the-forecasted-values",
    "href": "In-class_Ex/In-class_Ex07.html#visualising-the-forecasted-values",
    "title": "In-class Exercise 07",
    "section": "Visualising the forecasted values",
    "text": "Visualising the forecasted values\nIn the code chunk below autoplot() of feasts package is used to plot the raw and fitted values.\n\n\nCode\nASEAN_fc %&gt;%\n  autoplot(ASEAN)"
  }
]